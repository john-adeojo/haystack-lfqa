{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc16e04f-771a-454d-86f3-3c2469bf8fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johna\\anaconda3\\envs\\lfqa_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from haystack.nodes import TextConverter, PDFToTextConverter, DocxToTextConverter, PreProcessor\n",
    "\n",
    "# def import_docs(filename, file_path):\n",
    "#     file_extension = os.path.splitext(file_path)[1]\n",
    "#     if file_extension == '.docx':\n",
    "#         converter = DocxToTextConverter(remove_numeric_tables=False, valid_languages=[\"en\"])\n",
    "#         doc = converter.convert(file_path=file_path, meta=None)[0]\n",
    "#     if file_extension == '.pdf':\n",
    "#         converter = PDFToTextConverter(remove_numeric_tables=True, valid_languages=[\"en\"])\n",
    "#         doc = converter.convert(file_path=file_path, meta=None)[0]\n",
    "#     return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d3b135-3eb3-462d-aeac-e8f03bca711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"ngrams reading.pdf\"\n",
    "# file_path = f\"C:\\\\Users\\\\johna\\\\anaconda3\\\\envs\\\\lfqa_env\\\\haystack-lfqa\\\\documents\\\\{filename}\"\n",
    "# doc_dir = f\"C:\\\\Users\\\\johna\\\\anaconda3\\\\envs\\\\lfqa_env\\\\haystack-lfqa\\\\documents\\\\\"\n",
    "# # doc = import_docs(filename, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82b0f58-cf03-4238-80b6-1c4ace11ae6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|                                                                             | 0/2 [00:00<?, ?docs/s]We found one or more sentences whose word count is higher than the split length.\n",
      "Preprocessing: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20.07docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files_input: 2\n",
      "n_docs_output: 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "from haystack.utils import convert_files_to_docs\n",
    "\n",
    "doc_dir = r\"C:\\Users\\johna\\anaconda3\\envs\\lfqa_env\\haystack-lfqa\\documents\"\n",
    "all_docs = convert_files_to_docs(dir_path=doc_dir)\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    clean_whitespace=True,\n",
    "    clean_header_footer=False,\n",
    "    split_by=\"word\",\n",
    "    split_respect_sentence_boundary=True,\n",
    "    split_overlap=30, \n",
    "    split_length=100\n",
    ")\n",
    "docs = preprocessor.process(all_docs)\n",
    "\n",
    "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cbd3aff-7328-40d1-a247-15079576c12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Document: {'content': '\\n\\nBefore you Begin\\nThink back to the data valuation and project determination of the AI Solution that your group thought of from Module 4’s group exercise. In 100 words or less, describe the pros and cons of the AI Solution that you chose to implement for your use case within your organisation. (This portion of the exercise will not be graded).\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 0, '_split_overlap': [{'doc_id': '3e44ec6b553485e6807dcf207ce1f8bd', 'range': (156, 347)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd3a5d2b67d11eadd54a63ea23d527222'}>,\n",
       " <Document: {'content': 'In 100 words or less, describe the pros and cons of the AI Solution that you chose to implement for your use case within your organisation. (This portion of the exercise will not be graded).\\n\\nGroup Exercise 4\\nGuidelines\\n\\nIn this exercise, the group will weigh and determine how to handle issues such as customer buy-in, adoption (if applicable), and trust, especially with regard to data privacy, storage, transfer, and access for your proposed AI solution.\\n\\nTogether, look at the potential for discrimination as a result of human, data, or algorithmic bias in your proposed AI solution. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 1, '_split_overlap': [{'doc_id': 'd3a5d2b67d11eadd54a63ea23d527222', 'range': (0, 191)}, {'doc_id': '4f651420f8fb06ebc2f4777ab3574dbc', 'range': (192, 587)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e44ec6b553485e6807dcf207ce1f8bd'}>,\n",
       " <Document: {'content': 'Group Exercise 4\\nGuidelines\\n\\nIn this exercise, the group will weigh and determine how to handle issues such as customer buy-in, adoption (if applicable), and trust, especially with regard to data privacy, storage, transfer, and access for your proposed AI solution.\\n\\nTogether, look at the potential for discrimination as a result of human, data, or algorithmic bias in your proposed AI solution. Next, formulate a viable strategy to safeguard against possible discrimination at all stages of the process. Then move on to the following questions.\\nExercise\\n\\n1. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 2, '_split_overlap': [{'doc_id': '3e44ec6b553485e6807dcf207ce1f8bd', 'range': (0, 395)}, {'doc_id': 'f3b751c51aea578d38d4259f75722b51', 'range': (267, 558)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f651420f8fb06ebc2f4777ab3574dbc'}>,\n",
       " <Document: {'content': 'Together, look at the potential for discrimination as a result of human, data, or algorithmic bias in your proposed AI solution. Next, formulate a viable strategy to safeguard against possible discrimination at all stages of the process. Then move on to the following questions.\\nExercise\\n\\n1. In 500 or fewer words (excluding citations), determine how you will handle issues such as customer buy-in, adoption (if applicable), and trust, especially with regard to their data?\\n\\nOur loan underwriting use case has a core challenge, the cold start problem. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 3, '_split_overlap': [{'doc_id': '4f651420f8fb06ebc2f4777ab3574dbc', 'range': (0, 291)}, {'doc_id': 'bcdb3942cded4d12ca3e02514fe13d9e', 'range': (292, 551)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f3b751c51aea578d38d4259f75722b51'}>,\n",
       " <Document: {'content': 'In 500 or fewer words (excluding citations), determine how you will handle issues such as customer buy-in, adoption (if applicable), and trust, especially with regard to their data?\\n\\nOur loan underwriting use case has a core challenge, the cold start problem. We need to fist attract a significant volume of users to the platform before we’re able to make use of data and benefit from network effects. It’s possible to look to form a strategy around solving this problem that addresses three key areas: Partnerships, financial health, and platforming.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 4, '_split_overlap': [{'doc_id': 'f3b751c51aea578d38d4259f75722b51', 'range': (0, 259)}, {'doc_id': '548248a9c52305405641e8b7cfd3efd4', 'range': (260, 552)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bcdb3942cded4d12ca3e02514fe13d9e'}>,\n",
       " <Document: {'content': 'We need to fist attract a significant volume of users to the platform before we’re able to make use of data and benefit from network effects. It’s possible to look to form a strategy around solving this problem that addresses three key areas: Partnerships, financial health, and platforming.\\n\\nPartnerships\\nBeing a Fintech, we can operate in an agile and have the capacity to standup state of the art tech. However, brand recognition and data are not available to us on the outset. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 5, '_split_overlap': [{'doc_id': 'bcdb3942cded4d12ca3e02514fe13d9e', 'range': (0, 292)}, {'doc_id': '2e19a284d2cde9147febc30cf5a32d7b', 'range': (293, 480)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '548248a9c52305405641e8b7cfd3efd4'}>,\n",
       " <Document: {'content': 'Partnerships\\nBeing a Fintech, we can operate in an agile and have the capacity to standup state of the art tech. However, brand recognition and data are not available to us on the outset. Overcoming this hurdle would require us to seek a partnership with either and incumbent or a big tech player, or another more established Fintech. Plum, a fintech that uses AI to help consumers accumulate savings partners with Monzo and Starling [1].\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 6, '_split_overlap': [{'doc_id': '548248a9c52305405641e8b7cfd3efd4', 'range': (0, 187)}, {'doc_id': '140bb87417a8557948dcdcf2d91b114c', 'range': (188, 439)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2e19a284d2cde9147febc30cf5a32d7b'}>,\n",
       " <Document: {'content': 'Overcoming this hurdle would require us to seek a partnership with either and incumbent or a big tech player, or another more established Fintech. Plum, a fintech that uses AI to help consumers accumulate savings partners with Monzo and Starling [1].\\n\\nFinancial Health\\nWe need a solid financial proposition so that customers will be willing to share their financial data with us and partners see value in working with us. The Step Change Debt Charity estimates that 4.4 million people struggle to keep up with their credit commitments [2]. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 7, '_split_overlap': [{'doc_id': '2e19a284d2cde9147febc30cf5a32d7b', 'range': (0, 251)}, {'doc_id': 'fd211395fa284e5279b4ce830a3a873c', 'range': (252, 539)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '140bb87417a8557948dcdcf2d91b114c'}>,\n",
       " <Document: {'content': 'Financial Health\\nWe need a solid financial proposition so that customers will be willing to share their financial data with us and partners see value in working with us. The Step Change Debt Charity estimates that 4.4 million people struggle to keep up with their credit commitments [2]. It’s evident that financial health and wellbeing is a massive opportunity and service that has large addressable market. Our proposition will look to focus on this, by providing a platform to manage credit card debt. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 8, '_split_overlap': [{'doc_id': '140bb87417a8557948dcdcf2d91b114c', 'range': (0, 287)}, {'doc_id': '3012dc55bd5dd2720da9cdb42f61286', 'range': (288, 504)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd211395fa284e5279b4ce830a3a873c'}>,\n",
       " <Document: {'content': 'It’s evident that financial health and wellbeing is a massive opportunity and service that has large addressable market. Our proposition will look to focus on this, by providing a platform to manage credit card debt. Initially we would provide basic functionality including: a marketplace for retail credit, credit scoring, and AI enabled debt management advise. Note, the marketplace would offer refinancing and debt consolidation loans to help move customers away from high interest credit cards.\\n\\nPlatforming\\nWe could attempt to position ourselves as the UK’s choice financial health platform. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 9, '_split_overlap': [{'doc_id': 'fd211395fa284e5279b4ce830a3a873c', 'range': (0, 216)}, {'doc_id': 'dc982d8227ef1706292a2b56003f5f46', 'range': (363, 596)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3012dc55bd5dd2720da9cdb42f61286'}>,\n",
       " <Document: {'content': 'Note, the marketplace would offer refinancing and debt consolidation loans to help move customers away from high interest credit cards.\\n\\nPlatforming\\nWe could attempt to position ourselves as the UK’s choice financial health platform. Taking this approach enables us to overcome the cold start problem by leveraging partnerships across smaller FinTechs, and eventually seeing the benefit of network effects taking over [3]. Our propositions could be broken out into microservices offered by partners initially via our platform. For example, a partnership with a Fintech like Cleo would enable us to embed AI driven debt and advise. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 10, '_split_overlap': [{'doc_id': '3012dc55bd5dd2720da9cdb42f61286', 'range': (0, 233)}, {'doc_id': 'a2283ae3d7dfb2bc487e6d3a736e263d', 'range': (423, 630)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dc982d8227ef1706292a2b56003f5f46'}>,\n",
       " <Document: {'content': 'Our propositions could be broken out into microservices offered by partners initially via our platform. For example, a partnership with a Fintech like Cleo would enable us to embed AI driven debt and advise. Partnering with tech-savvy challenger banks like Shawbrook Bank would give us access to debt consolidation, and cheap personal loans for our marketplace. Credit scoring could initially be provided by the likes of Equifax for example. The premise here is that our platform gives us the ability to integrate seamlessly with existing Fintechs through APIs.\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 11, '_split_overlap': [{'doc_id': 'dc982d8227ef1706292a2b56003f5f46', 'range': (0, 207)}, {'doc_id': '1327cf64348cda931ed14a1bd235eeca', 'range': (362, 562)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a2283ae3d7dfb2bc487e6d3a736e263d'}>,\n",
       " <Document: {'content': 'Credit scoring could initially be provided by the likes of Equifax for example. The premise here is that our platform gives us the ability to integrate seamlessly with existing Fintechs through APIs.\\n\\n[1] Plum integrates chatbot solution with Monzo, Starling Bank through Open Banking – Global WealthTech Summit 2021. (n.d.). Global Wealthtech Summit. Retrieved 28 February 2022, from\\n\\n[2] Step Change Debt Charity. (2022, January). Falling behind to keep up: the credit safety net and problem debt.\\n\\n[3] Bahri, G. (2021, February 8). The Platformization of Banking. The Platformization of Banking. Retrieved 21 February 2022, from\\n\\n2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 12, '_split_overlap': [{'doc_id': 'a2283ae3d7dfb2bc487e6d3a736e263d', 'range': (0, 200)}, {'doc_id': '6944aeb186fb28b6ec274d121068a72', 'range': (433, 635)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1327cf64348cda931ed14a1bd235eeca'}>,\n",
       " <Document: {'content': 'Falling behind to keep up: the credit safety net and problem debt.\\n\\n[3] Bahri, G. (2021, February 8). The Platformization of Banking. The Platformization of Banking. Retrieved 21 February 2022, from\\n\\n2. In 500 or fewer words (excluding citations), examine how you will successfully address the potential for human, data, or algorithmic bias (choose only one to discuss) to help ensure non-discrimination?\\n\\nAndriy Burkov’s machine learning engineering book cites the types of bias that occur in machine learning [1].\\n\\nAdditionally, we can avoid algorithmic bias by ensuring are machine learning models for underwriting are developed following best practices. I.e.', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 13, '_split_overlap': [{'doc_id': '1327cf64348cda931ed14a1bd235eeca', 'range': (0, 202)}, {'doc_id': '7a4dba1acfd27e5df6a18a3114326c1c', 'range': (406, 661)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6944aeb186fb28b6ec274d121068a72'}>,\n",
       " <Document: {'content': 'Andriy Burkov’s machine learning engineering book cites the types of bias that occur in machine learning [1].\\n\\nAdditionally, we can avoid algorithmic bias by ensuring are machine learning models for underwriting are developed following best practices. I.e., using a training, validation and test set properly to evaluate model performance, model monitoring and re-training loops, and setting transparent model performance criteria.\\n\\n[1] Burkov, A. (2020). Machine Learning Engineering. True Positive Inc.\\n\\n3. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 14, '_split_overlap': [{'doc_id': '6944aeb186fb28b6ec274d121068a72', 'range': (0, 255)}, {'doc_id': '4831383e348e512e94d51ece7eb8641f', 'range': (256, 508)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7a4dba1acfd27e5df6a18a3114326c1c'}>,\n",
       " <Document: {'content': ', using a training, validation and test set properly to evaluate model performance, model monitoring and re-training loops, and setting transparent model performance criteria.\\n\\n[1] Burkov, A. (2020). Machine Learning Engineering. True Positive Inc.\\n\\n3. In 500 words or fewer (excluding citations), analyse what concerns might be raised by your legal department regarding customer privacy, and how will you ensure compliance with current—and any anticipated future—regulations.\\n\\n4. In 500 words for fewer (excluding citations), examine what ethical standards must you ensure your project meets, and what best practices will you follow, to assuage concerns and build customer trust?\\n\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 15, '_split_overlap': [{'doc_id': '7a4dba1acfd27e5df6a18a3114326c1c', 'range': (0, 252)}, {'doc_id': '4f5352193220332d90265a12df85c469', 'range': (481, 681)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4831383e348e512e94d51ece7eb8641f'}>,\n",
       " <Document: {'content': 'In 500 words for fewer (excluding citations), examine what ethical standards must you ensure your project meets, and what best practices will you follow, to assuage concerns and build customer trust?\\n\\nPost-Exercise Reflection\\n\\nNow that you have completed the exercise, think back to the data valuation and project determination for your AI solution that your group identified in Module 4’s group exercise. In 100-200 words, describe how the exercise you have just completed modified your idea. (This portion of the exercise will not be graded).', 'content_type': 'text', 'score': None, 'meta': {'name': 'AIF_Mod5_GroupEx_JohnAdeojo.docx', '_split_id': 16, '_split_overlap': [{'doc_id': '4831383e348e512e94d51ece7eb8641f', 'range': (0, 200)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f5352193220332d90265a12df85c469'}>,\n",
       " <Document: {'content': 'Speech and Language Processing.\\nDaniel Jurafsky & James H. Martin.\\nCopyright c⃝ 2019.\\nAll\\nrights reserved.\\nDraft of October 2, 2019.\\nCHAPTER\\n3\\nN-gram Language Models\\n“You are uniformly charming!” cried he, with a smile of associating and now\\nand then I bowed and they perceived a chaise and four to wish for.\\nRandom sentence generated from a Jane Austen trigram model\\nPredicting is difﬁcult—especially about the future, as the old quip goes. But how\\nabout predicting something that seems much easier, like the next few words someone\\nis going to say? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 0, '_split_overlap': [{'doc_id': 'd2d00b09cde8ebb3c56308f1fdd23cb', 'range': (309, 549)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b9591dd2bb2bb899abe9268f49e38eab'}>,\n",
       " <Document: {'content': 'Random sentence generated from a Jane Austen trigram model\\nPredicting is difﬁcult—especially about the future, as the old quip goes. But how\\nabout predicting something that seems much easier, like the next few words someone\\nis going to say? What word, for example, is likely to follow\\nPlease turn your homework ...\\nHopefully, most of you concluded that a very likely word is in, or possibly over,\\nbut probably not refrigerator or the. In the following sections we will formalize\\nthis intuition by introducing models that assign a probability to each possible next\\nword. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 1, '_split_overlap': [{'doc_id': 'b9591dd2bb2bb899abe9268f49e38eab', 'range': (0, 240)}, {'doc_id': '268f94e21fedc4a81302c7e5d66eb894', 'range': (241, 569)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2d00b09cde8ebb3c56308f1fdd23cb'}>,\n",
       " <Document: {'content': 'What word, for example, is likely to follow\\nPlease turn your homework ...\\nHopefully, most of you concluded that a very likely word is in, or possibly over,\\nbut probably not refrigerator or the. In the following sections we will formalize\\nthis intuition by introducing models that assign a probability to each possible next\\nword. The same models will also serve to assign a probability to an entire sentence.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 2, '_split_overlap': [{'doc_id': 'd2d00b09cde8ebb3c56308f1fdd23cb', 'range': (0, 328)}, {'doc_id': '84eea6e7c1dc781c0bf320b6bc338f90', 'range': (194, 407)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '268f94e21fedc4a81302c7e5d66eb894'}>,\n",
       " <Document: {'content': 'In the following sections we will formalize\\nthis intuition by introducing models that assign a probability to each possible next\\nword. The same models will also serve to assign a probability to an entire sentence.\\nSuch a model, for example, could predict that the following sequence has a much\\nhigher probability of appearing in a text:\\nall of a sudden I notice three guys standing on the sidewalk\\nthan does this same set of words in a different order:\\non guys all I of notice sidewalk three a sudden standing the\\nWhy would you want to predict upcoming words, or assign probabilities to sen-\\ntences? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 3, '_split_overlap': [{'doc_id': '268f94e21fedc4a81302c7e5d66eb894', 'range': (0, 213)}, {'doc_id': 'd5b6adad9881682ee25525a0ca22de03', 'range': (214, 599)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '84eea6e7c1dc781c0bf320b6bc338f90'}>,\n",
       " <Document: {'content': 'Such a model, for example, could predict that the following sequence has a much\\nhigher probability of appearing in a text:\\nall of a sudden I notice three guys standing on the sidewalk\\nthan does this same set of words in a different order:\\non guys all I of notice sidewalk three a sudden standing the\\nWhy would you want to predict upcoming words, or assign probabilities to sen-\\ntences? Probabilities are essential in any task in which we have to identify words in\\nnoisy, ambiguous input, like speech recognition. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 4, '_split_overlap': [{'doc_id': '84eea6e7c1dc781c0bf320b6bc338f90', 'range': (0, 385)}, {'doc_id': '8a32cf944d942fa130ef195dbe181a81', 'range': (386, 512)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd5b6adad9881682ee25525a0ca22de03'}>,\n",
       " <Document: {'content': 'Probabilities are essential in any task in which we have to identify words in\\nnoisy, ambiguous input, like speech recognition. For a speech recognizer to realize\\nthat you said I will be back soonish and not I will be bassoon dish, it helps to know\\nthat back soonish is a much more probable sequence than bassoon dish. For writing\\ntools like spelling correction or grammatical error correction, we need to ﬁnd and\\ncorrect errors in writing like Their are two midterms, in which There was mistyped\\nas Their, or Everything has improve, in which improve should have been improved.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 5, '_split_overlap': [{'doc_id': 'd5b6adad9881682ee25525a0ca22de03', 'range': (0, 126)}, {'doc_id': '18180c68a3119c8aa20760345e6a3f22', 'range': (318, 576)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a32cf944d942fa130ef195dbe181a81'}>,\n",
       " <Document: {'content': 'For writing\\ntools like spelling correction or grammatical error correction, we need to ﬁnd and\\ncorrect errors in writing like Their are two midterms, in which There was mistyped\\nas Their, or Everything has improve, in which improve should have been improved.\\nThe phrase There are will be much more probable than Their are, and has improved\\nthan has improve, allowing us to help users by detecting and correcting these errors.\\nAssigning probabilities to sequences of words is also essential in machine trans-\\nlation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 6, '_split_overlap': [{'doc_id': '8a32cf944d942fa130ef195dbe181a81', 'range': (0, 258)}, {'doc_id': '292aac8570fe226ac812ae529b12e43', 'range': (259, 515)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '18180c68a3119c8aa20760345e6a3f22'}>,\n",
       " <Document: {'content': 'The phrase There are will be much more probable than Their are, and has improved\\nthan has improve, allowing us to help users by detecting and correcting these errors.\\nAssigning probabilities to sequences of words is also essential in machine trans-\\nlation. Suppose we are translating a Chinese source sentence:\\n他 向 记者\\n介绍了\\n主要 内容\\nHe to\\nreporters introduced main content\\nAs part of the process we might have built the following set of potential rough\\nEnglish translations:\\nhe introduced reporters to the main contents of the statement\\nhe briefed to reporters the main contents of the statement\\nhe briefed reporters on the main contents of the statement\\x0c2\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nA probabilistic model of word sequences could suggest that briefed reporters on\\nis a more probable English phrase than briefed to reporters (which has an awkward\\nto after briefed) or introduced reporters to (which uses a verb that is less ﬂuent\\nEnglish in this context), allowing us to correctly select the boldfaced sentence above.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 7, '_split_overlap': [{'doc_id': '18180c68a3119c8aa20760345e6a3f22', 'range': (0, 256)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '292aac8570fe226ac812ae529b12e43'}>,\n",
       " <Document: {'content': 'Probabilities are also important for augmentative and alternative communi-\\ncation systems (Trnka et al. 2007, Kane et al. 2017). People often use such AAC\\nAAC\\ndevices if they are physically unable or sign but can instead using eye gaze or other\\nspeciﬁc movements to select words from a menu to be spoken by the system. Word\\nprediction can be used to suggest likely words for the menu.\\nModels that assign probabilities to sequences of words are called language mod-\\nels or LMs. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 8, '_split_overlap': [{'doc_id': '2db6e938730807f66eacf732ff5a1586', 'range': (129, 476)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '216b2bf91fecd283398fe4ecec8e5612'}>,\n",
       " <Document: {'content': 'People often use such AAC\\nAAC\\ndevices if they are physically unable or sign but can instead using eye gaze or other\\nspeciﬁc movements to select words from a menu to be spoken by the system. Word\\nprediction can be used to suggest likely words for the menu.\\nModels that assign probabilities to sequences of words are called language mod-\\nels or LMs. In this chapter we introduce the simplest model that assigns probabilities\\nlanguage model\\nLM\\nto sentences and sequences of words, the n-gram. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 9, '_split_overlap': [{'doc_id': '216b2bf91fecd283398fe4ecec8e5612', 'range': (0, 347)}, {'doc_id': 'b33283b2329416328df8dea517ac031b', 'range': (256, 489)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2db6e938730807f66eacf732ff5a1586'}>,\n",
       " <Document: {'content': 'Models that assign probabilities to sequences of words are called language mod-\\nels or LMs. In this chapter we introduce the simplest model that assigns probabilities\\nlanguage model\\nLM\\nto sentences and sequences of words, the n-gram. An n-gram is a sequence of N\\nn-gram\\nwords: a 2-gram (or bigram) is a two-word sequence of words like “please turn”,\\n“turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se-\\nquence of words like “please turn your”, or “turn your homework”. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 10, '_split_overlap': [{'doc_id': '2db6e938730807f66eacf732ff5a1586', 'range': (0, 233)}, {'doc_id': 'd24d03f1fc57e917d6016adaf514c91e', 'range': (234, 494)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b33283b2329416328df8dea517ac031b'}>,\n",
       " <Document: {'content': 'An n-gram is a sequence of N\\nn-gram\\nwords: a 2-gram (or bigram) is a two-word sequence of words like “please turn”,\\n“turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word se-\\nquence of words like “please turn your”, or “turn your homework”. We’ll see how\\nto use n-gram models to estimate the probability of the last word of an n-gram given\\nthe previous words, and also to assign probabilities to entire sequences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 11, '_split_overlap': [{'doc_id': 'b33283b2329416328df8dea517ac031b', 'range': (0, 260)}, {'doc_id': 'e26364995ae61f7dac364e7c4b0cfa0b', 'range': (261, 433)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd24d03f1fc57e917d6016adaf514c91e'}>,\n",
       " <Document: {'content': 'We’ll see how\\nto use n-gram models to estimate the probability of the last word of an n-gram given\\nthe previous words, and also to assign probabilities to entire sequences. In a bit of\\nterminological ambiguity, we usually drop the word “model”, and thus the term n-\\ngram is used to mean either the word sequence itself or the predictive model that\\nassigns it a probability. In later chapters we’ll introduce more sophisticated language\\nmodels like the RNN LMs of Chapter 9.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 12, '_split_overlap': [{'doc_id': 'd24d03f1fc57e917d6016adaf514c91e', 'range': (0, 172)}, {'doc_id': '6071b6b0b9a63fcb1837257572f586cf', 'range': (173, 473)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e26364995ae61f7dac364e7c4b0cfa0b'}>,\n",
       " <Document: {'content': 'In a bit of\\nterminological ambiguity, we usually drop the word “model”, and thus the term n-\\ngram is used to mean either the word sequence itself or the predictive model that\\nassigns it a probability. In later chapters we’ll introduce more sophisticated language\\nmodels like the RNN LMs of Chapter 9.\\n3.1\\nN-Grams\\nLet’s begin with the task of computing P(w|h), the probability of a word w given\\nsome history h. Suppose the history h is “its water is so transparent that” and we\\nwant to know the probability that the next word is the:\\nP(the|its water is so transparent that).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 13, '_split_overlap': [{'doc_id': 'e26364995ae61f7dac364e7c4b0cfa0b', 'range': (0, 300)}, {'doc_id': '67781d0631beda4446b02c4dd2bf5d9b', 'range': (301, 573)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6071b6b0b9a63fcb1837257572f586cf'}>,\n",
       " <Document: {'content': '3.1\\nN-Grams\\nLet’s begin with the task of computing P(w|h), the probability of a word w given\\nsome history h. Suppose the history h is “its water is so transparent that” and we\\nwant to know the probability that the next word is the:\\nP(the|its water is so transparent that).\\n(3.1)\\nOne way to estimate this probability is from relative frequency counts: take a\\nvery large corpus, count the number of times we see its water is so transparent that,\\nand count the number of times this is followed by the. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 14, '_split_overlap': [{'doc_id': '6071b6b0b9a63fcb1837257572f586cf', 'range': (0, 272)}, {'doc_id': 'c3336f500cd549af6de9b603d703bd3a', 'range': (273, 498)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '67781d0631beda4446b02c4dd2bf5d9b'}>,\n",
       " <Document: {'content': '(3.1)\\nOne way to estimate this probability is from relative frequency counts: take a\\nvery large corpus, count the number of times we see its water is so transparent that,\\nand count the number of times this is followed by the. This would be answering the\\nquestion “Out of the times we saw the history h, how many times was it followed by\\nthe word w”, as follows:\\nP(the|its water is so transparent that) =\\nC(its water is so transparent that the)\\nC(its water is so transparent that)\\n(3.2)\\nWith a large enough corpus, such as the web, we can compute these counts and\\nestimate the probability from Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 15, '_split_overlap': [{'doc_id': '67781d0631beda4446b02c4dd2bf5d9b', 'range': (0, 225)}, {'doc_id': '441ebd847385231f29e8f0eb5ae4c6fb', 'range': (226, 596)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c3336f500cd549af6de9b603d703bd3a'}>,\n",
       " <Document: {'content': 'This would be answering the\\nquestion “Out of the times we saw the history h, how many times was it followed by\\nthe word w”, as follows:\\nP(the|its water is so transparent that) =\\nC(its water is so transparent that the)\\nC(its water is so transparent that)\\n(3.2)\\nWith a large enough corpus, such as the web, we can compute these counts and\\nestimate the probability from Eq. 3.2. You should pause now, go to the web, and\\ncompute this estimate for yourself.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 16, '_split_overlap': [{'doc_id': 'c3336f500cd549af6de9b603d703bd3a', 'range': (0, 370)}, {'doc_id': 'b3327311e0496eeed392f01c0f88233e', 'range': (371, 452)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '441ebd847385231f29e8f0eb5ae4c6fb'}>,\n",
       " <Document: {'content': '3.2. You should pause now, go to the web, and\\ncompute this estimate for yourself.\\nWhile this method of estimating probabilities directly from counts works ﬁne in\\nmany cases, it turns out that even the web isn’t big enough to give us good estimates\\nin most cases. This is because language is creative; new sentences are created all the\\ntime, and we won’t always be able to count entire sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 17, '_split_overlap': [{'doc_id': '441ebd847385231f29e8f0eb5ae4c6fb', 'range': (0, 81)}, {'doc_id': 'a96b1b7ab3b099d8f92745eb4c2ac862', 'range': (82, 395)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3327311e0496eeed392f01c0f88233e'}>,\n",
       " <Document: {'content': 'While this method of estimating probabilities directly from counts works ﬁne in\\nmany cases, it turns out that even the web isn’t big enough to give us good estimates\\nin most cases. This is because language is creative; new sentences are created all the\\ntime, and we won’t always be able to count entire sentences. Even simple extensions\\nof the example sentence may have counts of zero on the web (such as “Walden\\nPond’s water is so transparent that the”; well, used to have counts of zero).\\x0c', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 18, '_split_overlap': [{'doc_id': 'b3327311e0496eeed392f01c0f88233e', 'range': (0, 313)}, {'doc_id': 'c899a1006cf6ab6542de41ceccf04e43', 'range': (314, 490)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a96b1b7ab3b099d8f92745eb4c2ac862'}>,\n",
       " <Document: {'content': 'Even simple extensions\\nof the example sentence may have counts of zero on the web (such as “Walden\\nPond’s water is so transparent that the”; well, used to have counts of zero).\\x0c3.1\\n•\\nN-GRAMS\\n3\\nSimilarly, if we wanted to know the joint probability of an entire sequence of\\nwords like its water is so transparent, we could do it by asking “out of all possible\\nsequences of ﬁve words, how many of them are its water is so transparent?” We\\nwould have to get the count of its water is so transparent and divide by the sum of\\nthe counts of all possible ﬁve word sequences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 19, '_split_overlap': [{'doc_id': 'a96b1b7ab3b099d8f92745eb4c2ac862', 'range': (0, 176)}, {'doc_id': '96b56f2bac095bff77dac3b1991ab56', 'range': (177, 566)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c899a1006cf6ab6542de41ceccf04e43'}>,\n",
       " <Document: {'content': '3.1\\n•\\nN-GRAMS\\n3\\nSimilarly, if we wanted to know the joint probability of an entire sequence of\\nwords like its water is so transparent, we could do it by asking “out of all possible\\nsequences of ﬁve words, how many of them are its water is so transparent?” We\\nwould have to get the count of its water is so transparent and divide by the sum of\\nthe counts of all possible ﬁve word sequences. That seems rather a lot to estimate!\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 20, '_split_overlap': [{'doc_id': 'c899a1006cf6ab6542de41ceccf04e43', 'range': (0, 389)}, {'doc_id': 'bb6f90f886f3fec92bc6e9a0fd47534e', 'range': (390, 426)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '96b56f2bac095bff77dac3b1991ab56'}>,\n",
       " <Document: {'content': 'That seems rather a lot to estimate!\\nFor this reason, we’ll need to introduce cleverer ways of estimating the proba-\\nbility of a word w given a history h, or the probability of an entire word sequence W.\\nLet’s start with a little formalizing of notation. To represent the probability of a par-\\nticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use\\nthe simpliﬁcation P(the). We’ll represent a sequence of N words either as w1 ...wn\\nor wn\\n1 (so the expression wn−1\\n1\\nmeans the string w1,w2,...,wn−1). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 21, '_split_overlap': [{'doc_id': '96b56f2bac095bff77dac3b1991ab56', 'range': (0, 36)}, {'doc_id': '9cb15389d2a543efc2e4d4ccc775d026', 'range': (255, 528)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb6f90f886f3fec92bc6e9a0fd47534e'}>,\n",
       " <Document: {'content': 'To represent the probability of a par-\\nticular random variable Xi taking on the value “the”, or P(Xi = “the”), we will use\\nthe simpliﬁcation P(the). We’ll represent a sequence of N words either as w1 ...wn\\nor wn\\n1 (so the expression wn−1\\n1\\nmeans the string w1,w2,...,wn−1). For the joint prob-\\nability of each word in a sequence having a particular value P(X = w1,Y = w2,Z =\\nw3,...,W = wn) we’ll use P(w1,w2,...,wn).\\nNow how can we compute probabilities of entire sequences like P(w1,w2,...,wn)?\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 22, '_split_overlap': [{'doc_id': 'bb6f90f886f3fec92bc6e9a0fd47534e', 'range': (0, 273)}, {'doc_id': '4a273cc560118139f07e683a206c0983', 'range': (274, 495)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9cb15389d2a543efc2e4d4ccc775d026'}>,\n",
       " <Document: {'content': 'For the joint prob-\\nability of each word in a sequence having a particular value P(X = w1,Y = w2,Z =\\nw3,...,W = wn) we’ll use P(w1,w2,...,wn).\\nNow how can we compute probabilities of entire sequences like P(w1,w2,...,wn)?\\nOne thing we can do is decompose this probability using the chain rule of proba-\\nbility:\\nP(X1...Xn) = P(X1)P(X2|X1)P(X3|X2\\n1 )...P(Xn|Xn−1\\n1\\n)\\n=\\nn\\n�\\nk=1\\nP(Xk|Xk−1\\n1\\n)\\n(3.3)\\nApplying the chain rule to words, we get\\nP(wn\\n1) = P(w1)P(w2|w1)P(w3|w2\\n1)...P(wn|wn−1\\n1\\n)\\n=\\nn\\n�\\nk=1\\nP(wk|wk−1\\n1\\n)\\n(3.4)\\nThe chain rule shows the link between computing the joint probability of a se-\\nquence and computing the conditional probability of a word given previous words.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 23, '_split_overlap': [{'doc_id': '9cb15389d2a543efc2e4d4ccc775d026', 'range': (0, 221)}, {'doc_id': '23b839aa3a34d06f82c9f4509e9f18f2', 'range': (222, 675)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4a273cc560118139f07e683a206c0983'}>,\n",
       " <Document: {'content': 'One thing we can do is decompose this probability using the chain rule of proba-\\nbility:\\nP(X1...Xn) = P(X1)P(X2|X1)P(X3|X2\\n1 )...P(Xn|Xn−1\\n1\\n)\\n=\\nn\\n�\\nk=1\\nP(Xk|Xk−1\\n1\\n)\\n(3.3)\\nApplying the chain rule to words, we get\\nP(wn\\n1) = P(w1)P(w2|w1)P(w3|w2\\n1)...P(wn|wn−1\\n1\\n)\\n=\\nn\\n�\\nk=1\\nP(wk|wk−1\\n1\\n)\\n(3.4)\\nThe chain rule shows the link between computing the joint probability of a se-\\nquence and computing the conditional probability of a word given previous words.\\nEquation 3.4 suggests that we could estimate the joint probability of an entire se-\\nquence of words by multiplying together a number of conditional probabilities. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 24, '_split_overlap': [{'doc_id': '4a273cc560118139f07e683a206c0983', 'range': (0, 453)}, {'doc_id': '9aadde63c553ebc8a901134f9af0427b', 'range': (454, 616)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '23b839aa3a34d06f82c9f4509e9f18f2'}>,\n",
       " <Document: {'content': 'Equation 3.4 suggests that we could estimate the joint probability of an entire se-\\nquence of words by multiplying together a number of conditional probabilities. But\\nusing the chain rule doesn’t really seem to help us! We don’t know any way to\\ncompute the exact probability of a word given a long sequence of preceding words,\\nP(wn|wn−1\\n1\\n). As we said above, we can’t just estimate by counting the number of\\ntimes every word occurs following every long string, because language is creative\\nand any particular context might have never occurred before!\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 25, '_split_overlap': [{'doc_id': '23b839aa3a34d06f82c9f4509e9f18f2', 'range': (0, 162)}, {'doc_id': '1fcd822af9702ad362989311e7600c69', 'range': (342, 551)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9aadde63c553ebc8a901134f9af0427b'}>,\n",
       " <Document: {'content': 'As we said above, we can’t just estimate by counting the number of\\ntimes every word occurs following every long string, because language is creative\\nand any particular context might have never occurred before!\\nThe intuition of the n-gram model is that instead of computing the probability of\\na word given its entire history, we can approximate the history by just the last few\\nwords.\\nThe bigram model, for example, approximates the probability of a word given\\nbigram\\nall the previous words P(wn|wn−1\\n1\\n) by using only the conditional probability of the\\npreceding word P(wn|wn−1). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 26, '_split_overlap': [{'doc_id': '9aadde63c553ebc8a901134f9af0427b', 'range': (0, 209)}, {'doc_id': '771a213318af64d99c4a3698aaa708d4', 'range': (384, 579)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1fcd822af9702ad362989311e7600c69'}>,\n",
       " <Document: {'content': 'The bigram model, for example, approximates the probability of a word given\\nbigram\\nall the previous words P(wn|wn−1\\n1\\n) by using only the conditional probability of the\\npreceding word P(wn|wn−1). In other words, instead of computing the probability\\nP(the|Walden Pond’s water is so transparent that)\\n(3.5)\\nwe approximate it with the probability\\nP(the|that)\\n(3.6)\\x0c4\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nWhen we use a bigram model to predict the conditional probability of the next\\nword, we are thus making the following approximation:\\nP(wn|wn−1\\n1\\n) ≈ P(wn|wn−1)\\n(3.7)\\nThe assumption that the probability of a word depends only on the previous word\\nis called a Markov assumption. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 27, '_split_overlap': [{'doc_id': '1fcd822af9702ad362989311e7600c69', 'range': (0, 195)}, {'doc_id': '57febcbe0052119f5e09632488ab96df', 'range': (196, 674)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '771a213318af64d99c4a3698aaa708d4'}>,\n",
       " <Document: {'content': 'In other words, instead of computing the probability\\nP(the|Walden Pond’s water is so transparent that)\\n(3.5)\\nwe approximate it with the probability\\nP(the|that)\\n(3.6)\\x0c4\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nWhen we use a bigram model to predict the conditional probability of the next\\nword, we are thus making the following approximation:\\nP(wn|wn−1\\n1\\n) ≈ P(wn|wn−1)\\n(3.7)\\nThe assumption that the probability of a word depends only on the previous word\\nis called a Markov assumption. Markov models are the class of probabilistic models\\nMarkov\\nthat assume we can predict the probability of some future unit without looking too\\nfar into the past. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 28, '_split_overlap': [{'doc_id': '771a213318af64d99c4a3698aaa708d4', 'range': (0, 478)}, {'doc_id': 'a50127256f6a2a02512fb4598facac69', 'range': (479, 639)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '57febcbe0052119f5e09632488ab96df'}>,\n",
       " <Document: {'content': 'Markov models are the class of probabilistic models\\nMarkov\\nthat assume we can predict the probability of some future unit without looking too\\nfar into the past. We can generalize the bigram (which looks one word into the past)\\nto the trigram (which looks two words into the past) and thus to the n-gram (which\\nn-gram\\nlooks n−1 words into the past).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 29, '_split_overlap': [{'doc_id': '57febcbe0052119f5e09632488ab96df', 'range': (0, 160)}, {'doc_id': 'd8c04608ccfbfc7132c5e279de8da792', 'range': (161, 348)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a50127256f6a2a02512fb4598facac69'}>,\n",
       " <Document: {'content': 'We can generalize the bigram (which looks one word into the past)\\nto the trigram (which looks two words into the past) and thus to the n-gram (which\\nn-gram\\nlooks n−1 words into the past).\\nThus, the general equation for this n-gram approximation to the conditional\\nprobability of the next word in a sequence is\\nP(wn|wn−1\\n1\\n) ≈ P(wn|wn−1\\nn−N+1)\\n(3.8)\\nGiven the bigram assumption for the probability of an individual word, we can\\ncompute the probability of a complete word sequence by substituting Eq. 3.7 into\\nEq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 30, '_split_overlap': [{'doc_id': 'a50127256f6a2a02512fb4598facac69', 'range': (0, 187)}, {'doc_id': 'e607e34171333cf8c296171eca20b7e9', 'range': (188, 511)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd8c04608ccfbfc7132c5e279de8da792'}>,\n",
       " <Document: {'content': 'Thus, the general equation for this n-gram approximation to the conditional\\nprobability of the next word in a sequence is\\nP(wn|wn−1\\n1\\n) ≈ P(wn|wn−1\\nn−N+1)\\n(3.8)\\nGiven the bigram assumption for the probability of an individual word, we can\\ncompute the probability of a complete word sequence by substituting Eq. 3.7 into\\nEq. 3.4:\\nP(wn\\n1) ≈\\nn\\n�\\nk=1\\nP(wk|wk−1)\\n(3.9)\\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\\nestimate probabilities is called maximum likelihood estimation or MLE. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 31, '_split_overlap': [{'doc_id': 'd8c04608ccfbfc7132c5e279de8da792', 'range': (0, 323)}, {'doc_id': 'e78dfd75642b1545900a2cc9cc2f3dca', 'range': (324, 511)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e607e34171333cf8c296171eca20b7e9'}>,\n",
       " <Document: {'content': '3.4:\\nP(wn\\n1) ≈\\nn\\n�\\nk=1\\nP(wk|wk−1)\\n(3.9)\\nHow do we estimate these bigram or n-gram probabilities? An intuitive way to\\nestimate probabilities is called maximum likelihood estimation or MLE. We get\\nmaximum\\nlikelihood\\nestimation\\nthe MLE estimate for the parameters of an n-gram model by getting counts from a\\ncorpus, and normalizing the counts so that they lie between 0 and 1.1\\nnormalize\\nFor example, to compute a particular bigram probability of a word y given a\\nprevious word x, we’ll compute the count of the bigram C(xy) and normalize by the\\nsum of all the bigrams that share the same ﬁrst word x:\\nP(wn|wn−1) =\\nC(wn−1wn)\\n�\\nwC(wn−1w)\\n(3.10)\\nWe can simplify this equation, since the sum of all bigram counts that start with\\na given word wn−1 must be equal to the unigram count for that word wn−1 (the reader\\nshould take a moment to be convinced of this):\\nP(wn|wn−1) = C(wn−1wn)\\nC(wn−1)\\n(3.11)\\nLet’s work through an example using a mini-corpus of three sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 32, '_split_overlap': [{'doc_id': 'e607e34171333cf8c296171eca20b7e9', 'range': (0, 187)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e78dfd75642b1545900a2cc9cc2f3dca'}>,\n",
       " <Document: {'content': 'We’ll\\nﬁrst need to augment each sentence with a special symbol <s> at the beginning\\nof the sentence, to give us the bigram context of the ﬁrst word. We’ll also need a\\nspecial end-symbol. </s>2\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I do not like green eggs and ham </s>\\n1\\nFor probabilistic models, normalizing means dividing by some total count so that the resulting prob-\\nabilities fall legally between 0 and 1.\\n2\\nWe need the end-symbol to make the bigram grammar a true probability distribution. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 33, '_split_overlap': [{'doc_id': '3a43bad508539c69f1d10c44dd65cc3a', 'range': (187, 498)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '490748f5c7fe6cc66e5c9d017595291'}>,\n",
       " <Document: {'content': '</s>2\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I do not like green eggs and ham </s>\\n1\\nFor probabilistic models, normalizing means dividing by some total count so that the resulting prob-\\nabilities fall legally between 0 and 1.\\n2\\nWe need the end-symbol to make the bigram grammar a true probability distribution. Without an\\nend-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model\\nwould deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See\\nExercise 3.5.\\x0c', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 34, '_split_overlap': [{'doc_id': '490748f5c7fe6cc66e5c9d017595291', 'range': (0, 311)}, {'doc_id': '45e387f361dcf129935785a411ed64aa', 'range': (312, 544)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3a43bad508539c69f1d10c44dd65cc3a'}>,\n",
       " <Document: {'content': 'Without an\\nend-symbol, the sentence probabilities for all sentences of a given length would sum to one. This model\\nwould deﬁne an inﬁnite set of probability distributions, with one distribution per sentence length. See\\nExercise 3.5.\\x0c3.1\\n•\\nN-GRAMS\\n5\\nHere are the calculations for some of the bigram probabilities from this corpus\\nP(I|<s>) = 2\\n3 = .67\\nP(Sam|<s>) = 1\\n3 = .33\\nP(am|I) = 2\\n3 = .67\\nP(</s>|Sam) = 1\\n2 = 0.5\\nP(Sam|am) = 1\\n2 = .5\\nP(do|I) = 1\\n3 = .33\\nFor the general case of MLE n-gram parameter estimation:\\nP(wn|wn−1\\nn−N+1) = C(wn−1\\nn−N+1wn)\\nC(wn−1\\nn−N+1)\\n(3.12)\\nEquation 3.12 (like Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 35, '_split_overlap': [{'doc_id': '3a43bad508539c69f1d10c44dd65cc3a', 'range': (0, 232)}, {'doc_id': '7bdf7562bc6462d5f4b6255c2e01c0c6', 'range': (233, 594)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '45e387f361dcf129935785a411ed64aa'}>,\n",
       " <Document: {'content': '3.1\\n•\\nN-GRAMS\\n5\\nHere are the calculations for some of the bigram probabilities from this corpus\\nP(I|<s>) = 2\\n3 = .67\\nP(Sam|<s>) = 1\\n3 = .33\\nP(am|I) = 2\\n3 = .67\\nP(</s>|Sam) = 1\\n2 = 0.5\\nP(Sam|am) = 1\\n2 = .5\\nP(do|I) = 1\\n3 = .33\\nFor the general case of MLE n-gram parameter estimation:\\nP(wn|wn−1\\nn−N+1) = C(wn−1\\nn−N+1wn)\\nC(wn−1\\nn−N+1)\\n(3.12)\\nEquation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the\\nobserved frequency of a particular sequence by the observed frequency of a preﬁx.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 36, '_split_overlap': [{'doc_id': '45e387f361dcf129935785a411ed64aa', 'range': (0, 361)}, {'doc_id': 'e03e64ed89fc6765322f62b6428090c3', 'range': (362, 498)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7bdf7562bc6462d5f4b6255c2e01c0c6'}>,\n",
       " <Document: {'content': '3.11) estimates the n-gram probability by dividing the\\nobserved frequency of a particular sequence by the observed frequency of a preﬁx.\\nThis ratio is called a relative frequency. We said above that this use of relative\\nrelative\\nfrequency\\nfrequencies as a way to estimate probabilities is an example of maximum likelihood\\nestimation or MLE. In MLE, the resulting parameter set maximizes the likelihood\\nof the training set T given the model M (i.e., P(T|M)). For example, suppose the\\nword Chinese occurs 400 times in a corpus of a million words like the Brown corpus.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 37, '_split_overlap': [{'doc_id': '7bdf7562bc6462d5f4b6255c2e01c0c6', 'range': (0, 136)}, {'doc_id': 'e5aaa65024818e7cb81c985fc8957048', 'range': (341, 566)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e03e64ed89fc6765322f62b6428090c3'}>,\n",
       " <Document: {'content': 'In MLE, the resulting parameter set maximizes the likelihood\\nof the training set T given the model M (i.e., P(T|M)). For example, suppose the\\nword Chinese occurs 400 times in a corpus of a million words like the Brown corpus.\\nWhat is the probability that a random word selected from some other text of, say,\\na million words will be the word Chinese? The MLE of its probability is\\n400\\n1000000\\nor .0004. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 38, '_split_overlap': [{'doc_id': 'e03e64ed89fc6765322f62b6428090c3', 'range': (0, 225)}, {'doc_id': 'c666db90a0f8bc9b1ccabac9169a31c3', 'range': (226, 401)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e5aaa65024818e7cb81c985fc8957048'}>,\n",
       " <Document: {'content': 'What is the probability that a random word selected from some other text of, say,\\na million words will be the word Chinese? The MLE of its probability is\\n400\\n1000000\\nor .0004. Now .0004 is not the best possible estimate of the probability of Chinese\\noccurring in all situations; it might turn out that in some other corpus or context\\nChinese is a very unlikely word. But it is the probability that makes it most likely\\nthat Chinese will occur 400 times in a million-word corpus. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 39, '_split_overlap': [{'doc_id': 'e5aaa65024818e7cb81c985fc8957048', 'range': (0, 175)}, {'doc_id': '766a7b55a025b4d8fe981baf34f48e81', 'range': (176, 478)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c666db90a0f8bc9b1ccabac9169a31c3'}>,\n",
       " <Document: {'content': 'Now .0004 is not the best possible estimate of the probability of Chinese\\noccurring in all situations; it might turn out that in some other corpus or context\\nChinese is a very unlikely word. But it is the probability that makes it most likely\\nthat Chinese will occur 400 times in a million-word corpus. We present ways to\\nmodify the MLE estimates slightly to get better probability estimates in Section 3.4.\\nLet’s move on to some examples from a slightly larger corpus than our 14-word\\nexample above. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 40, '_split_overlap': [{'doc_id': 'c666db90a0f8bc9b1ccabac9169a31c3', 'range': (0, 302)}, {'doc_id': '8fdb34690133c971f651befbe2761aac', 'range': (303, 500)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '766a7b55a025b4d8fe981baf34f48e81'}>,\n",
       " <Document: {'content': 'We present ways to\\nmodify the MLE estimates slightly to get better probability estimates in Section 3.4.\\nLet’s move on to some examples from a slightly larger corpus than our 14-word\\nexample above. We’ll use data from the now-defunct Berkeley Restaurant Project,\\na dialogue system from the last century that answered questions about a database\\nof restaurants in Berkeley, California (Jurafsky et al., 1994). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 41, '_split_overlap': [{'doc_id': '766a7b55a025b4d8fe981baf34f48e81', 'range': (0, 197)}, {'doc_id': 'b3e3c38d92e1766e14a43287ca1223ac', 'range': (198, 407)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8fdb34690133c971f651befbe2761aac'}>,\n",
       " <Document: {'content': 'We’ll use data from the now-defunct Berkeley Restaurant Project,\\na dialogue system from the last century that answered questions about a database\\nof restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some text-\\nnormalized sample user queries (a sample of 9332 sentences is on the website):\\ncan you tell me about any good cantonese restaurants close by\\nmid priced thai food is what i’m looking for\\ntell me about chez panisse\\ncan you give me a listing of the kinds of food that are available\\ni’m looking for a good place to eat breakfast\\nwhen is caffe venezia open during the day\\nFigure 3.1 shows the bigram counts from a piece of a bigram grammar from the\\nBerkeley Restaurant Project. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 42, '_split_overlap': [{'doc_id': '8fdb34690133c971f651befbe2761aac', 'range': (0, 209)}, {'doc_id': '5a7568f8e829014484e90d7ae5ac7b1e', 'range': (210, 702)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b3e3c38d92e1766e14a43287ca1223ac'}>,\n",
       " <Document: {'content': 'Here are some text-\\nnormalized sample user queries (a sample of 9332 sentences is on the website):\\ncan you tell me about any good cantonese restaurants close by\\nmid priced thai food is what i’m looking for\\ntell me about chez panisse\\ncan you give me a listing of the kinds of food that are available\\ni’m looking for a good place to eat breakfast\\nwhen is caffe venezia open during the day\\nFigure 3.1 shows the bigram counts from a piece of a bigram grammar from the\\nBerkeley Restaurant Project. Note that the majority of the values are zero. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 43, '_split_overlap': [{'doc_id': 'b3e3c38d92e1766e14a43287ca1223ac', 'range': (0, 492)}, {'doc_id': 'ab1d348f7924b9a39e8ca6fe2c5bc8fe', 'range': (493, 539)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5a7568f8e829014484e90d7ae5ac7b1e'}>,\n",
       " <Document: {'content': 'Note that the majority of the values are zero. In fact,\\nwe have chosen the sample words to cohere with each other; a matrix selected from\\na random set of seven words would be even more sparse.\\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\\nin Fig. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 44, '_split_overlap': [{'doc_id': '5a7568f8e829014484e90d7ae5ac7b1e', 'range': (0, 46)}, {'doc_id': 'e9692a38e7c927f27d1d20ccf9cbdda4', 'range': (47, 282)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ab1d348f7924b9a39e8ca6fe2c5bc8fe'}>,\n",
       " <Document: {'content': 'In fact,\\nwe have chosen the sample words to cohere with each other; a matrix selected from\\na random set of seven words would be even more sparse.\\nFigure 3.2 shows the bigram probabilities after normalization (dividing each cell\\nin Fig. 3.1 by the appropriate unigram for its row, taken from the following set of\\nunigram probabilities):\\ni\\nwant to\\neat\\nchinese food lunch spend\\n2533 927\\n2417 746 158\\n1093 341\\n278\\nHere are a few other useful probabilities:\\nP(i|<s>) = 0.25\\nP(english|want) = 0.0011\\nP(food|english) = 0.5\\nP(</s>|food) = 0.68\\nNow we can compute the probability of sentences like I want English food or\\nI want Chinese food by simply multiplying the appropriate bigram probabilities to-\\ngether, as follows:\\x0c6\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\ni\\nwant\\nto\\neat\\nchinese\\nfood\\nlunch\\nspend\\ni\\n5\\n827\\n0\\n9\\n0\\n0\\n0\\n2\\nwant\\n2\\n0\\n608\\n1\\n6\\n6\\n5\\n1\\nto\\n2\\n0\\n4\\n686\\n2\\n0\\n6\\n211\\neat\\n0\\n0\\n2\\n0\\n16\\n2\\n42\\n0\\nchinese\\n1\\n0\\n0\\n0\\n0\\n82\\n1\\n0\\nfood\\n15\\n0\\n15\\n0\\n1\\n4\\n0\\n0\\nlunch\\n2\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\nspend\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\nFigure 3.1\\nBigram counts for eight of the words (out of V = 1446) in the Berkeley Restau-\\nrant Project corpus of 9332 sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 45, '_split_overlap': [{'doc_id': 'ab1d348f7924b9a39e8ca6fe2c5bc8fe', 'range': (0, 235)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e9692a38e7c927f27d1d20ccf9cbdda4'}>,\n",
       " <Document: {'content': 'Zero counts are in gray.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 46, '_split_overlap': []}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ece0dda59e83ad3be2f92cf1badce64'}>,\n",
       " <Document: {'content': 'i\\nwant\\nto\\neat\\nchinese\\nfood\\nlunch\\nspend\\ni\\n0.002\\n0.33\\n0\\n0.0036\\n0\\n0\\n0\\n0.00079\\nwant\\n0.0022\\n0\\n0.66\\n0.0011\\n0.0065\\n0.0065\\n0.0054\\n0.0011\\nto\\n0.00083\\n0\\n0.0017\\n0.28\\n0.00083\\n0\\n0.0025\\n0.087\\neat\\n0\\n0\\n0.0027\\n0\\n0.021\\n0.0027\\n0.056\\n0\\nchinese\\n0.0063\\n0\\n0\\n0\\n0\\n0.52\\n0.0063\\n0\\nfood\\n0.014\\n0\\n0.014\\n0\\n0.00092\\n0.0037\\n0\\n0\\nlunch\\n0.0059\\n0\\n0\\n0\\n0\\n0.0029\\n0\\n0\\nspend\\n0.0036\\n0\\n0.0036\\n0\\n0\\n0\\n0\\n0\\nFigure 3.2\\nBigram probabilities for eight words in the Berkeley Restaurant Project corpus\\nof 9332 sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 47, '_split_overlap': []}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c6bbd985c039209ce7271997cdcb0da7'}>,\n",
       " <Document: {'content': 'Zero probabilities are in gray.\\nP(<s> i want english food </s>)\\n= P(i|<s>)P(want|i)P(english|want)\\nP(food|english)P(</s>|food)\\n= .25×.33×.0011×0.5×0.68\\n= .000031\\nWe leave it as Exercise 3.2 to compute the probability of i want chinese food.\\nWhat kinds of linguistic phenomena are captured in these bigram statistics?\\nSome of the bigram probabilities above encode some facts that we think of as strictly\\nsyntactic in nature, like the fact that what comes after eat is usually a noun or an\\nadjective, or that what comes after to is usually a verb. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 48, '_split_overlap': [{'doc_id': '27b04908d8ae92b05df8b5e6cdc43ef4', 'range': (317, 545)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b89d2dc1ef027d3650fd3fc80b2fe822'}>,\n",
       " <Document: {'content': 'Some of the bigram probabilities above encode some facts that we think of as strictly\\nsyntactic in nature, like the fact that what comes after eat is usually a noun or an\\nadjective, or that what comes after to is usually a verb. Others might be a fact about\\nthe personal assistant task, like the high probability of sentences beginning with\\nthe words I. And some might even be cultural rather than linguistic, like the higher\\nprobability that people are looking for Chinese versus English food.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 49, '_split_overlap': [{'doc_id': 'b89d2dc1ef027d3650fd3fc80b2fe822', 'range': (0, 228)}, {'doc_id': '7c27f96101781bcff9ad45c57c3ffbb2', 'range': (229, 494)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '27b04908d8ae92b05df8b5e6cdc43ef4'}>,\n",
       " <Document: {'content': 'Others might be a fact about\\nthe personal assistant task, like the high probability of sentences beginning with\\nthe words I. And some might even be cultural rather than linguistic, like the higher\\nprobability that people are looking for Chinese versus English food.\\nSome practical issues:\\nAlthough for pedagogical purposes we have only described\\nbigram models, in practice it’s more common to use trigram models, which con-\\ntrigram\\ndition on the previous two words rather than the previous word, or 4-gram or even\\n4-gram\\n5-gram models, when there is sufﬁcient training data. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 50, '_split_overlap': [{'doc_id': '27b04908d8ae92b05df8b5e6cdc43ef4', 'range': (0, 265)}, {'doc_id': 'a456f97ae71464571a701a65d76295e7', 'range': (266, 574)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c27f96101781bcff9ad45c57c3ffbb2'}>,\n",
       " <Document: {'content': 'Some practical issues:\\nAlthough for pedagogical purposes we have only described\\nbigram models, in practice it’s more common to use trigram models, which con-\\ntrigram\\ndition on the previous two words rather than the previous word, or 4-gram or even\\n4-gram\\n5-gram models, when there is sufﬁcient training data. Note that for these larger n-\\n5-gram\\ngrams, we’ll need to assume extra context for the contexts to the left and right of the\\nsentence end. For example, to compute trigram probabilities at the very beginning of\\nthe sentence, we can use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 51, '_split_overlap': [{'doc_id': '7c27f96101781bcff9ad45c57c3ffbb2', 'range': (0, 308)}, {'doc_id': '59353deea1f11e95841cc541118cb9c5', 'range': (309, 601)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a456f97ae71464571a701a65d76295e7'}>,\n",
       " <Document: {'content': 'Note that for these larger n-\\n5-gram\\ngrams, we’ll need to assume extra context for the contexts to the left and right of the\\nsentence end. For example, to compute trigram probabilities at the very beginning of\\nthe sentence, we can use two pseudo-words for the ﬁrst trigram (i.e., P(I|<s><s>).\\nWe always represent and compute language model probabilities in log format\\nas log probabilities. Since probabilities are (by deﬁnition) less than or equal to\\nlog\\nprobabilities\\n1, the more probabilities we multiply together, the smaller the product becomes.\\nMultiplying enough n-grams together would result in numerical underﬂow. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 52, '_split_overlap': [{'doc_id': 'a456f97ae71464571a701a65d76295e7', 'range': (0, 292)}, {'doc_id': '56272338f7331c50f29fb61c1eb69bd0', 'range': (390, 621)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '59353deea1f11e95841cc541118cb9c5'}>,\n",
       " <Document: {'content': 'Since probabilities are (by deﬁnition) less than or equal to\\nlog\\nprobabilities\\n1, the more probabilities we multiply together, the smaller the product becomes.\\nMultiplying enough n-grams together would result in numerical underﬂow. By using\\nlog probabilities instead of raw probabilities, we get numbers that are not as small.\\x0c3.2\\n•\\nEVALUATING LANGUAGE MODELS\\n7\\nAdding in log space is equivalent to multiplying in linear space, so we combine log\\nprobabilities by adding them. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 53, '_split_overlap': [{'doc_id': '59353deea1f11e95841cc541118cb9c5', 'range': (0, 231)}, {'doc_id': '2fde4ae1d1c3b9fad65ada678d0e25f', 'range': (232, 475)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '56272338f7331c50f29fb61c1eb69bd0'}>,\n",
       " <Document: {'content': 'By using\\nlog probabilities instead of raw probabilities, we get numbers that are not as small.\\x0c3.2\\n•\\nEVALUATING LANGUAGE MODELS\\n7\\nAdding in log space is equivalent to multiplying in linear space, so we combine log\\nprobabilities by adding them. The result of doing all computation and storage in log\\nspace is that we only need to convert back into probabilities if we need to report\\nthem at the end; then we can just take the exp of the logprob:\\np1 × p2 × p3 × p4 = exp(log p1 +log p2 +log p3 +log p4)\\n(3.13)\\n3.2\\nEvaluating Language Models\\nThe best way to evaluate the performance of a language model is to embed it in\\nan application and measure how much the application improves. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 54, '_split_overlap': [{'doc_id': '56272338f7331c50f29fb61c1eb69bd0', 'range': (0, 243)}, {'doc_id': 'be90813c0094bea633dd55f82f762c67', 'range': (244, 679)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2fde4ae1d1c3b9fad65ada678d0e25f'}>,\n",
       " <Document: {'content': 'The result of doing all computation and storage in log\\nspace is that we only need to convert back into probabilities if we need to report\\nthem at the end; then we can just take the exp of the logprob:\\np1 × p2 × p3 × p4 = exp(log p1 +log p2 +log p3 +log p4)\\n(3.13)\\n3.2\\nEvaluating Language Models\\nThe best way to evaluate the performance of a language model is to embed it in\\nan application and measure how much the application improves. Such end-to-end\\nevaluation is called extrinsic evaluation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 55, '_split_overlap': [{'doc_id': '2fde4ae1d1c3b9fad65ada678d0e25f', 'range': (0, 435)}, {'doc_id': '16ba863b85d5c9f5a42e255dde5cdc6d', 'range': (436, 494)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be90813c0094bea633dd55f82f762c67'}>,\n",
       " <Document: {'content': 'Such end-to-end\\nevaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to\\nextrinsic\\nevaluation\\nknow if a particular improvement in a component is really going to help the task\\nat hand. Thus, for speech recognition, we can compare the performance of two\\nlanguage models by running the speech recognizer twice, once with each language\\nmodel, and seeing which gives the more accurate transcription.\\nUnfortunately, running big NLP systems end-to-end is often very expensive. In-\\nstead, it would be nice to have a metric that can be used to quickly evaluate potential\\nimprovements in a language model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 56, '_split_overlap': [{'doc_id': 'be90813c0094bea633dd55f82f762c67', 'range': (0, 58)}, {'doc_id': '15835cdc9a6355cd088aca69b1af99e1', 'range': (421, 621)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '16ba863b85d5c9f5a42e255dde5cdc6d'}>,\n",
       " <Document: {'content': 'Unfortunately, running big NLP systems end-to-end is often very expensive. In-\\nstead, it would be nice to have a metric that can be used to quickly evaluate potential\\nimprovements in a language model. An intrinsic evaluation metric is one that mea-\\nintrinsic\\nevaluation\\nsures the quality of a model independent of any application.\\nFor an intrinsic evaluation of a language model we need a test set. As with many\\nof the statistical models in our ﬁeld, the probabilities of an n-gram model come from\\nthe corpus it is trained on, the training set or training corpus. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 57, '_split_overlap': [{'doc_id': '16ba863b85d5c9f5a42e255dde5cdc6d', 'range': (0, 200)}, {'doc_id': '436458a6f0034726ad3279df5327d2a3', 'range': (399, 563)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '15835cdc9a6355cd088aca69b1af99e1'}>,\n",
       " <Document: {'content': 'As with many\\nof the statistical models in our ﬁeld, the probabilities of an n-gram model come from\\nthe corpus it is trained on, the training set or training corpus. We can then measure\\ntraining set\\nthe quality of an n-gram model by its performance on some unseen data called the\\ntest set or test corpus. We will also sometimes call test sets and other datasets that\\ntest set\\nare not in our training sets held out corpora because we hold them out from the\\nheld out\\ntraining data.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 58, '_split_overlap': [{'doc_id': '15835cdc9a6355cd088aca69b1af99e1', 'range': (0, 164)}, {'doc_id': 'e853eacb7460fb806f857f80da629eea', 'range': (304, 478)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '436458a6f0034726ad3279df5327d2a3'}>,\n",
       " <Document: {'content': 'We will also sometimes call test sets and other datasets that\\ntest set\\nare not in our training sets held out corpora because we hold them out from the\\nheld out\\ntraining data.\\nSo if we are given a corpus of text and want to compare two different n-gram\\nmodels, we divide the data into training and test sets, train the parameters of both\\nmodels on the training set, and then compare how well the two trained models ﬁt the\\ntest set.\\nBut what does it mean to “ﬁt the test set”? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 59, '_split_overlap': [{'doc_id': '436458a6f0034726ad3279df5327d2a3', 'range': (0, 174)}, {'doc_id': '292c246cc0aa5ebb0a4772bfa18ffffe', 'range': (175, 474)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e853eacb7460fb806f857f80da629eea'}>,\n",
       " <Document: {'content': 'So if we are given a corpus of text and want to compare two different n-gram\\nmodels, we divide the data into training and test sets, train the parameters of both\\nmodels on the training set, and then compare how well the two trained models ﬁt the\\ntest set.\\nBut what does it mean to “ﬁt the test set”? The answer is simple: whichever\\nmodel assigns a higher probability to the test set—meaning it more accurately\\npredicts the test set—is a better model. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 60, '_split_overlap': [{'doc_id': 'e853eacb7460fb806f857f80da629eea', 'range': (0, 299)}, {'doc_id': 'b95e6dafbc3fa7b8aac20d8576fe2614', 'range': (256, 450)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '292c246cc0aa5ebb0a4772bfa18ffffe'}>,\n",
       " <Document: {'content': 'But what does it mean to “ﬁt the test set”? The answer is simple: whichever\\nmodel assigns a higher probability to the test set—meaning it more accurately\\npredicts the test set—is a better model. Given two probabilistic models, the better\\nmodel is the one that has a tighter ﬁt to the test data or that better predicts the details\\nof the test data, and hence will assign a higher probability to the test data.\\nSince our evaluation metric is based on test set probability, it’s important not to\\nlet the test sentences into the training set. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 61, '_split_overlap': [{'doc_id': '292c246cc0aa5ebb0a4772bfa18ffffe', 'range': (0, 194)}, {'doc_id': '5750199a3b21a32da2173acfcdbd9849', 'range': (195, 538)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b95e6dafbc3fa7b8aac20d8576fe2614'}>,\n",
       " <Document: {'content': 'Given two probabilistic models, the better\\nmodel is the one that has a tighter ﬁt to the test data or that better predicts the details\\nof the test data, and hence will assign a higher probability to the test data.\\nSince our evaluation metric is based on test set probability, it’s important not to\\nlet the test sentences into the training set. Suppose we are trying to compute the\\nprobability of a particular “test” sentence. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 62, '_split_overlap': [{'doc_id': 'b95e6dafbc3fa7b8aac20d8576fe2614', 'range': (0, 343)}, {'doc_id': '89399ad7209b81063e3a332a28fb7d35', 'range': (214, 425)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5750199a3b21a32da2173acfcdbd9849'}>,\n",
       " <Document: {'content': 'Since our evaluation metric is based on test set probability, it’s important not to\\nlet the test sentences into the training set. Suppose we are trying to compute the\\nprobability of a particular “test” sentence. If our test sentence is part of the training\\ncorpus, we will mistakenly assign it an artiﬁcially high probability when it occurs\\nin the test set. We call this situation training on the test set. Training on the test\\nset introduces a bias that makes the probabilities all look too high, and causes huge\\ninaccuracies in perplexity, the probability-based metric we introduce below.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 63, '_split_overlap': [{'doc_id': '5750199a3b21a32da2173acfcdbd9849', 'range': (0, 211)}, {'doc_id': '2a28b7ecd43bac311237dfd1614e1522', 'range': (358, 590)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '89399ad7209b81063e3a332a28fb7d35'}>,\n",
       " <Document: {'content': 'We call this situation training on the test set. Training on the test\\nset introduces a bias that makes the probabilities all look too high, and causes huge\\ninaccuracies in perplexity, the probability-based metric we introduce below.\\nSometimes we use a particular test set so often that we implicitly tune to its\\ncharacteristics. We then need a fresh test set that is truly unseen. In such cases, we\\ncall the initial test set the development test set or, devset. How do we divide our\\ndevelopment\\ntest\\ndata into training, development, and test sets? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 64, '_split_overlap': [{'doc_id': '89399ad7209b81063e3a332a28fb7d35', 'range': (0, 232)}, {'doc_id': 'c37bca396420dc4ef0917ebc0dc9cfcb', 'range': (329, 547)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2a28b7ecd43bac311237dfd1614e1522'}>,\n",
       " <Document: {'content': 'We then need a fresh test set that is truly unseen. In such cases, we\\ncall the initial test set the development test set or, devset. How do we divide our\\ndevelopment\\ntest\\ndata into training, development, and test sets? We want our test set to be as large\\nas possible, since a small test set may be accidentally unrepresentative, but we also\\nwant as much training data as possible. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 65, '_split_overlap': [{'doc_id': '2a28b7ecd43bac311237dfd1614e1522', 'range': (0, 218)}, {'doc_id': '47504a2c57fe55f44a691a7616c94675', 'range': (219, 380)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c37bca396420dc4ef0917ebc0dc9cfcb'}>,\n",
       " <Document: {'content': 'We want our test set to be as large\\nas possible, since a small test set may be accidentally unrepresentative, but we also\\nwant as much training data as possible. At the minimum, we would want to pick\\x0c8\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nthe smallest test set that gives us enough statistical power to measure a statistically\\nsigniﬁcant difference between two potential models. In practice, we often just divide\\nour data into 80% training, 10% development, and 10% test. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 66, '_split_overlap': [{'doc_id': 'c37bca396420dc4ef0917ebc0dc9cfcb', 'range': (0, 161)}, {'doc_id': 'ead085780c6e191d9187f8bf2fd575d4', 'range': (162, 469)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47504a2c57fe55f44a691a7616c94675'}>,\n",
       " <Document: {'content': 'At the minimum, we would want to pick\\x0c8\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nthe smallest test set that gives us enough statistical power to measure a statistically\\nsigniﬁcant difference between two potential models. In practice, we often just divide\\nour data into 80% training, 10% development, and 10% test. Given a large corpus\\nthat we want to divide into training and test, test data can either be taken from some\\ncontinuous sequence of text inside the corpus, or we can remove smaller “stripes”\\nof text from randomly selected parts of our corpus and combine them into a test set.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 67, '_split_overlap': [{'doc_id': '47504a2c57fe55f44a691a7616c94675', 'range': (0, 307)}, {'doc_id': '12408c959a67ef72e027c472823ba3c7', 'range': (308, 582)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ead085780c6e191d9187f8bf2fd575d4'}>,\n",
       " <Document: {'content': 'Given a large corpus\\nthat we want to divide into training and test, test data can either be taken from some\\ncontinuous sequence of text inside the corpus, or we can remove smaller “stripes”\\nof text from randomly selected parts of our corpus and combine them into a test set.\\n3.2.1\\nPerplexity\\nIn practice we don’t use raw probability as our metric for evaluating language mod-\\nels, but a variant called perplexity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 68, '_split_overlap': [{'doc_id': 'ead085780c6e191d9187f8bf2fd575d4', 'range': (0, 274)}, {'doc_id': '1d88463a8962fd79871e16b86b2b18cb', 'range': (275, 413)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '12408c959a67ef72e027c472823ba3c7'}>,\n",
       " <Document: {'content': '3.2.1\\nPerplexity\\nIn practice we don’t use raw probability as our metric for evaluating language mod-\\nels, but a variant called perplexity. The perplexity (sometimes called PP for short)\\nperplexity\\nof a language model on a test set is the inverse probability of the test set, normalized\\nby the number of words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 69, '_split_overlap': [{'doc_id': '12408c959a67ef72e027c472823ba3c7', 'range': (0, 138)}, {'doc_id': '22c03522af6727fb3eb8b7bd79ca6fab', 'range': (139, 309)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1d88463a8962fd79871e16b86b2b18cb'}>,\n",
       " <Document: {'content': 'The perplexity (sometimes called PP for short)\\nperplexity\\nof a language model on a test set is the inverse probability of the test set, normalized\\nby the number of words. For a test set W = w1w2 ...wN,:\\nPP(W) = P(w1w2 ...wN)− 1\\nN\\n(3.14)\\n=\\nN\\n�\\n1\\nP(w1w2 ...wN)\\nWe can use the chain rule to expand the probability of W:\\nPP(W) =\\nN\\n�\\n�\\n�\\n�\\nN\\n�\\ni=1\\n1\\nP(wi|w1 ...wi−1)\\n(3.15)\\nThus, if we are computing the perplexity of W with a bigram language model,\\nwe get:\\nPP(W) =\\nN\\n�\\n�\\n�\\n�\\nN\\n�\\ni=1\\n1\\nP(wi|wi−1)\\n(3.16)\\nNote that because of the inverse in Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 70, '_split_overlap': [{'doc_id': '1d88463a8962fd79871e16b86b2b18cb', 'range': (0, 170)}, {'doc_id': '4c06f4641af046d8f8156153666800d6', 'range': (171, 538)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22c03522af6727fb3eb8b7bd79ca6fab'}>,\n",
       " <Document: {'content': 'For a test set W = w1w2 ...wN,:\\nPP(W) = P(w1w2 ...wN)− 1\\nN\\n(3.14)\\n=\\nN\\n�\\n1\\nP(w1w2 ...wN)\\nWe can use the chain rule to expand the probability of W:\\nPP(W) =\\nN\\n�\\n�\\n�\\n�\\nN\\n�\\ni=1\\n1\\nP(wi|w1 ...wi−1)\\n(3.15)\\nThus, if we are computing the perplexity of W with a bigram language model,\\nwe get:\\nPP(W) =\\nN\\n�\\n�\\n�\\n�\\nN\\n�\\ni=1\\n1\\nP(wi|wi−1)\\n(3.16)\\nNote that because of the inverse in Eq. 3.15, the higher the conditional probabil-\\nity of the word sequence, the lower the perplexity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 71, '_split_overlap': [{'doc_id': '22c03522af6727fb3eb8b7bd79ca6fab', 'range': (0, 367)}, {'doc_id': '9e55f941cdedd3d86134befa97d74c91', 'range': (368, 462)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c06f4641af046d8f8156153666800d6'}>,\n",
       " <Document: {'content': '3.15, the higher the conditional probabil-\\nity of the word sequence, the lower the perplexity. Thus, minimizing perplexity is\\nequivalent to maximizing the test set probability according to the language model.\\nWhat we generally use for word sequence in Eq. 3.15 or Eq. 3.16 is the entire se-\\nquence of words in some test set. Since this sequence will cross many sentence\\nboundaries, we need to include the begin- and end-sentence markers <s> and </s>\\nin the probability computation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 72, '_split_overlap': [{'doc_id': '4c06f4641af046d8f8156153666800d6', 'range': (0, 94)}, {'doc_id': '2bed0641f4d4940312adc6cab90e476e', 'range': (268, 481)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e55f941cdedd3d86134befa97d74c91'}>,\n",
       " <Document: {'content': '3.16 is the entire se-\\nquence of words in some test set. Since this sequence will cross many sentence\\nboundaries, we need to include the begin- and end-sentence markers <s> and </s>\\nin the probability computation. We also need to include the end-of-sentence marker\\n</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-\\nkens N.\\nThere is another way to think about perplexity: as the weighted average branch-\\ning factor of a language. The branching factor of a language is the number of possi-\\nble next words that can follow any word. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 73, '_split_overlap': [{'doc_id': '9e55f941cdedd3d86134befa97d74c91', 'range': (0, 213)}, {'doc_id': '28065675eed142188c5862b8645e934e', 'range': (214, 561)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2bed0641f4d4940312adc6cab90e476e'}>,\n",
       " <Document: {'content': 'We also need to include the end-of-sentence marker\\n</s> (but not the beginning-of-sentence marker <s>) in the total count of word to-\\nkens N.\\nThere is another way to think about perplexity: as the weighted average branch-\\ning factor of a language. The branching factor of a language is the number of possi-\\nble next words that can follow any word. Consider the task of recognizing the digits\\nin English (zero, one, two,..., nine), given that (both in some training set and in some\\ntest set) each of the 10 digits occurs with equal probability P = 1\\n10. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 74, '_split_overlap': [{'doc_id': '2bed0641f4d4940312adc6cab90e476e', 'range': (0, 347)}, {'doc_id': 'b6ab8081e06acf74f99de52029915e47', 'range': (348, 552)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '28065675eed142188c5862b8645e934e'}>,\n",
       " <Document: {'content': 'Consider the task of recognizing the digits\\nin English (zero, one, two,..., nine), given that (both in some training set and in some\\ntest set) each of the 10 digits occurs with equal probability P = 1\\n10. The perplexity of\\nthis mini-language is in fact 10. To see that, imagine a test string of digits of length\\nN, and assume that in the training set all the digits occurred with equal probability.\\nBy Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 75, '_split_overlap': [{'doc_id': '28065675eed142188c5862b8645e934e', 'range': (0, 204)}, {'doc_id': '3d318840ccb70617280620847ed1f8c6', 'range': (205, 405)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b6ab8081e06acf74f99de52029915e47'}>,\n",
       " <Document: {'content': 'The perplexity of\\nthis mini-language is in fact 10. To see that, imagine a test string of digits of length\\nN, and assume that in the training set all the digits occurred with equal probability.\\nBy Eq. 3.15, the perplexity will be\\x0c3.3\\n•\\nGENERALIZATION AND ZEROS\\n9\\nPP(W) = P(w1w2 ...wN)− 1\\nN\\n= ( 1\\n10\\nN\\n)− 1\\nN\\n=\\n1\\n10\\n−1\\n= 10\\n(3.17)\\nBut suppose that the number zero is really frequent and occurs far more often\\nthan other numbers. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 76, '_split_overlap': [{'doc_id': 'b6ab8081e06acf74f99de52029915e47', 'range': (0, 200)}, {'doc_id': '22250769ab7d88abd2634a36df32c58c', 'range': (201, 427)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3d318840ccb70617280620847ed1f8c6'}>,\n",
       " <Document: {'content': '3.15, the perplexity will be\\x0c3.3\\n•\\nGENERALIZATION AND ZEROS\\n9\\nPP(W) = P(w1w2 ...wN)− 1\\nN\\n= ( 1\\n10\\nN\\n)− 1\\nN\\n=\\n1\\n10\\n−1\\n= 10\\n(3.17)\\nBut suppose that the number zero is really frequent and occurs far more often\\nthan other numbers. Let’s say that 0 occur 91 times in the training set, and each\\nof the other digits occurred 1 time each. Now we see the following test set: 0 0\\n0 0 0 3 0 0 0 0. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 77, '_split_overlap': [{'doc_id': '3d318840ccb70617280620847ed1f8c6', 'range': (0, 226)}, {'doc_id': '669504d03f22c8d27e26bc68e183af92', 'range': (227, 386)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '22250769ab7d88abd2634a36df32c58c'}>,\n",
       " <Document: {'content': 'Let’s say that 0 occur 91 times in the training set, and each\\nof the other digits occurred 1 time each. Now we see the following test set: 0 0\\n0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since\\nmost of the time the next number will be zero, which is very predictable, i.e. has\\na high probability. Thus, although the branching factor is still 10, the perplexity or\\nweighted branching factor is smaller. We leave this exact calculation as exercise 12.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 78, '_split_overlap': [{'doc_id': '22250769ab7d88abd2634a36df32c58c', 'range': (0, 159)}, {'doc_id': 'b88393f9eab256d44b9abeb1d716e197', 'range': (160, 482)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '669504d03f22c8d27e26bc68e183af92'}>,\n",
       " <Document: {'content': 'We should expect the perplexity of this test set to be lower since\\nmost of the time the next number will be zero, which is very predictable, i.e. has\\na high probability. Thus, although the branching factor is still 10, the perplexity or\\nweighted branching factor is smaller. We leave this exact calculation as exercise 12.\\nWe see in Section 3.7 that perplexity is also closely related to the information-\\ntheoretic notion of entropy.\\nFinally, let’s look at an example of how perplexity can be used to compare dif-\\nferent n-gram models. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 79, '_split_overlap': [{'doc_id': '669504d03f22c8d27e26bc68e183af92', 'range': (0, 322)}, {'doc_id': '12c7f34c389df2d01d9c0cfcc7fd6ed6', 'range': (323, 535)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b88393f9eab256d44b9abeb1d716e197'}>,\n",
       " <Document: {'content': 'We see in Section 3.7 that perplexity is also closely related to the information-\\ntheoretic notion of entropy.\\nFinally, let’s look at an example of how perplexity can be used to compare dif-\\nferent n-gram models. We trained unigram, bigram, and trigram grammars on 38\\nmillion words (including start-of-sentence tokens) from the Wall Street Journal, us-\\ning a 19,979 word vocabulary. We then computed the perplexity of each of these\\nmodels on a test set of 1.5 million words with Eq. 3.16. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 80, '_split_overlap': [{'doc_id': 'b88393f9eab256d44b9abeb1d716e197', 'range': (0, 212)}, {'doc_id': 'c18399da52fd3c8c29a3a579b1af36a2', 'range': (213, 488)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '12c7f34c389df2d01d9c0cfcc7fd6ed6'}>,\n",
       " <Document: {'content': 'We trained unigram, bigram, and trigram grammars on 38\\nmillion words (including start-of-sentence tokens) from the Wall Street Journal, us-\\ning a 19,979 word vocabulary. We then computed the perplexity of each of these\\nmodels on a test set of 1.5 million words with Eq. 3.16. The table below shows the\\nperplexity of a 1.5 million word WSJ test set according to each of these grammars.\\nUnigram Bigram Trigram\\nPerplexity 962\\n170\\n109\\nAs we see above, the more information the n-gram gives us about the word\\nsequence, the lower the perplexity (since as Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 81, '_split_overlap': [{'doc_id': '12c7f34c389df2d01d9c0cfcc7fd6ed6', 'range': (0, 275)}, {'doc_id': 'b32e8c4c50105e3943a44afd5a2a83b6', 'range': (276, 552)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c18399da52fd3c8c29a3a579b1af36a2'}>,\n",
       " <Document: {'content': 'The table below shows the\\nperplexity of a 1.5 million word WSJ test set according to each of these grammars.\\nUnigram Bigram Trigram\\nPerplexity 962\\n170\\n109\\nAs we see above, the more information the n-gram gives us about the word\\nsequence, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related\\ninversely to the likelihood of the test sequence according to the model).\\nNote that in computing perplexities, the n-gram model P must be constructed\\nwithout any knowledge of the test set or any prior knowledge of the vocabulary of\\nthe test set. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 82, '_split_overlap': [{'doc_id': 'c18399da52fd3c8c29a3a579b1af36a2', 'range': (0, 276)}, {'doc_id': '64dafc0c67ac874330bdcb0d4cac690d', 'range': (386, 557)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b32e8c4c50105e3943a44afd5a2a83b6'}>,\n",
       " <Document: {'content': 'Note that in computing perplexities, the n-gram model P must be constructed\\nwithout any knowledge of the test set or any prior knowledge of the vocabulary of\\nthe test set. Any kind of knowledge of the test set can cause the perplexity to be\\nartiﬁcially low. The perplexity of two language models is only comparable if they\\nuse identical vocabularies.\\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\\nprovement in the performance of a language processing task like speech recognition\\nor machine translation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 83, '_split_overlap': [{'doc_id': 'b32e8c4c50105e3943a44afd5a2a83b6', 'range': (0, 171)}, {'doc_id': 'cc1c2f0238ae81e0feeaccb76bfe5e54', 'range': (258, 536)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '64dafc0c67ac874330bdcb0d4cac690d'}>,\n",
       " <Document: {'content': 'The perplexity of two language models is only comparable if they\\nuse identical vocabularies.\\nAn (intrinsic) improvement in perplexity does not guarantee an (extrinsic) im-\\nprovement in the performance of a language processing task like speech recognition\\nor machine translation. Nonetheless, because perplexity often correlates with such\\nimprovements, it is commonly used as a quick check on an algorithm. But a model’s\\nimprovement in perplexity should always be conﬁrmed by an end-to-end evaluation\\nof a real task before concluding the evaluation of the model.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 84, '_split_overlap': [{'doc_id': '64dafc0c67ac874330bdcb0d4cac690d', 'range': (0, 278)}, {'doc_id': '3e67fc48a60ff3dd6fa6aaa4b5d067a7', 'range': (279, 561)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc1c2f0238ae81e0feeaccb76bfe5e54'}>,\n",
       " <Document: {'content': 'Nonetheless, because perplexity often correlates with such\\nimprovements, it is commonly used as a quick check on an algorithm. But a model’s\\nimprovement in perplexity should always be conﬁrmed by an end-to-end evaluation\\nof a real task before concluding the evaluation of the model.\\n3.3\\nGeneralization and Zeros\\nThe n-gram model, like many statistical models, is dependent on the training corpus.\\nOne implication of this is that the probabilities often encode speciﬁc facts about a\\ngiven training corpus. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 85, '_split_overlap': [{'doc_id': 'cc1c2f0238ae81e0feeaccb76bfe5e54', 'range': (0, 282)}, {'doc_id': 'e52867566e985e80d255e688bae8208d', 'range': (283, 504)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '3e67fc48a60ff3dd6fa6aaa4b5d067a7'}>,\n",
       " <Document: {'content': '3.3\\nGeneralization and Zeros\\nThe n-gram model, like many statistical models, is dependent on the training corpus.\\nOne implication of this is that the probabilities often encode speciﬁc facts about a\\ngiven training corpus. Another implication is that n-grams do a better and better job\\nof modeling the training corpus as we increase the value of N.\\x0c10\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nWe can visualize both of these facts by borrowing the technique of Shannon\\n(1951) and Miller and Selfridge (1950) of generating random sentences from dif-\\nferent n-gram models. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 86, '_split_overlap': [{'doc_id': '3e67fc48a60ff3dd6fa6aaa4b5d067a7', 'range': (0, 221)}, {'doc_id': '4f94c469a6aaab9e5136a2f07f03335f', 'range': (222, 562)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e52867566e985e80d255e688bae8208d'}>,\n",
       " <Document: {'content': 'Another implication is that n-grams do a better and better job\\nof modeling the training corpus as we increase the value of N.\\x0c10\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nWe can visualize both of these facts by borrowing the technique of Shannon\\n(1951) and Miller and Selfridge (1950) of generating random sentences from dif-\\nferent n-gram models. It’s simplest to visualize how this works for the unigram\\ncase. Imagine all the words of the English language covering the probability space\\nbetween 0 and 1, each word covering an interval proportional to its frequency. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 87, '_split_overlap': [{'doc_id': 'e52867566e985e80d255e688bae8208d', 'range': (0, 340)}, {'doc_id': '7d456ab0801fb0f37a8fda36acfe32d', 'range': (341, 560)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f94c469a6aaab9e5136a2f07f03335f'}>,\n",
       " <Document: {'content': 'It’s simplest to visualize how this works for the unigram\\ncase. Imagine all the words of the English language covering the probability space\\nbetween 0 and 1, each word covering an interval proportional to its frequency. We\\nchoose a random value between 0 and 1 and print the word whose interval includes\\nthis chosen value. We continue choosing random numbers and generating words\\nuntil we randomly generate the sentence-ﬁnal token </s>. We can use the same\\ntechnique to generate bigrams by ﬁrst generating a random bigram that starts with\\n<s> (according to its bigram probability). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 88, '_split_overlap': [{'doc_id': '4f94c469a6aaab9e5136a2f07f03335f', 'range': (0, 219)}, {'doc_id': '620693efb5b5d740694c976fb8b94f32', 'range': (323, 581)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7d456ab0801fb0f37a8fda36acfe32d'}>,\n",
       " <Document: {'content': 'We continue choosing random numbers and generating words\\nuntil we randomly generate the sentence-ﬁnal token </s>. We can use the same\\ntechnique to generate bigrams by ﬁrst generating a random bigram that starts with\\n<s> (according to its bigram probability). Let’s say the second word of that bigram\\nis w. We next chose a random bigram starting with w (again, drawn according to its\\nbigram probability), and so on.\\nTo give an intuition for the increasing power of higher-order n-grams, Fig. 3.3\\nshows random sentences generated from unigram, bigram, trigram, and 4-gram\\nmodels trained on Shakespeare’s works.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 89, '_split_overlap': [{'doc_id': '7d456ab0801fb0f37a8fda36acfe32d', 'range': (0, 258)}, {'doc_id': '7782d50c19b425bcd25f54a638243be1', 'range': (259, 608)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '620693efb5b5d740694c976fb8b94f32'}>,\n",
       " <Document: {'content': 'Let’s say the second word of that bigram\\nis w. We next chose a random bigram starting with w (again, drawn according to its\\nbigram probability), and so on.\\nTo give an intuition for the increasing power of higher-order n-grams, Fig. 3.3\\nshows random sentences generated from unigram, bigram, trigram, and 4-gram\\nmodels trained on Shakespeare’s works.\\n1\\n–To him swallowed confess hear both. Which. Of save on trail for are ay device and\\nrote life have\\ngram\\n–Hill he late speaks; or! ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 90, '_split_overlap': [{'doc_id': '620693efb5b5d740694c976fb8b94f32', 'range': (0, 349)}, {'doc_id': '4c0e8a1614a0deac006ff7391113b7fc', 'range': (232, 480)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7782d50c19b425bcd25f54a638243be1'}>,\n",
       " <Document: {'content': '3.3\\nshows random sentences generated from unigram, bigram, trigram, and 4-gram\\nmodels trained on Shakespeare’s works.\\n1\\n–To him swallowed confess hear both. Which. Of save on trail for are ay device and\\nrote life have\\ngram\\n–Hill he late speaks; or! a more to leg less ﬁrst you enter\\n2\\n–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\\nking. Follow.\\ngram\\n–What means, sir. I confess she? then all sorts, he is trim, captain.\\n3\\n–Fly, and will rid me these news of price. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 91, '_split_overlap': [{'doc_id': '7782d50c19b425bcd25f54a638243be1', 'range': (0, 248)}, {'doc_id': '107c21a15ed5c0a5b26966e925a89f40', 'range': (249, 507)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4c0e8a1614a0deac006ff7391113b7fc'}>,\n",
       " <Document: {'content': 'a more to leg less ﬁrst you enter\\n2\\n–Why dost stand forth thy canopy, forsooth; he is this palpable hit the King Henry. Live\\nking. Follow.\\ngram\\n–What means, sir. I confess she? then all sorts, he is trim, captain.\\n3\\n–Fly, and will rid me these news of price. Therefore the sadness of parting, as they say,\\n’tis done.\\ngram\\n–This shall forbid it should be branded, if renown made it empty.\\n4\\n–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 92, '_split_overlap': [{'doc_id': '4c0e8a1614a0deac006ff7391113b7fc', 'range': (0, 258)}, {'doc_id': '812ad50101bf7ad38f65dfb69cf8b383', 'range': (259, 473)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '107c21a15ed5c0a5b26966e925a89f40'}>,\n",
       " <Document: {'content': 'Therefore the sadness of parting, as they say,\\n’tis done.\\ngram\\n–This shall forbid it should be branded, if renown made it empty.\\n4\\n–King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A\\ngreat banquet serv’d in;\\ngram\\n–It cannot be but so.\\nFigure 3.3\\nEight sentences randomly generated from four n-grams computed from Shakespeare’s works. All\\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\\nfor capitalization to improve readability.\\nThe longer the context on which we train the model, the more coherent the sen-\\ntences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 93, '_split_overlap': [{'doc_id': '107c21a15ed5c0a5b26966e925a89f40', 'range': (0, 214)}, {'doc_id': 'f4c1ba316edba0d1e491339b808c77bd', 'range': (368, 608)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '812ad50101bf7ad38f65dfb69cf8b383'}>,\n",
       " <Document: {'content': 'All\\ncharacters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected\\nfor capitalization to improve readability.\\nThe longer the context on which we train the model, the more coherent the sen-\\ntences. In the unigram sentences, there is no coherent relation between words or any\\nsentence-ﬁnal punctuation. The bigram sentences have some local word-to-word\\ncoherence (especially if we consider that punctuation counts as a word). The tri-\\ngram and 4-gram sentences are beginning to look a lot like Shakespeare. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 94, '_split_overlap': [{'doc_id': '812ad50101bf7ad38f65dfb69cf8b383', 'range': (0, 240)}, {'doc_id': '31bd0c4a3bf52266224f5bfe63535bfa', 'range': (345, 548)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f4c1ba316edba0d1e491339b808c77bd'}>,\n",
       " <Document: {'content': 'The bigram sentences have some local word-to-word\\ncoherence (especially if we consider that punctuation counts as a word). The tri-\\ngram and 4-gram sentences are beginning to look a lot like Shakespeare. Indeed, a\\ncareful investigation of the 4-gram sentences shows that they look a little too much\\nlike Shakespeare. The words It cannot be but so are directly from King John. This is\\nbecause, not to put the knock on Shakespeare, his oeuvre is not very large as corpora\\ngo (N = 884,647,V = 29,066), and our n-gram probability matrices are ridiculously\\nsparse. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 95, '_split_overlap': [{'doc_id': 'f4c1ba316edba0d1e491339b808c77bd', 'range': (0, 203)}, {'doc_id': '9f76551ff314e3381386861d653d1d8d', 'range': (376, 559)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '31bd0c4a3bf52266224f5bfe63535bfa'}>,\n",
       " <Document: {'content': 'This is\\nbecause, not to put the knock on Shakespeare, his oeuvre is not very large as corpora\\ngo (N = 884,647,V = 29,066), and our n-gram probability matrices are ridiculously\\nsparse. There are V 2 = 844,000,000 possible bigrams alone, and the number of pos-\\nsible 4-grams is V 4 = 7×1017. Thus, once the generator has chosen the ﬁrst 4-gram\\n(It cannot be but), there are only ﬁve possible continuations (that, I, he, thou, and\\nso); indeed, for many 4-grams, there is only one continuation.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 96, '_split_overlap': [{'doc_id': '31bd0c4a3bf52266224f5bfe63535bfa', 'range': (0, 183)}, {'doc_id': '396ba635b09a81e9ea1a2df61a2075bf', 'range': (290, 490)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f76551ff314e3381386861d653d1d8d'}>,\n",
       " <Document: {'content': 'Thus, once the generator has chosen the ﬁrst 4-gram\\n(It cannot be but), there are only ﬁve possible continuations (that, I, he, thou, and\\nso); indeed, for many 4-grams, there is only one continuation.\\nTo get an idea of the dependence of a grammar on its training set, let’s look at an\\nn-gram grammar trained on a completely different corpus: the Wall Street Journal\\n(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so\\nwe might expect some overlap between our n-grams for the two genres. Fig. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 97, '_split_overlap': [{'doc_id': '9f76551ff314e3381386861d653d1d8d', 'range': (0, 200)}, {'doc_id': '7905673f080cfd47ab06a38eaea352c1', 'range': (201, 517)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '396ba635b09a81e9ea1a2df61a2075bf'}>,\n",
       " <Document: {'content': 'To get an idea of the dependence of a grammar on its training set, let’s look at an\\nn-gram grammar trained on a completely different corpus: the Wall Street Journal\\n(WSJ) newspaper. Shakespeare and the Wall Street Journal are both English, so\\nwe might expect some overlap between our n-grams for the two genres. Fig. 3.4\\x0c3.3\\n•\\nGENERALIZATION AND ZEROS\\n11\\nshows sentences generated by unigram, bigram, and trigram grammars trained on\\n40 million words from WSJ.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 98, '_split_overlap': [{'doc_id': '396ba635b09a81e9ea1a2df61a2075bf', 'range': (0, 316)}, {'doc_id': '17bd5c37d9afbab6e48169b5557c974a', 'range': (182, 459)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7905673f080cfd47ab06a38eaea352c1'}>,\n",
       " <Document: {'content': 'Shakespeare and the Wall Street Journal are both English, so\\nwe might expect some overlap between our n-grams for the two genres. Fig. 3.4\\x0c3.3\\n•\\nGENERALIZATION AND ZEROS\\n11\\nshows sentences generated by unigram, bigram, and trigram grammars trained on\\n40 million words from WSJ.\\n1\\nMonths the my and issue of year foreign new exchange’s september\\ngram were recession exchange new endorsed a acquire to six executives\\n2\\nLast December through the way to preserve the Hudson corporation N.\\nB. E. C. Taylor would seem to complete the major central planners one\\ngram\\npoint ﬁve percent of U. S. E. has already old M. X. corporation of living\\non information such as more frequently ﬁshing to keep her\\n3\\nThey also point to ninety nine point six billion dollars from two hundred\\nfour oh six three percent of the rates of interest stores as Mexico and\\ngram\\nBrazil on market conditions\\nFigure 3.4\\nThree sentences randomly generated from three n-gram models computed from\\n40 million words of the Wall Street Journal, lower-casing all characters and treating punctua-\\ntion as words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 99, '_split_overlap': [{'doc_id': '7905673f080cfd47ab06a38eaea352c1', 'range': (0, 277)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '17bd5c37d9afbab6e48169b5557c974a'}>,\n",
       " <Document: {'content': 'Output was then hand-corrected for capitalization to improve readability.\\nCompare these examples to the pseudo-Shakespeare in Fig. 3.3. While they both\\nmodel “English-like sentences”, there is clearly no overlap in generated sentences,\\nand little overlap even in small phrases. Statistical models are likely to be pretty use-\\nless as predictors if the training sets and the test sets are as different as Shakespeare\\nand WSJ.\\nHow should we deal with this problem when we build n-gram models? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 100, '_split_overlap': [{'doc_id': '396af3364beda6a69dbc94577edac1a7', 'range': (278, 490)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ee09b399aedebb86ad7ff86d4722f17a'}>,\n",
       " <Document: {'content': 'Statistical models are likely to be pretty use-\\nless as predictors if the training sets and the test sets are as different as Shakespeare\\nand WSJ.\\nHow should we deal with this problem when we build n-gram models? One step\\nis to be sure to use a training corpus that has a similar genre to whatever task we are\\ntrying to accomplish. To build a language model for translating legal documents,\\nwe need a training corpus of legal documents. To build a language model for a\\nquestion-answering system, we need a training corpus of questions.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 101, '_split_overlap': [{'doc_id': 'ee09b399aedebb86ad7ff86d4722f17a', 'range': (0, 212)}, {'doc_id': 'd5407839af51fc25e2c1e42e5f708aa8', 'range': (332, 535)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '396af3364beda6a69dbc94577edac1a7'}>,\n",
       " <Document: {'content': 'To build a language model for translating legal documents,\\nwe need a training corpus of legal documents. To build a language model for a\\nquestion-answering system, we need a training corpus of questions.\\nIt is equally important to get training data in the appropriate dialect, especially\\nwhen processing social media posts or spoken transcripts. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 102, '_split_overlap': [{'doc_id': '396af3364beda6a69dbc94577edac1a7', 'range': (0, 203)}, {'doc_id': 'c7c333453347f229ef7560b63847aa2b', 'range': (105, 345)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd5407839af51fc25e2c1e42e5f708aa8'}>,\n",
       " <Document: {'content': 'To build a language model for a\\nquestion-answering system, we need a training corpus of questions.\\nIt is equally important to get training data in the appropriate dialect, especially\\nwhen processing social media posts or spoken transcripts. Thus tweets in AAVE\\n(African American Vernacular English) often use words like ﬁnna—an auxiliary\\nverb that marks immediate future tense —that don’t occur in other dialects, or\\nspellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):\\n(3.18) Bored af den my phone ﬁnna die!!!\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 103, '_split_overlap': [{'doc_id': 'd5407839af51fc25e2c1e42e5f708aa8', 'range': (0, 240)}, {'doc_id': 'dd87f3b33ff48408ad02ea152b5f18cc', 'range': (241, 541)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c7c333453347f229ef7560b63847aa2b'}>,\n",
       " <Document: {'content': 'Thus tweets in AAVE\\n(African American Vernacular English) often use words like ﬁnna—an auxiliary\\nverb that marks immediate future tense —that don’t occur in other dialects, or\\nspellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):\\n(3.18) Bored af den my phone ﬁnna die!!!\\nwhile tweets from varieties like Nigerian English have markedly different vocabu-\\nlary and n-gram patterns from American English (Jurgens et al., 2017):\\n(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\\ntweet, nyt gan u dey tweet. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 104, '_split_overlap': [{'doc_id': 'c7c333453347f229ef7560b63847aa2b', 'range': (0, 300)}, {'doc_id': '7e0e5b5946109ef23cf0d63e6550fa75', 'range': (301, 564)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dd87f3b33ff48408ad02ea152b5f18cc'}>,\n",
       " <Document: {'content': 'while tweets from varieties like Nigerian English have markedly different vocabu-\\nlary and n-gram patterns from American English (Jurgens et al., 2017):\\n(3.19) @username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u\\ntweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter\\nMatching genres and dialects is still not sufﬁcient. Our models may still be\\nsubject to the problem of sparsity. For any n-gram that occurred a sufﬁcient number\\nof times, we might have a good estimate of its probability. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 105, '_split_overlap': [{'doc_id': 'dd87f3b33ff48408ad02ea152b5f18cc', 'range': (0, 263)}, {'doc_id': 'ae73b410f76bbdcaddf9692c88b08826', 'range': (354, 521)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e0e5b5946109ef23cf0d63e6550fa75'}>,\n",
       " <Document: {'content': 'Our models may still be\\nsubject to the problem of sparsity. For any n-gram that occurred a sufﬁcient number\\nof times, we might have a good estimate of its probability. But because any corpus is\\nlimited, some perfectly acceptable English word sequences are bound to be missing\\nfrom it. That is, we’ll have many cases of putative “zero probability n-grams” that\\nshould really have some non-zero probability. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 106, '_split_overlap': [{'doc_id': '7e0e5b5946109ef23cf0d63e6550fa75', 'range': (0, 167)}, {'doc_id': '8a8b45195d1a8166cc1010e834ceb4dd', 'range': (168, 405)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ae73b410f76bbdcaddf9692c88b08826'}>,\n",
       " <Document: {'content': 'But because any corpus is\\nlimited, some perfectly acceptable English word sequences are bound to be missing\\nfrom it. That is, we’ll have many cases of putative “zero probability n-grams” that\\nshould really have some non-zero probability. Consider the words that follow the\\nbigram denied the in the WSJ Treebank3 corpus, together with their counts:\\ndenied the allegations:\\n5\\ndenied the speculation: 2\\ndenied the rumors:\\n1\\ndenied the report:\\n1\\nBut suppose our test set has phrases like:\\x0c12\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\ndenied the offer\\ndenied the loan\\nOur model will incorrectly estimate that the P(offer|denied the) is 0!\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 107, '_split_overlap': [{'doc_id': 'ae73b410f76bbdcaddf9692c88b08826', 'range': (0, 237)}, {'doc_id': '4544428a04363fc5c6899df79131a6ae', 'range': (238, 626)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8a8b45195d1a8166cc1010e834ceb4dd'}>,\n",
       " <Document: {'content': 'Consider the words that follow the\\nbigram denied the in the WSJ Treebank3 corpus, together with their counts:\\ndenied the allegations:\\n5\\ndenied the speculation: 2\\ndenied the rumors:\\n1\\ndenied the report:\\n1\\nBut suppose our test set has phrases like:\\x0c12\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\ndenied the offer\\ndenied the loan\\nOur model will incorrectly estimate that the P(offer|denied the) is 0!\\nThese zeros— things that don’t ever occur in the training set but do occur in\\nzeros\\nthe test set—are a problem for two reasons. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 108, '_split_overlap': [{'doc_id': '8a8b45195d1a8166cc1010e834ceb4dd', 'range': (0, 388)}, {'doc_id': '157b49fe42e2f8ff93f7ecd3ccbc12df', 'range': (389, 516)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4544428a04363fc5c6899df79131a6ae'}>,\n",
       " <Document: {'content': 'These zeros— things that don’t ever occur in the training set but do occur in\\nzeros\\nthe test set—are a problem for two reasons. First, their presence means we are\\nunderestimating the probability of all sorts of words that might occur, which will\\nhurt the performance of any application we want to run on this data.\\nSecond, if the probability of any word in the test set is 0, the entire probability\\nof the test set is 0. By deﬁnition, perplexity is based on the inverse probability of the\\ntest set. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 109, '_split_overlap': [{'doc_id': '4544428a04363fc5c6899df79131a6ae', 'range': (0, 127)}, {'doc_id': '65496bf43bd28c2700d95f0fd8854b89', 'range': (315, 498)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '157b49fe42e2f8ff93f7ecd3ccbc12df'}>,\n",
       " <Document: {'content': 'Second, if the probability of any word in the test set is 0, the entire probability\\nof the test set is 0. By deﬁnition, perplexity is based on the inverse probability of the\\ntest set. Thus if some words have zero probability, we can’t compute perplexity at\\nall, since we can’t divide by 0!\\n3.3.1\\nUnknown Words\\nThe previous section discussed the problem of words whose bigram probability is\\nzero. But what about words we simply have never seen before?\\nSometimes we have a language task in which this can’t happen because we know\\nall the words that can occur. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 110, '_split_overlap': [{'doc_id': '157b49fe42e2f8ff93f7ecd3ccbc12df', 'range': (0, 183)}, {'doc_id': '63ef4d5eda0f27855ef745e5f4b13c9d', 'range': (396, 557)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '65496bf43bd28c2700d95f0fd8854b89'}>,\n",
       " <Document: {'content': 'But what about words we simply have never seen before?\\nSometimes we have a language task in which this can’t happen because we know\\nall the words that can occur. In such a closed vocabulary system the test set can\\nclosed\\nvocabulary\\nonly contain words from this lexicon, and there will be no unknown words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 111, '_split_overlap': [{'doc_id': '65496bf43bd28c2700d95f0fd8854b89', 'range': (0, 161)}, {'doc_id': '7cef53c74b296093e2cf9efa8f28d650', 'range': (55, 305)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '63ef4d5eda0f27855ef745e5f4b13c9d'}>,\n",
       " <Document: {'content': 'Sometimes we have a language task in which this can’t happen because we know\\nall the words that can occur. In such a closed vocabulary system the test set can\\nclosed\\nvocabulary\\nonly contain words from this lexicon, and there will be no unknown words. This is\\na reasonable assumption in some domains, such as speech recognition or machine\\ntranslation, where we have a pronunciation dictionary or a phrase table that are ﬁxed\\nin advance, and so the language model can only use the words in that dictionary or\\nphrase table.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 112, '_split_overlap': [{'doc_id': '63ef4d5eda0f27855ef745e5f4b13c9d', 'range': (0, 250)}, {'doc_id': '14a50d268b0a5d4e93a0945ccc917e6e', 'range': (251, 520)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7cef53c74b296093e2cf9efa8f28d650'}>,\n",
       " <Document: {'content': 'This is\\na reasonable assumption in some domains, such as speech recognition or machine\\ntranslation, where we have a pronunciation dictionary or a phrase table that are ﬁxed\\nin advance, and so the language model can only use the words in that dictionary or\\nphrase table.\\nIn other cases we have to deal with words we haven’t seen before, which we’ll\\ncall unknown words, or out of vocabulary (OOV) words. The percentage of OOV\\nOOV\\nwords that appear in the test set is called the OOV rate. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 113, '_split_overlap': [{'doc_id': '7cef53c74b296093e2cf9efa8f28d650', 'range': (0, 269)}, {'doc_id': 'f84cb29c5e696006bef50b3b9cc09b75', 'range': (270, 485)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '14a50d268b0a5d4e93a0945ccc917e6e'}>,\n",
       " <Document: {'content': 'In other cases we have to deal with words we haven’t seen before, which we’ll\\ncall unknown words, or out of vocabulary (OOV) words. The percentage of OOV\\nOOV\\nwords that appear in the test set is called the OOV rate. An open vocabulary system\\nopen\\nvocabulary\\nis one in which we model these potential unknown words in the test set by adding a\\npseudo-word called <UNK>.\\nThere are two common ways to train the probabilities of the unknown word\\nmodel <UNK>. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 114, '_split_overlap': [{'doc_id': '14a50d268b0a5d4e93a0945ccc917e6e', 'range': (0, 215)}, {'doc_id': 'd143c52e666c89321ce97652e4616a81', 'range': (216, 452)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f84cb29c5e696006bef50b3b9cc09b75'}>,\n",
       " <Document: {'content': 'An open vocabulary system\\nopen\\nvocabulary\\nis one in which we model these potential unknown words in the test set by adding a\\npseudo-word called <UNK>.\\nThere are two common ways to train the probabilities of the unknown word\\nmodel <UNK>. The ﬁrst one is to turn the problem back into a closed vocabulary one\\nby choosing a ﬁxed vocabulary in advance:\\n1. Choose a vocabulary (word list) that is ﬁxed in advance.\\n2. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 115, '_split_overlap': [{'doc_id': 'f84cb29c5e696006bef50b3b9cc09b75', 'range': (0, 236)}, {'doc_id': '6e7d85b5cee4afa363df651ea66d54ff', 'range': (237, 411)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd143c52e666c89321ce97652e4616a81'}>,\n",
       " <Document: {'content': 'The ﬁrst one is to turn the problem back into a closed vocabulary one\\nby choosing a ﬁxed vocabulary in advance:\\n1. Choose a vocabulary (word list) that is ﬁxed in advance.\\n2. Convert in the training set any word that is not in this set (any OOV word) to\\nthe unknown word token <UNK> in a text normalization step.\\n3. Estimate the probabilities for <UNK> from its counts just like any other regular\\nword in the training set.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 116, '_split_overlap': [{'doc_id': 'd143c52e666c89321ce97652e4616a81', 'range': (0, 174)}, {'doc_id': '5725b086959cd9eb7b04f673b6e41f28', 'range': (175, 422)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6e7d85b5cee4afa363df651ea66d54ff'}>,\n",
       " <Document: {'content': 'Convert in the training set any word that is not in this set (any OOV word) to\\nthe unknown word token <UNK> in a text normalization step.\\n3. Estimate the probabilities for <UNK> from its counts just like any other regular\\nword in the training set.\\nThe second alternative, in situations where we don’t have a prior vocabulary in ad-\\nvance, is to create such a vocabulary implicitly, replacing words in the training data\\nby <UNK> based on their frequency. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 117, '_split_overlap': [{'doc_id': '6e7d85b5cee4afa363df651ea66d54ff', 'range': (0, 247)}, {'doc_id': 'cc192c2f83d49c6e1db9a506d6c6d154', 'range': (248, 453)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5725b086959cd9eb7b04f673b6e41f28'}>,\n",
       " <Document: {'content': 'The second alternative, in situations where we don’t have a prior vocabulary in ad-\\nvance, is to create such a vocabulary implicitly, replacing words in the training data\\nby <UNK> based on their frequency. For example we can replace by <UNK> all words\\nthat occur fewer than n times in the training set, where n is some small number, or\\nequivalently select a vocabulary size V in advance (say 50,000) and choose the top\\nV words by frequency and replace the rest by UNK. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 118, '_split_overlap': [{'doc_id': '5725b086959cd9eb7b04f673b6e41f28', 'range': (0, 205)}, {'doc_id': '1b17060e52c9af0cfb9e86dbdf6813ca', 'range': (206, 468)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cc192c2f83d49c6e1db9a506d6c6d154'}>,\n",
       " <Document: {'content': 'For example we can replace by <UNK> all words\\nthat occur fewer than n times in the training set, where n is some small number, or\\nequivalently select a vocabulary size V in advance (say 50,000) and choose the top\\nV words by frequency and replace the rest by UNK. In either case we then proceed\\nto train the language model as before, treating <UNK> like a regular word.\\nThe exact choice of <UNK> model does have an effect on metrics like perplexity.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 119, '_split_overlap': [{'doc_id': 'cc192c2f83d49c6e1db9a506d6c6d154', 'range': (0, 262)}, {'doc_id': '13ff23f1b3847b37aa0dca4792d6620a', 'range': (263, 448)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1b17060e52c9af0cfb9e86dbdf6813ca'}>,\n",
       " <Document: {'content': 'In either case we then proceed\\nto train the language model as before, treating <UNK> like a regular word.\\nThe exact choice of <UNK> model does have an effect on metrics like perplexity.\\nA language model can achieve low perplexity by choosing a small vocabulary and\\nassigning the unknown word a high probability. For this reason, perplexities should\\nonly be compared across language models with the same vocabularies (Buck et al.,\\n2014).\\x0c', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 120, '_split_overlap': [{'doc_id': '1b17060e52c9af0cfb9e86dbdf6813ca', 'range': (0, 185)}, {'doc_id': '5d05819ff14c24695f0a1a2f44c17c35', 'range': (186, 436)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '13ff23f1b3847b37aa0dca4792d6620a'}>,\n",
       " <Document: {'content': 'A language model can achieve low perplexity by choosing a small vocabulary and\\nassigning the unknown word a high probability. For this reason, perplexities should\\nonly be compared across language models with the same vocabularies (Buck et al.,\\n2014).\\x0c3.4\\n•\\nSMOOTHING\\n13\\n3.4\\nSmoothing\\nWhat do we do with words that are in our vocabulary (they are not unknown words)\\nbut appear in a test set in an unseen context (for example they appear after a word\\nthey never appeared after in training)? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 121, '_split_overlap': [{'doc_id': '13ff23f1b3847b37aa0dca4792d6620a', 'range': (0, 250)}, {'doc_id': '43cddd0024e5896ceb0f27763bf47be5', 'range': (251, 488)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5d05819ff14c24695f0a1a2f44c17c35'}>,\n",
       " <Document: {'content': '3.4\\n•\\nSMOOTHING\\n13\\n3.4\\nSmoothing\\nWhat do we do with words that are in our vocabulary (they are not unknown words)\\nbut appear in a test set in an unseen context (for example they appear after a word\\nthey never appeared after in training)? To keep a language model from assigning\\nzero probability to these unseen events, we’ll have to shave off a bit of probability\\nmass from some more frequent events and give it to the events we’ve never seen.\\nThis modiﬁcation is called smoothing or discounting. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 122, '_split_overlap': [{'doc_id': '5d05819ff14c24695f0a1a2f44c17c35', 'range': (0, 237)}, {'doc_id': '987988651de9e122bf41a0358775c51c', 'range': (238, 496)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '43cddd0024e5896ceb0f27763bf47be5'}>,\n",
       " <Document: {'content': 'To keep a language model from assigning\\nzero probability to these unseen events, we’ll have to shave off a bit of probability\\nmass from some more frequent events and give it to the events we’ve never seen.\\nThis modiﬁcation is called smoothing or discounting. In this section and the fol-\\nsmoothing\\ndiscounting\\nlowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing,\\nadd-k smoothing, stupid backoff, and Kneser-Ney smoothing.\\n3.4.1\\nLaplace Smoothing\\nThe simplest way to do smoothing is to add one to all the bigram counts, before\\nwe normalize them into probabilities. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 123, '_split_overlap': [{'doc_id': '43cddd0024e5896ceb0f27763bf47be5', 'range': (0, 258)}, {'doc_id': '8865a4f38b2fbdfe44736671cbef663f', 'range': (259, 590)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '987988651de9e122bf41a0358775c51c'}>,\n",
       " <Document: {'content': 'In this section and the fol-\\nsmoothing\\ndiscounting\\nlowing ones we’ll introduce a variety of ways to do smoothing: add-1 smoothing,\\nadd-k smoothing, stupid backoff, and Kneser-Ney smoothing.\\n3.4.1\\nLaplace Smoothing\\nThe simplest way to do smoothing is to add one to all the bigram counts, before\\nwe normalize them into probabilities. All the counts that used to be zero will now\\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\\nLaplace smoothing. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 124, '_split_overlap': [{'doc_id': '987988651de9e122bf41a0358775c51c', 'range': (0, 331)}, {'doc_id': '4d507b1273088374b8b5ac29ed3cdfc4', 'range': (332, 477)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8865a4f38b2fbdfe44736671cbef663f'}>,\n",
       " <Document: {'content': 'All the counts that used to be zero will now\\nhave a count of 1, the counts of 1 will be 2, and so on. This algorithm is called\\nLaplace smoothing. Laplace smoothing does not perform well enough to be used\\nLaplace\\nsmoothing\\nin modern n-gram models, but it usefully introduces many of the concepts that we\\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\\nsmoothing algorithm for other tasks like text classiﬁcation (Chapter 4).\\nLet’s start with the application of Laplace smoothing to unigram probabilities.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 125, '_split_overlap': [{'doc_id': '8865a4f38b2fbdfe44736671cbef663f', 'range': (0, 145)}, {'doc_id': '7e6a37cc07102950ce346305792a1fd4', 'range': (146, 539)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d507b1273088374b8b5ac29ed3cdfc4'}>,\n",
       " <Document: {'content': 'Laplace smoothing does not perform well enough to be used\\nLaplace\\nsmoothing\\nin modern n-gram models, but it usefully introduces many of the concepts that we\\nsee in other smoothing algorithms, gives a useful baseline, and is also a practical\\nsmoothing algorithm for other tasks like text classiﬁcation (Chapter 4).\\nLet’s start with the application of Laplace smoothing to unigram probabilities.\\nRecall that the unsmoothed maximum likelihood estimate of the unigram probability\\nof the word wi is its count ci normalized by the total number of word tokens N:\\nP(wi) = ci\\nN\\nLaplace smoothing merely adds one to each count (hence its alternate name add-\\none smoothing). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 126, '_split_overlap': [{'doc_id': '4d507b1273088374b8b5ac29ed3cdfc4', 'range': (0, 393)}, {'doc_id': '93f8593385f3b3695179fc23bfe1848c', 'range': (394, 663)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e6a37cc07102950ce346305792a1fd4'}>,\n",
       " <Document: {'content': 'Recall that the unsmoothed maximum likelihood estimate of the unigram probability\\nof the word wi is its count ci normalized by the total number of word tokens N:\\nP(wi) = ci\\nN\\nLaplace smoothing merely adds one to each count (hence its alternate name add-\\none smoothing). Since there are V words in the vocabulary and each one was incre-\\nadd-one\\nmented, we also need to adjust the denominator to take into account the extra V\\nobservations. (What happens to our P values if we don’t increase the denominator?)\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 127, '_split_overlap': [{'doc_id': '7e6a37cc07102950ce346305792a1fd4', 'range': (0, 269)}, {'doc_id': 'b791cb53ad71d416300004e7dc5c9a98', 'range': (270, 506)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '93f8593385f3b3695179fc23bfe1848c'}>,\n",
       " <Document: {'content': 'Since there are V words in the vocabulary and each one was incre-\\nadd-one\\nmented, we also need to adjust the denominator to take into account the extra V\\nobservations. (What happens to our P values if we don’t increase the denominator?)\\nPLaplace(wi) = ci +1\\nN +V\\n(3.20)\\nInstead of changing both the numerator and denominator, it is convenient to\\ndescribe how a smoothing algorithm affects the numerator, by deﬁning an adjusted\\ncount c∗. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 128, '_split_overlap': [{'doc_id': '93f8593385f3b3695179fc23bfe1848c', 'range': (0, 236)}, {'doc_id': 'd37a626c5fcb5ccbd31ada067ce5d89e', 'range': (237, 436)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b791cb53ad71d416300004e7dc5c9a98'}>,\n",
       " <Document: {'content': 'PLaplace(wi) = ci +1\\nN +V\\n(3.20)\\nInstead of changing both the numerator and denominator, it is convenient to\\ndescribe how a smoothing algorithm affects the numerator, by deﬁning an adjusted\\ncount c∗. This adjusted count is easier to compare directly with the MLE counts and\\ncan be turned into a probability like an MLE count by normalizing by N. To deﬁne\\nthis count, since we are only changing the numerator in addition to adding 1 we’ll\\nalso need to multiply by a normalization factor\\nN\\nN+V :\\nc∗\\ni = (ci +1)\\nN\\nN +V\\n(3.21)\\nWe can now turn c∗\\ni into a probability P∗\\ni by normalizing by N.\\nA related way to view smoothing is as discounting (lowering) some non-zero\\ndiscounting\\ncounts in order to get the probability mass that will be assigned to the zero counts.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 129, '_split_overlap': [{'doc_id': 'b791cb53ad71d416300004e7dc5c9a98', 'range': (0, 199)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd37a626c5fcb5ccbd31ada067ce5d89e'}>,\n",
       " <Document: {'content': 'Thus, instead of referring to the discounted counts c∗, we might describe a smooth-\\ning algorithm in terms of a relative discount dc, the ratio of the discounted counts\\ndiscount\\nto the original counts:\\x0c14\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\ndc = c∗\\nc\\nNow that we have the intuition for the unigram case, let’s smooth our Berkeley\\nRestaurant Project bigrams. Figure 3.5 shows the add-one smoothed counts for the\\nbigrams in Fig. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 130, '_split_overlap': [{'doc_id': '4a8a0f4998272a72556d4d7a140836a7', 'range': (357, 425)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '15a73b57e1f4b4aa40722435e6636cf'}>,\n",
       " <Document: {'content': 'Figure 3.5 shows the add-one smoothed counts for the\\nbigrams in Fig. 3.1.\\ni\\nwant\\nto\\neat\\nchinese\\nfood\\nlunch\\nspend\\ni\\n6\\n828\\n1\\n10\\n1\\n1\\n1\\n3\\nwant\\n3\\n1\\n609\\n2\\n7\\n7\\n6\\n2\\nto\\n3\\n1\\n5\\n687\\n3\\n1\\n7\\n212\\neat\\n1\\n1\\n3\\n1\\n17\\n3\\n43\\n1\\nchinese\\n2\\n1\\n1\\n1\\n1\\n83\\n2\\n1\\nfood\\n16\\n1\\n16\\n1\\n2\\n5\\n1\\n1\\nlunch\\n3\\n1\\n1\\n1\\n1\\n2\\n1\\n1\\nspend\\n2\\n1\\n2\\n1\\n1\\n1\\n1\\n1\\nFigure 3.5\\nAdd-one smoothed bigram counts for eight of the words (out of V = 1446) in\\nthe Berkeley Restaurant Project corpus of 9332 sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 131, '_split_overlap': [{'doc_id': '15a73b57e1f4b4aa40722435e6636cf', 'range': (0, 68)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4a8a0f4998272a72556d4d7a140836a7'}>,\n",
       " <Document: {'content': 'Previously-zero counts are in gray.\\nFigure 3.6 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2.\\nRecall that normal bigram probabilities are computed by normalizing each row of\\ncounts by the unigram count:\\nP(wn|wn−1) = C(wn−1wn)\\nC(wn−1)\\n(3.22)\\nFor add-one smoothed bigram counts, we need to augment the unigram count by\\nthe number of total word types in the vocabulary V:\\nP∗\\nLaplace(wn|wn−1) =\\nC(wn−1wn)+1\\n�\\nw (C(wn−1w)+1) = C(wn−1wn)+1\\nC(wn−1)+V\\n(3.23)\\nThus, each of the unigram counts given in the previous section will need to be\\naugmented by V = 1446. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 132, '_split_overlap': [{'doc_id': 'f9312a1d18b5f621aca24b945b483ce4', 'range': (117, 575)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '85329263df7c96472b2bad9510d887af'}>,\n",
       " <Document: {'content': 'Recall that normal bigram probabilities are computed by normalizing each row of\\ncounts by the unigram count:\\nP(wn|wn−1) = C(wn−1wn)\\nC(wn−1)\\n(3.22)\\nFor add-one smoothed bigram counts, we need to augment the unigram count by\\nthe number of total word types in the vocabulary V:\\nP∗\\nLaplace(wn|wn−1) =\\nC(wn−1wn)+1\\n�\\nw (C(wn−1w)+1) = C(wn−1wn)+1\\nC(wn−1)+V\\n(3.23)\\nThus, each of the unigram counts given in the previous section will need to be\\naugmented by V = 1446. The result is the smoothed bigram probabilities in Fig. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 133, '_split_overlap': [{'doc_id': '85329263df7c96472b2bad9510d887af', 'range': (0, 458)}, {'doc_id': 'd389d550b182cc56dd8c7e64e760e533', 'range': (459, 514)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f9312a1d18b5f621aca24b945b483ce4'}>,\n",
       " <Document: {'content': 'The result is the smoothed bigram probabilities in Fig. 3.6.\\ni\\nwant\\nto\\neat\\nchinese\\nfood\\nlunch\\nspend\\ni\\n0.0015\\n0.21\\n0.00025\\n0.0025\\n0.00025\\n0.00025\\n0.00025\\n0.00075\\nwant\\n0.0013\\n0.00042\\n0.26\\n0.00084\\n0.0029\\n0.0029\\n0.0025\\n0.00084\\nto\\n0.00078\\n0.00026\\n0.0013\\n0.18\\n0.00078\\n0.00026\\n0.0018\\n0.055\\neat\\n0.00046\\n0.00046\\n0.0014\\n0.00046\\n0.0078\\n0.0014\\n0.02\\n0.00046\\nchinese\\n0.0012\\n0.00062\\n0.00062\\n0.00062\\n0.00062\\n0.052\\n0.0012\\n0.00062\\nfood\\n0.0063\\n0.00039\\n0.0063\\n0.00039\\n0.00079\\n0.002\\n0.00039\\n0.00039\\nlunch\\n0.0017\\n0.00056\\n0.00056\\n0.00056\\n0.00056\\n0.0011\\n0.00056\\n0.00056\\nspend\\n0.0012\\n0.00058\\n0.0012\\n0.00058\\n0.00058\\n0.00058\\n0.00058\\n0.00058\\nFigure 3.6\\nAdd-one smoothed bigram probabilities for eight of the words (out of V = 1446) in the BeRP\\ncorpus of 9332 sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 134, '_split_overlap': [{'doc_id': 'f9312a1d18b5f621aca24b945b483ce4', 'range': (0, 55)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd389d550b182cc56dd8c7e64e760e533'}>,\n",
       " <Document: {'content': 'Previously-zero probabilities are in gray.\\nIt is often convenient to reconstruct the count matrix so we can see how much a\\nsmoothing algorithm has changed the original counts. These adjusted counts can be\\ncomputed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 135, '_split_overlap': [{'doc_id': '62fd47f4a9d78f1ed5de7ebd5345d382', 'range': (43, 269)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b2234cb468b61dd319e891d7dd264472'}>,\n",
       " <Document: {'content': 'It is often convenient to reconstruct the count matrix so we can see how much a\\nsmoothing algorithm has changed the original counts. These adjusted counts can be\\ncomputed by Eq. 3.24. Figure 3.7 shows the reconstructed counts.\\nc∗(wn−1wn) = [C(wn−1wn)+1]×C(wn−1)\\nC(wn−1)+V\\n(3.24)\\x0c3.4\\n•\\nSMOOTHING\\n15\\ni\\nwant\\nto\\neat\\nchinese\\nfood\\nlunch\\nspend\\ni\\n3.8\\n527\\n0.64\\n6.4\\n0.64\\n0.64\\n0.64\\n1.9\\nwant\\n1.2\\n0.39\\n238\\n0.78\\n2.7\\n2.7\\n2.3\\n0.78\\nto\\n1.9\\n0.63\\n3.1\\n430\\n1.9\\n0.63\\n4.4\\n133\\neat\\n0.34\\n0.34\\n1\\n0.34\\n5.8\\n1\\n15\\n0.34\\nchinese\\n0.2\\n0.098\\n0.098\\n0.098\\n0.098\\n8.2\\n0.2\\n0.098\\nfood\\n6.9\\n0.43\\n6.9\\n0.43\\n0.86\\n2.2\\n0.43\\n0.43\\nlunch\\n0.57\\n0.19\\n0.19\\n0.19\\n0.19\\n0.38\\n0.19\\n0.19\\nspend\\n0.32\\n0.16\\n0.32\\n0.16\\n0.16\\n0.16\\n0.16\\n0.16\\nFigure 3.7\\nAdd-one reconstituted counts for eight words (of V = 1446) in the BeRP corpus\\nof 9332 sentences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 136, '_split_overlap': [{'doc_id': 'b2234cb468b61dd319e891d7dd264472', 'range': (0, 226)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62fd47f4a9d78f1ed5de7ebd5345d382'}>,\n",
       " <Document: {'content': 'Previously-zero counts are in gray.\\nNote that add-one smoothing has made a very big change to the counts. C(want to)\\nchanged from 609 to 238! We can see this in probability space as well: P(to|want)\\ndecreases from .66 in the unsmoothed case to .26 in the smoothed case. Looking at\\nthe discount d (the ratio between new and old counts) shows us how strikingly the\\ncounts for each preﬁx word have been reduced; the discount for the bigram want to\\nis .39, while the discount for Chinese food is .10, a factor of 10!\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 137, '_split_overlap': [{'doc_id': '64423b540d161e75cb3fb56e46e9b85c', 'range': (270, 512)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d4e5988447f649ca1237682a7d1c8f8'}>,\n",
       " <Document: {'content': 'Looking at\\nthe discount d (the ratio between new and old counts) shows us how strikingly the\\ncounts for each preﬁx word have been reduced; the discount for the bigram want to\\nis .39, while the discount for Chinese food is .10, a factor of 10!\\nThe sharp change in counts and probabilities occurs because too much probabil-\\nity mass is moved to all the zeros.\\n3.4.2\\nAdd-k smoothing\\nOne alternative to add-one smoothing is to move a bit less of the probability mass\\nfrom the seen to the unseen events. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 138, '_split_overlap': [{'doc_id': '2d4e5988447f649ca1237682a7d1c8f8', 'range': (0, 242)}, {'doc_id': '671eac2a8b9b7626d6bf8751e928b2eb', 'range': (243, 498)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '64423b540d161e75cb3fb56e46e9b85c'}>,\n",
       " <Document: {'content': 'The sharp change in counts and probabilities occurs because too much probabil-\\nity mass is moved to all the zeros.\\n3.4.2\\nAdd-k smoothing\\nOne alternative to add-one smoothing is to move a bit less of the probability mass\\nfrom the seen to the unseen events. Instead of adding 1 to each count, we add a frac-\\ntional count k (.5? .05? .01?). This algorithm is therefore called add-k smoothing.\\nadd-k\\nP∗\\nAdd-k(wn|wn−1) = C(wn−1wn)+k\\nC(wn−1)+kV\\n(3.25)\\nAdd-k smoothing requires that we have a method for choosing k; this can be\\ndone, for example, by optimizing on a devset. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 139, '_split_overlap': [{'doc_id': '64423b540d161e75cb3fb56e46e9b85c', 'range': (0, 255)}, {'doc_id': 'a573b4853b144e2f542f4c3ebaf87b14', 'range': (338, 566)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '671eac2a8b9b7626d6bf8751e928b2eb'}>,\n",
       " <Document: {'content': 'This algorithm is therefore called add-k smoothing.\\nadd-k\\nP∗\\nAdd-k(wn|wn−1) = C(wn−1wn)+k\\nC(wn−1)+kV\\n(3.25)\\nAdd-k smoothing requires that we have a method for choosing k; this can be\\ndone, for example, by optimizing on a devset. Although add-k is useful for some\\ntasks (including text classiﬁcation), it turns out that it still doesn’t work well for\\nlanguage modeling, generating counts with poor variances and often inappropriate\\ndiscounts (Gale and Church, 1994).\\n3.4.3\\nBackoff and Interpolation\\nThe discounting we have been discussing so far can help solve the problem of zero\\nfrequency n-grams. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 140, '_split_overlap': [{'doc_id': '671eac2a8b9b7626d6bf8751e928b2eb', 'range': (0, 228)}, {'doc_id': 'ee71cea5666b790cd15ece767688e217', 'range': (229, 598)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a573b4853b144e2f542f4c3ebaf87b14'}>,\n",
       " <Document: {'content': 'Although add-k is useful for some\\ntasks (including text classiﬁcation), it turns out that it still doesn’t work well for\\nlanguage modeling, generating counts with poor variances and often inappropriate\\ndiscounts (Gale and Church, 1994).\\n3.4.3\\nBackoff and Interpolation\\nThe discounting we have been discussing so far can help solve the problem of zero\\nfrequency n-grams. But there is an additional source of knowledge we can draw on.\\nIf we are trying to compute P(wn|wn−2wn−1) but we have no examples of a particular\\ntrigram wn−2wn−1wn, we can instead estimate its probability by using the bigram\\nprobability P(wn|wn−1). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 141, '_split_overlap': [{'doc_id': 'a573b4853b144e2f542f4c3ebaf87b14', 'range': (0, 369)}, {'doc_id': '346fd0ad6723763ae69906a87dda0d86', 'range': (370, 619)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ee71cea5666b790cd15ece767688e217'}>,\n",
       " <Document: {'content': 'But there is an additional source of knowledge we can draw on.\\nIf we are trying to compute P(wn|wn−2wn−1) but we have no examples of a particular\\ntrigram wn−2wn−1wn, we can instead estimate its probability by using the bigram\\nprobability P(wn|wn−1). Similarly, if we don’t have counts to compute P(wn|wn−1),\\nwe can look to the unigram P(wn).\\nIn other words, sometimes using less context is a good thing, helping to general-\\nize more for contexts that the model hasn’t learned much about. There are two ways\\nto use this n-gram “hierarchy”. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 142, '_split_overlap': [{'doc_id': 'ee71cea5666b790cd15ece767688e217', 'range': (0, 249)}, {'doc_id': '7c10f53dd91002a166ba43c198df98c5', 'range': (342, 538)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '346fd0ad6723763ae69906a87dda0d86'}>,\n",
       " <Document: {'content': 'In other words, sometimes using less context is a good thing, helping to general-\\nize more for contexts that the model hasn’t learned much about. There are two ways\\nto use this n-gram “hierarchy”. In backoff, we use the trigram if the evidence is\\nbackoff\\nsufﬁcient, otherwise we use the bigram, otherwise the unigram. In other words, we\\nonly “back off” to a lower-order n-gram if we have zero evidence for a higher-order\\nn-gram. By contrast, in interpolation, we always mix the probability estimates from\\ninterpolation\\nall the n-gram estimators, weighing and combining the trigram, bigram, and unigram\\ncounts.\\x0c', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 143, '_split_overlap': [{'doc_id': '346fd0ad6723763ae69906a87dda0d86', 'range': (0, 196)}, {'doc_id': '20e558db7bfa6eb91eaa59f010f08434', 'range': (318, 609)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7c10f53dd91002a166ba43c198df98c5'}>,\n",
       " <Document: {'content': 'In other words, we\\nonly “back off” to a lower-order n-gram if we have zero evidence for a higher-order\\nn-gram. By contrast, in interpolation, we always mix the probability estimates from\\ninterpolation\\nall the n-gram estimators, weighing and combining the trigram, bigram, and unigram\\ncounts.\\x0c16\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nIn simple linear interpolation, we combine different order n-grams by linearly in-\\nterpolating all the models. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 144, '_split_overlap': [{'doc_id': '7c10f53dd91002a166ba43c198df98c5', 'range': (0, 291)}, {'doc_id': '449118a7f1b448e55c33b208cb7d6548', 'range': (111, 440)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '20e558db7bfa6eb91eaa59f010f08434'}>,\n",
       " <Document: {'content': 'By contrast, in interpolation, we always mix the probability estimates from\\ninterpolation\\nall the n-gram estimators, weighing and combining the trigram, bigram, and unigram\\ncounts.\\x0c16\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nIn simple linear interpolation, we combine different order n-grams by linearly in-\\nterpolating all the models. Thus, we estimate the trigram probability P(wn|wn−2wn−1)\\nby mixing together the unigram, bigram, and trigram probabilities, each weighted\\nby a λ:\\nˆP(wn|wn−2wn−1) = λ1P(wn|wn−2wn−1)\\n+λ2P(wn|wn−1)\\n+λ3P(wn)\\n(3.26)\\nsuch that the λs sum to 1:\\n�\\ni\\nλi = 1\\n(3.27)\\nIn a slightly more sophisticated version of linear interpolation, each λ weight is\\ncomputed by conditioning on the context. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 145, '_split_overlap': [{'doc_id': '20e558db7bfa6eb91eaa59f010f08434', 'range': (0, 329)}, {'doc_id': 'e5f657f242f818948853af54aa519a03', 'range': (330, 709)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '449118a7f1b448e55c33b208cb7d6548'}>,\n",
       " <Document: {'content': 'Thus, we estimate the trigram probability P(wn|wn−2wn−1)\\nby mixing together the unigram, bigram, and trigram probabilities, each weighted\\nby a λ:\\nˆP(wn|wn−2wn−1) = λ1P(wn|wn−2wn−1)\\n+λ2P(wn|wn−1)\\n+λ3P(wn)\\n(3.26)\\nsuch that the λs sum to 1:\\n�\\ni\\nλi = 1\\n(3.27)\\nIn a slightly more sophisticated version of linear interpolation, each λ weight is\\ncomputed by conditioning on the context. This way, if we have particularly accurate\\ncounts for a particular bigram, we assume that the counts of the trigrams based on\\nthis bigram will be more trustworthy, so we can make the λs for those trigrams\\nhigher and thus give that trigram more weight in the interpolation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 146, '_split_overlap': [{'doc_id': '449118a7f1b448e55c33b208cb7d6548', 'range': (0, 379)}, {'doc_id': '9c316e79037801586e1313de3d41595b', 'range': (380, 652)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e5f657f242f818948853af54aa519a03'}>,\n",
       " <Document: {'content': 'This way, if we have particularly accurate\\ncounts for a particular bigram, we assume that the counts of the trigrams based on\\nthis bigram will be more trustworthy, so we can make the λs for those trigrams\\nhigher and thus give that trigram more weight in the interpolation. Equation 3.28\\nshows the equation for interpolation with context-conditioned weights:\\nˆP(wn|wn−2wn−1) = λ1(wn−1\\nn−2)P(wn|wn−2wn−1)\\n+λ2(wn−1\\nn−2)P(wn|wn−1)\\n+λ3(wn−1\\nn−2)P(wn)\\n(3.28)\\nHow are these λ values set? Both the simple interpolation and conditional inter-\\npolation λs are learned from a held-out corpus. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 147, '_split_overlap': [{'doc_id': 'e5f657f242f818948853af54aa519a03', 'range': (0, 272)}, {'doc_id': '91b5552540050e3b1a49f5348ea6a33b', 'range': (273, 581)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9c316e79037801586e1313de3d41595b'}>,\n",
       " <Document: {'content': 'Equation 3.28\\nshows the equation for interpolation with context-conditioned weights:\\nˆP(wn|wn−2wn−1) = λ1(wn−1\\nn−2)P(wn|wn−2wn−1)\\n+λ2(wn−1\\nn−2)P(wn|wn−1)\\n+λ3(wn−1\\nn−2)P(wn)\\n(3.28)\\nHow are these λ values set? Both the simple interpolation and conditional inter-\\npolation λs are learned from a held-out corpus. A held-out corpus is an additional\\nheld-out\\ntraining corpus that we use to set hyperparameters like these λ values, by choosing\\nthe λ values that maximize the likelihood of the held-out corpus. That is, we ﬁx\\nthe n-gram probabilities and then search for the λ values that—when plugged into\\nEq. 3.26—give us the highest probability of the held-out set. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 148, '_split_overlap': [{'doc_id': '9c316e79037801586e1313de3d41595b', 'range': (0, 308)}, {'doc_id': '1250be08f9117dbe8b5a3d68a86f575d', 'range': (309, 660)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '91b5552540050e3b1a49f5348ea6a33b'}>,\n",
       " <Document: {'content': 'A held-out corpus is an additional\\nheld-out\\ntraining corpus that we use to set hyperparameters like these λ values, by choosing\\nthe λ values that maximize the likelihood of the held-out corpus. That is, we ﬁx\\nthe n-gram probabilities and then search for the λ values that—when plugged into\\nEq. 3.26—give us the highest probability of the held-out set. There are various ways\\nto ﬁnd this optimal set of λs. One way is to use the EM algorithm, an iterative\\nlearning algorithm that converges on locally optimal λs (Jelinek and Mercer, 1980).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 149, '_split_overlap': [{'doc_id': '91b5552540050e3b1a49f5348ea6a33b', 'range': (0, 351)}, {'doc_id': 'bb6d2c3ae8d058497caeddd595f87a87', 'range': (352, 538)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1250be08f9117dbe8b5a3d68a86f575d'}>,\n",
       " <Document: {'content': 'There are various ways\\nto ﬁnd this optimal set of λs. One way is to use the EM algorithm, an iterative\\nlearning algorithm that converges on locally optimal λs (Jelinek and Mercer, 1980).\\nIn a backoff n-gram model, if the n-gram we need has zero counts, we approxi-\\nmate it by backing off to the (N-1)-gram. We continue backing off until we reach a\\nhistory that has some counts.\\nIn order for a backoff model to give a correct probability distribution, we have\\nto discount the higher-order n-grams to save some probability mass for the lower\\ndiscount\\norder n-grams. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 150, '_split_overlap': [{'doc_id': '1250be08f9117dbe8b5a3d68a86f575d', 'range': (0, 186)}, {'doc_id': '530c469624c9d755422b039ab2c1918e', 'range': (378, 563)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bb6d2c3ae8d058497caeddd595f87a87'}>,\n",
       " <Document: {'content': 'In order for a backoff model to give a correct probability distribution, we have\\nto discount the higher-order n-grams to save some probability mass for the lower\\ndiscount\\norder n-grams. Just as with add-one smoothing, if the higher-order n-grams aren’t\\ndiscounted and we just used the undiscounted MLE probability, then as soon as we\\nreplaced an n-gram which has zero probability with a lower-order n-gram, we would\\nbe adding probability mass, and the total probability assigned to all possible strings\\nby the language model would be greater than 1! ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 151, '_split_overlap': [{'doc_id': 'bb6d2c3ae8d058497caeddd595f87a87', 'range': (0, 185)}, {'doc_id': 'c4538d69b91646e96d863913f2367065', 'range': (186, 549)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '530c469624c9d755422b039ab2c1918e'}>,\n",
       " <Document: {'content': 'Just as with add-one smoothing, if the higher-order n-grams aren’t\\ndiscounted and we just used the undiscounted MLE probability, then as soon as we\\nreplaced an n-gram which has zero probability with a lower-order n-gram, we would\\nbe adding probability mass, and the total probability assigned to all possible strings\\nby the language model would be greater than 1! In addition to this explicit discount\\nfactor, we’ll need a function α to distribute this probability mass to the lower order\\nn-grams.\\nThis kind of backoff with discounting is also called Katz backoff. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 152, '_split_overlap': [{'doc_id': '530c469624c9d755422b039ab2c1918e', 'range': (0, 363)}, {'doc_id': '26f4b24001698b4657e229cf3cdce683', 'range': (364, 564)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c4538d69b91646e96d863913f2367065'}>,\n",
       " <Document: {'content': 'In addition to this explicit discount\\nfactor, we’ll need a function α to distribute this probability mass to the lower order\\nn-grams.\\nThis kind of backoff with discounting is also called Katz backoff. In Katz back-\\nKatz backoff\\noff we rely on a discounted probability P∗ if we’ve seen this n-gram before (i.e., if\\nwe have non-zero counts). Otherwise, we recursively back off to the Katz probabil-\\nity for the shorter-history (N-1)-gram. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 153, '_split_overlap': [{'doc_id': 'c4538d69b91646e96d863913f2367065', 'range': (0, 200)}, {'doc_id': '497645bb2fe350100d1d2f9bed9aa097', 'range': (201, 436)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '26f4b24001698b4657e229cf3cdce683'}>,\n",
       " <Document: {'content': 'In Katz back-\\nKatz backoff\\noff we rely on a discounted probability P∗ if we’ve seen this n-gram before (i.e., if\\nwe have non-zero counts). Otherwise, we recursively back off to the Katz probabil-\\nity for the shorter-history (N-1)-gram. The probability for a backoff n-gram PBO is\\x0c3.5\\n•\\nKNESER-NEY SMOOTHING\\n17\\nthus computed as follows:\\nPBO(wn|wn−1\\nn−N+1) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nP∗(wn|wn−1\\nn−N+1),\\nif C(wn\\nn−N+1) > 0\\nα(wn−1\\nn−N+1)PBO(wn|wn−1\\nn−N+2),\\notherwise.\\n(3.29)\\nKatz backoff is often combined with a smoothing method called Good-Turing.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 154, '_split_overlap': [{'doc_id': '26f4b24001698b4657e229cf3cdce683', 'range': (0, 235)}, {'doc_id': '19b0b954d5b33fdc5d3754fbe0f33b2f', 'range': (236, 526)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '497645bb2fe350100d1d2f9bed9aa097'}>,\n",
       " <Document: {'content': 'The probability for a backoff n-gram PBO is\\x0c3.5\\n•\\nKNESER-NEY SMOOTHING\\n17\\nthus computed as follows:\\nPBO(wn|wn−1\\nn−N+1) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nP∗(wn|wn−1\\nn−N+1),\\nif C(wn\\nn−N+1) > 0\\nα(wn−1\\nn−N+1)PBO(wn|wn−1\\nn−N+2),\\notherwise.\\n(3.29)\\nKatz backoff is often combined with a smoothing method called Good-Turing.\\nGood-Turing\\nThe combined Good-Turing backoff algorithm involves quite detailed computation\\nfor estimating the Good-Turing smoothing and the P∗ and α values.\\n3.5\\nKneser-Ney Smoothing\\nOne of the most commonly used and best performing n-gram smoothing methods\\nis the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-\\nKneser-Ney\\nman 1998).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 155, '_split_overlap': [{'doc_id': '497645bb2fe350100d1d2f9bed9aa097', 'range': (0, 290)}, {'doc_id': '73f9efd51b95e29986d2f3ec7d737996', 'range': (448, 647)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '19b0b954d5b33fdc5d3754fbe0f33b2f'}>,\n",
       " <Document: {'content': '3.5\\nKneser-Ney Smoothing\\nOne of the most commonly used and best performing n-gram smoothing methods\\nis the interpolated Kneser-Ney algorithm (Kneser and Ney 1995, Chen and Good-\\nKneser-Ney\\nman 1998).\\nKneser-Ney has its roots in a method called absolute discounting. Recall that\\ndiscounting of the counts for frequent n-grams is necessary to save some probability\\nmass for the smoothing algorithm to distribute to the unseen n-grams.\\nTo see this, we can use a clever idea from Church and Gale (1991). Consider\\nan n-gram that has count 4. We need to discount this count by some amount. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 156, '_split_overlap': [{'doc_id': '19b0b954d5b33fdc5d3754fbe0f33b2f', 'range': (0, 199)}, {'doc_id': '1ec934466ec0a3585152768834140679', 'range': (433, 583)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '73f9efd51b95e29986d2f3ec7d737996'}>,\n",
       " <Document: {'content': 'To see this, we can use a clever idea from Church and Gale (1991). Consider\\nan n-gram that has count 4. We need to discount this count by some amount. But\\nhow much should we discount it? Church and Gale’s clever idea was to look at a\\nheld-out corpus and just see what the count is for all those bigrams that had count\\n4 in the training set. They computed a bigram grammar from 22 million words of\\nAP newswire and then checked the counts of each of these bigrams in another 22\\nmillion words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 157, '_split_overlap': [{'doc_id': '73f9efd51b95e29986d2f3ec7d737996', 'range': (0, 150)}, {'doc_id': 'cea8ea3c500a54f1c99853a4bd888877', 'range': (187, 490)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1ec934466ec0a3585152768834140679'}>,\n",
       " <Document: {'content': 'Church and Gale’s clever idea was to look at a\\nheld-out corpus and just see what the count is for all those bigrams that had count\\n4 in the training set. They computed a bigram grammar from 22 million words of\\nAP newswire and then checked the counts of each of these bigrams in another 22\\nmillion words. On average, a bigram that occurred 4 times in the ﬁrst 22 million\\nwords occurred 3.23 times in the next 22 million words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 158, '_split_overlap': [{'doc_id': '1ec934466ec0a3585152768834140679', 'range': (0, 303)}, {'doc_id': '6c7d30331d720c2d6f2c3e8aef3955c3', 'range': (154, 425)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cea8ea3c500a54f1c99853a4bd888877'}>,\n",
       " <Document: {'content': 'They computed a bigram grammar from 22 million words of\\nAP newswire and then checked the counts of each of these bigrams in another 22\\nmillion words. On average, a bigram that occurred 4 times in the ﬁrst 22 million\\nwords occurred 3.23 times in the next 22 million words. The following table from\\nChurch and Gale (1991) shows these counts for bigrams with c from 0 to 9:\\nBigram count in Bigram count in\\ntraining set heldout set\\n0 0.0000270\\n1 0.448\\n2 1.25\\n3 2.24\\n4 3.23\\n5 4.21\\n6 5.23\\n7 6.21\\n8 7.21\\n9 8.26\\nFigure 3.8\\nFor all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the\\ncounts of these bigrams in a held-out corpus also of 22 million words.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 159, '_split_overlap': [{'doc_id': 'cea8ea3c500a54f1c99853a4bd888877', 'range': (0, 271)}, {'doc_id': 'b90cf34145b3023408e6011ebf54f53c', 'range': (272, 664)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c7d30331d720c2d6f2c3e8aef3955c3'}>,\n",
       " <Document: {'content': 'The following table from\\nChurch and Gale (1991) shows these counts for bigrams with c from 0 to 9:\\nBigram count in Bigram count in\\ntraining set heldout set\\n0 0.0000270\\n1 0.448\\n2 1.25\\n3 2.24\\n4 3.23\\n5 4.21\\n6 5.23\\n7 6.21\\n8 7.21\\n9 8.26\\nFigure 3.8\\nFor all bigrams in 22 million words of AP newswire of count 0, 1, 2,...,9, the\\ncounts of these bigrams in a held-out corpus also of 22 million words.\\nThe astute reader may have noticed that except for the held-out counts for 0\\nand 1, all the other bigram counts in the held-out set could be estimated pretty well\\nby just subtracting 0.75 from the count in the training set! ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 160, '_split_overlap': [{'doc_id': '6c7d30331d720c2d6f2c3e8aef3955c3', 'range': (0, 392)}, {'doc_id': 'ae511bc2845ee292c14494ae0772aad5', 'range': (393, 616)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b90cf34145b3023408e6011ebf54f53c'}>,\n",
       " <Document: {'content': 'The astute reader may have noticed that except for the held-out counts for 0\\nand 1, all the other bigram counts in the held-out set could be estimated pretty well\\nby just subtracting 0.75 from the count in the training set! Absolute discounting\\nAbsolute\\ndiscounting\\nformalizes this intuition by subtracting a ﬁxed (absolute) discount d from each count.\\nThe intuition is that since we have good estimates already for the very high counts, a\\nsmall discount d won’t affect them much. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 161, '_split_overlap': [{'doc_id': 'b90cf34145b3023408e6011ebf54f53c', 'range': (0, 223)}, {'doc_id': 'f9aaa68eeee8de71b3fc906fe825fd6f', 'range': (224, 480)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ae511bc2845ee292c14494ae0772aad5'}>,\n",
       " <Document: {'content': 'Absolute discounting\\nAbsolute\\ndiscounting\\nformalizes this intuition by subtracting a ﬁxed (absolute) discount d from each count.\\nThe intuition is that since we have good estimates already for the very high counts, a\\nsmall discount d won’t affect them much. It will mainly modify the smaller counts,\\x0c18\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nfor which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that\\nin practice this discount is actually a good one for bigrams with counts 2 through 9.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 162, '_split_overlap': [{'doc_id': 'ae511bc2845ee292c14494ae0772aad5', 'range': (0, 256)}, {'doc_id': '9de2ebecff014f19a9f8e1c651ad9da2', 'range': (257, 507)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f9aaa68eeee8de71b3fc906fe825fd6f'}>,\n",
       " <Document: {'content': 'It will mainly modify the smaller counts,\\x0c18\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nfor which we don’t necessarily trust the estimate anyway, and Fig. 3.8 suggests that\\nin practice this discount is actually a good one for bigrams with counts 2 through 9.\\nThe equation for interpolated absolute discounting applied to bigrams:\\nPAbsoluteDiscounting(wi|wi−1) = C(wi−1wi)−d\\n�\\nvC(wi−1 v) +λ(wi−1)P(wi)\\n(3.30)\\nThe ﬁrst term is the discounted bigram, and the second term is the unigram with\\nan interpolation weight λ. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 163, '_split_overlap': [{'doc_id': 'f9aaa68eeee8de71b3fc906fe825fd6f', 'range': (0, 250)}, {'doc_id': '35a9644b6dfad3f0f7ba78f3a5a0ddc', 'range': (251, 506)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9de2ebecff014f19a9f8e1c651ad9da2'}>,\n",
       " <Document: {'content': 'The equation for interpolated absolute discounting applied to bigrams:\\nPAbsoluteDiscounting(wi|wi−1) = C(wi−1wi)−d\\n�\\nvC(wi−1 v) +λ(wi−1)P(wi)\\n(3.30)\\nThe ﬁrst term is the discounted bigram, and the second term is the unigram with\\nan interpolation weight λ. We could just set all the d values to .75, or we could keep\\na separate discount value of 0.5 for the bigrams with counts of 1.\\nKneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-\\ning with a more sophisticated way to handle the lower-order unigram distribution.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 164, '_split_overlap': [{'doc_id': '9de2ebecff014f19a9f8e1c651ad9da2', 'range': (0, 255)}, {'doc_id': 'dff10a11889b0a96ed873b4dce6cd629', 'range': (256, 538)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '35a9644b6dfad3f0f7ba78f3a5a0ddc'}>,\n",
       " <Document: {'content': 'We could just set all the d values to .75, or we could keep\\na separate discount value of 0.5 for the bigrams with counts of 1.\\nKneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-\\ning with a more sophisticated way to handle the lower-order unigram distribution.\\nConsider the job of predicting the next word in this sentence, assuming we are inter-\\npolating a bigram and a unigram model.\\nI can’t see without my reading\\n.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 165, '_split_overlap': [{'doc_id': '35a9644b6dfad3f0f7ba78f3a5a0ddc', 'range': (0, 282)}, {'doc_id': '4f82f3ce8401cc19a15472ac487a2172', 'range': (127, 440)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dff10a11889b0a96ed873b4dce6cd629'}>,\n",
       " <Document: {'content': 'Kneser-Ney discounting (Kneser and Ney, 1995) augments absolute discount-\\ning with a more sophisticated way to handle the lower-order unigram distribution.\\nConsider the job of predicting the next word in this sentence, assuming we are inter-\\npolating a bigram and a unigram model.\\nI can’t see without my reading\\n.\\nThe word glasses seems much more likely to follow here than, say, the word\\nKong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is\\nmore common, since Hong Kong is a very frequent word. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 166, '_split_overlap': [{'doc_id': 'dff10a11889b0a96ed873b4dce6cd629', 'range': (0, 313)}, {'doc_id': 'c1e57f2d1bc701533e268f7adf17ac0', 'range': (314, 528)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f82f3ce8401cc19a15472ac487a2172'}>,\n",
       " <Document: {'content': 'The word glasses seems much more likely to follow here than, say, the word\\nKong, so we’d like our unigram model to prefer glasses. But in fact it’s Kong that is\\nmore common, since Hong Kong is a very frequent word. A standard unigram model\\nwill assign Kong a higher probability than glasses. We would like to capture the\\nintuition that although Kong is frequent, it is mainly only frequent in the phrase Hong\\nKong, that is, after the word Hong. The word glasses has a much wider distribution.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 167, '_split_overlap': [{'doc_id': '4f82f3ce8401cc19a15472ac487a2172', 'range': (0, 214)}, {'doc_id': '1de9c13d01d0e58ccc9e95998ca73bfb', 'range': (292, 492)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c1e57f2d1bc701533e268f7adf17ac0'}>,\n",
       " <Document: {'content': 'We would like to capture the\\nintuition that although Kong is frequent, it is mainly only frequent in the phrase Hong\\nKong, that is, after the word Hong. The word glasses has a much wider distribution.\\nIn other words, instead of P(w), which answers the question “How likely is\\nw?”, we’d like to create a unigram model that we might call PCONTINUATION, which\\nanswers the question “How likely is w to appear as a novel continuation?”. How can\\nwe estimate this probability of seeing the word w as a novel continuation, in a new\\nunseen context? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 168, '_split_overlap': [{'doc_id': 'c1e57f2d1bc701533e268f7adf17ac0', 'range': (0, 200)}, {'doc_id': '4d194be84fbd8cbb7cf4eebccaf86fef', 'range': (201, 539)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1de9c13d01d0e58ccc9e95998ca73bfb'}>,\n",
       " <Document: {'content': 'In other words, instead of P(w), which answers the question “How likely is\\nw?”, we’d like to create a unigram model that we might call PCONTINUATION, which\\nanswers the question “How likely is w to appear as a novel continuation?”. How can\\nwe estimate this probability of seeing the word w as a novel continuation, in a new\\nunseen context? The Kneser-Ney intuition is to base our estimate of PCONTINUATION\\non the number of different contexts word w has appeared in, that is, the number of\\nbigram types it completes. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 169, '_split_overlap': [{'doc_id': '1de9c13d01d0e58ccc9e95998ca73bfb', 'range': (0, 338)}, {'doc_id': '9dcb588f6479397af753e7bb136d3eb4', 'range': (339, 514)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d194be84fbd8cbb7cf4eebccaf86fef'}>,\n",
       " <Document: {'content': 'The Kneser-Ney intuition is to base our estimate of PCONTINUATION\\non the number of different contexts word w has appeared in, that is, the number of\\nbigram types it completes. Every bigram type was a novel continuation the ﬁrst time\\nit was seen. We hypothesize that words that have appeared in more contexts in the\\npast are more likely to appear in some new context as well. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 170, '_split_overlap': [{'doc_id': '4d194be84fbd8cbb7cf4eebccaf86fef', 'range': (0, 175)}, {'doc_id': 'c36184ae76099a8d43460e56c079b7f7', 'range': (176, 374)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9dcb588f6479397af753e7bb136d3eb4'}>,\n",
       " <Document: {'content': 'Every bigram type was a novel continuation the ﬁrst time\\nit was seen. We hypothesize that words that have appeared in more contexts in the\\npast are more likely to appear in some new context as well. The number of times a\\nword w appears as a novel continuation can be expressed as:\\nPCONTINUATION(w) ∝ |{v : C(vw) > 0}|\\n(3.31)\\nTo turn this count into a probability, we normalize by the total number of word\\nbigram types. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 171, '_split_overlap': [{'doc_id': '9dcb588f6479397af753e7bb136d3eb4', 'range': (0, 198)}, {'doc_id': '8c66e998546eb5d92212d7ee71471e7a', 'range': (199, 418)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c36184ae76099a8d43460e56c079b7f7'}>,\n",
       " <Document: {'content': 'The number of times a\\nword w appears as a novel continuation can be expressed as:\\nPCONTINUATION(w) ∝ |{v : C(vw) > 0}|\\n(3.31)\\nTo turn this count into a probability, we normalize by the total number of word\\nbigram types. In summary:\\nPCONTINUATION(w) =\\n|{v : C(vw) > 0}|\\n|{(u′,w′) : C(u′w′) > 0}|\\n(3.32)\\nAn equivalent formulation based on a different metaphor is to use the number of\\nword types seen to precede w (Eq. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 172, '_split_overlap': [{'doc_id': 'c36184ae76099a8d43460e56c079b7f7', 'range': (0, 219)}, {'doc_id': 'ddc4604b479740d2d5247846b1968aa5', 'range': (220, 415)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8c66e998546eb5d92212d7ee71471e7a'}>,\n",
       " <Document: {'content': 'In summary:\\nPCONTINUATION(w) =\\n|{v : C(vw) > 0}|\\n|{(u′,w′) : C(u′w′) > 0}|\\n(3.32)\\nAn equivalent formulation based on a different metaphor is to use the number of\\nword types seen to precede w (Eq. 3.31 repeated):\\nPCONTINUATION(w) ∝ |{v : C(vw) > 0}|\\n(3.33)\\nnormalized by the number of words preceding all words, as follows:\\nPCONTINUATION(w) =\\n|{v : C(vw) > 0}|\\n�\\nw′ |{v : C(vw′) > 0}|\\n(3.34)\\nA frequent word (Kong) occurring in only one context (Hong) will have a low\\ncontinuation probability.\\x0c', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 173, '_split_overlap': [{'doc_id': '8c66e998546eb5d92212d7ee71471e7a', 'range': (0, 195)}, {'doc_id': '47ea7f34317de9bc39bbd0fc101ccf4d', 'range': (196, 492)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ddc4604b479740d2d5247846b1968aa5'}>,\n",
       " <Document: {'content': '3.31 repeated):\\nPCONTINUATION(w) ∝ |{v : C(vw) > 0}|\\n(3.33)\\nnormalized by the number of words preceding all words, as follows:\\nPCONTINUATION(w) =\\n|{v : C(vw) > 0}|\\n�\\nw′ |{v : C(vw′) > 0}|\\n(3.34)\\nA frequent word (Kong) occurring in only one context (Hong) will have a low\\ncontinuation probability.\\x0c3.6\\n•\\nTHE WEB AND STUPID BACKOFF\\n19\\nThe ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:\\nInterpolated\\nKneser-Ney\\nPKN(wi|wi−1) = max(C(wi−1wi)−d,0)\\nC(wi−1)\\n+λ(wi−1)PCONTINUATION(wi)\\n(3.35)\\nThe λ is a normalizing constant that is used to distribute the probability mass\\nwe’ve discounted.', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 174, '_split_overlap': [{'doc_id': 'ddc4604b479740d2d5247846b1968aa5', 'range': (0, 296)}, {'doc_id': '49c739923476525d3a67d8d1267a119', 'range': (297, 605)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '47ea7f34317de9bc39bbd0fc101ccf4d'}>,\n",
       " <Document: {'content': '3.6\\n•\\nTHE WEB AND STUPID BACKOFF\\n19\\nThe ﬁnal equation for Interpolated Kneser-Ney smoothing for bigrams is then:\\nInterpolated\\nKneser-Ney\\nPKN(wi|wi−1) = max(C(wi−1wi)−d,0)\\nC(wi−1)\\n+λ(wi−1)PCONTINUATION(wi)\\n(3.35)\\nThe λ is a normalizing constant that is used to distribute the probability mass\\nwe’ve discounted.:\\nλ(wi−1) =\\nd\\n�\\nvC(wi−1v)|{w : C(wi−1w) > 0}|\\n(3.36)\\nThe ﬁrst term,\\nd\\n�\\nvC(wi−1v), is the normalized discount. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 175, '_split_overlap': [{'doc_id': '47ea7f34317de9bc39bbd0fc101ccf4d', 'range': (0, 308)}, {'doc_id': 'f82953251d92eafede0969d7bf1379c7', 'range': (309, 419)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '49c739923476525d3a67d8d1267a119'}>,\n",
       " <Document: {'content': ':\\nλ(wi−1) =\\nd\\n�\\nvC(wi−1v)|{w : C(wi−1w) > 0}|\\n(3.36)\\nThe ﬁrst term,\\nd\\n�\\nvC(wi−1v), is the normalized discount. The second term,\\n|{w : C(wi−1w) > 0}|, is the number of word types that can follow wi−1 or, equiva-\\nlently, the number of word types that we discounted; in other words, the number of\\ntimes we applied the normalized discount.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 176, '_split_overlap': [{'doc_id': '49c739923476525d3a67d8d1267a119', 'range': (0, 110)}, {'doc_id': '6303c78a4693293813cf91a77b95f71', 'range': (111, 335)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f82953251d92eafede0969d7bf1379c7'}>,\n",
       " <Document: {'content': 'The second term,\\n|{w : C(wi−1w) > 0}|, is the number of word types that can follow wi−1 or, equiva-\\nlently, the number of word types that we discounted; in other words, the number of\\ntimes we applied the normalized discount.\\nThe general recursive formulation is as follows:\\nPKN(wi|wi−1\\ni−n+1) = max(cKN(wi\\ni−n+1)−d,0)\\n�\\nv cKN(wi−1\\ni−n+1v)\\n+λ(wi−1\\ni−n+1)PKN(wi|wi−1\\ni−n+2) (3.37)\\nwhere the deﬁnition of the count cKN depends on whether we are counting the\\nhighest-order n-gram being interpolated (for example trigram if we are interpolating\\ntrigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram\\nif we are interpolating trigram, bigram, and unigram):\\ncKN(·) =\\n� count(·)\\nfor the highest order\\ncontinuationcount(·)\\nfor lower orders\\n(3.38)\\nThe continuation count is the number of unique single word contexts for ·.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 177, '_split_overlap': [{'doc_id': 'f82953251d92eafede0969d7bf1379c7', 'range': (0, 224)}, {'doc_id': 'f1bd9d8fc70067dc3691e4d15842a9de', 'range': (225, 839)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6303c78a4693293813cf91a77b95f71'}>,\n",
       " <Document: {'content': 'The general recursive formulation is as follows:\\nPKN(wi|wi−1\\ni−n+1) = max(cKN(wi\\ni−n+1)−d,0)\\n�\\nv cKN(wi−1\\ni−n+1v)\\n+λ(wi−1\\ni−n+1)PKN(wi|wi−1\\ni−n+2) (3.37)\\nwhere the deﬁnition of the count cKN depends on whether we are counting the\\nhighest-order n-gram being interpolated (for example trigram if we are interpolating\\ntrigram, bigram, and unigram) or one of the lower-order n-grams (bigram or unigram\\nif we are interpolating trigram, bigram, and unigram):\\ncKN(·) =\\n� count(·)\\nfor the highest order\\ncontinuationcount(·)\\nfor lower orders\\n(3.38)\\nThe continuation count is the number of unique single word contexts for ·.\\nAt the termination of the recursion, unigrams are interpolated with the uniform\\ndistribution, where the parameter ε is the empty string:\\nPKN(w) = max(cKN(w)−d,0)\\n�\\nw′ cKN(w′)\\n+λ(ε) 1\\nV\\n(3.39)\\nIf we want to include an unknown word <UNK>, it’s just included as a regular vo-\\ncabulary entry with count zero, and hence its probability will be a lambda-weighted\\nuniform distribution λ(ε)\\nV .\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 178, '_split_overlap': [{'doc_id': '6303c78a4693293813cf91a77b95f71', 'range': (0, 614)}, {'doc_id': '555a5d8264c53ed7e0c20c5585d7d9e9', 'range': (615, 1001)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'f1bd9d8fc70067dc3691e4d15842a9de'}>,\n",
       " <Document: {'content': 'At the termination of the recursion, unigrams are interpolated with the uniform\\ndistribution, where the parameter ε is the empty string:\\nPKN(w) = max(cKN(w)−d,0)\\n�\\nw′ cKN(w′)\\n+λ(ε) 1\\nV\\n(3.39)\\nIf we want to include an unknown word <UNK>, it’s just included as a regular vo-\\ncabulary entry with count zero, and hence its probability will be a lambda-weighted\\nuniform distribution λ(ε)\\nV .\\nThe best-performing version of Kneser-Ney smoothing is called modiﬁed Kneser-\\nNey smoothing, and is due to Chen and Goodman (1998). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 179, '_split_overlap': [{'doc_id': 'f1bd9d8fc70067dc3691e4d15842a9de', 'range': (0, 386)}, {'doc_id': 'c5dc99a6e998d46e9c533e4f161765a6', 'range': (387, 518)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '555a5d8264c53ed7e0c20c5585d7d9e9'}>,\n",
       " <Document: {'content': 'The best-performing version of Kneser-Ney smoothing is called modiﬁed Kneser-\\nNey smoothing, and is due to Chen and Goodman (1998). Rather than use a single\\nmodiﬁed\\nKneser-Ney\\nﬁxed discount d, modiﬁed Kneser-Ney uses three different discounts d1, d2, and\\nd3+ for n-grams with counts of 1, 2 and three or more, respectively. See Chen and\\nGoodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.\\n3.6\\nThe Web and Stupid Backoff\\nBy using text from the web, it is possible to build extremely large language mod-\\nels. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 180, '_split_overlap': [{'doc_id': '555a5d8264c53ed7e0c20c5585d7d9e9', 'range': (0, 131)}, {'doc_id': '8b3e090641c6539a2e9ccf299dcf384d', 'range': (324, 518)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c5dc99a6e998d46e9c533e4f161765a6'}>,\n",
       " <Document: {'content': 'See Chen and\\nGoodman (1998, p. 19) or Heaﬁeld et al. (2013) for the details.\\n3.6\\nThe Web and Stupid Backoff\\nBy using text from the web, it is possible to build extremely large language mod-\\nels. In 2006 Google released a very large set of n-gram counts, including n-grams\\n(1-grams through 5-grams) from all the ﬁve-word sequences that appear at least\\x0c20\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\n40 times from 1,024,908,267,229 words of running text on the web; this includes\\n1,176,470,663 ﬁve-word sequences using over 13 million unique words types (Franz\\nand Brants, 2006). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 181, '_split_overlap': [{'doc_id': 'c5dc99a6e998d46e9c533e4f161765a6', 'range': (0, 194)}, {'doc_id': '152a9008588858b922edb0e9018178c3', 'range': (195, 568)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b3e090641c6539a2e9ccf299dcf384d'}>,\n",
       " <Document: {'content': 'In 2006 Google released a very large set of n-gram counts, including n-grams\\n(1-grams through 5-grams) from all the ﬁve-word sequences that appear at least\\x0c20\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\n40 times from 1,024,908,267,229 words of running text on the web; this includes\\n1,176,470,663 ﬁve-word sequences using over 13 million unique words types (Franz\\nand Brants, 2006). Some examples:\\n4-gram\\nCount\\nserve as the incoming\\n92\\nserve as the incubator\\n99\\nserve as the independent\\n794\\nserve as the index\\n223\\nserve as the indication\\n72\\nserve as the indicator\\n120\\nserve as the indicators\\n45\\nserve as the indispensable\\n111\\nserve as the indispensible\\n40\\nserve as the individual\\n234\\nEfﬁciency considerations are important when building language models that use\\nsuch large sets of n-grams. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 182, '_split_overlap': [{'doc_id': '8b3e090641c6539a2e9ccf299dcf384d', 'range': (0, 373)}, {'doc_id': '80074f4b242b8d969eb10977a03ea0df', 'range': (374, 780)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '152a9008588858b922edb0e9018178c3'}>,\n",
       " <Document: {'content': 'Some examples:\\n4-gram\\nCount\\nserve as the incoming\\n92\\nserve as the incubator\\n99\\nserve as the independent\\n794\\nserve as the index\\n223\\nserve as the indication\\n72\\nserve as the indicator\\n120\\nserve as the indicators\\n45\\nserve as the indispensable\\n111\\nserve as the indispensible\\n40\\nserve as the individual\\n234\\nEfﬁciency considerations are important when building language models that use\\nsuch large sets of n-grams. Rather than store each word as a string, it is generally\\nrepresented in memory as a 64-bit hash number, with the words themselves stored\\non disk. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 183, '_split_overlap': [{'doc_id': '152a9008588858b922edb0e9018178c3', 'range': (0, 406)}, {'doc_id': 'b9dcc1cd20992f81754309ead2bdcf65', 'range': (407, 552)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '80074f4b242b8d969eb10977a03ea0df'}>,\n",
       " <Document: {'content': 'Rather than store each word as a string, it is generally\\nrepresented in memory as a 64-bit hash number, with the words themselves stored\\non disk. Probabilities are generally quantized using only 4-8 bits (instead of 8-byte\\nﬂoats), and n-grams are stored in reverse tries.\\nN-grams can also be shrunk by pruning, for example only storing n-grams with\\ncounts greater than some threshold (such as the count threshold of 40 used for the\\nGoogle n-gram release) or using entropy to prune less-important n-grams (Stolcke,\\n1998). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 184, '_split_overlap': [{'doc_id': '80074f4b242b8d969eb10977a03ea0df', 'range': (0, 145)}, {'doc_id': '2473c2a5c4e5fceb90e37aa6667ac5d4', 'range': (272, 520)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b9dcc1cd20992f81754309ead2bdcf65'}>,\n",
       " <Document: {'content': 'N-grams can also be shrunk by pruning, for example only storing n-grams with\\ncounts greater than some threshold (such as the count threshold of 40 used for the\\nGoogle n-gram release) or using entropy to prune less-important n-grams (Stolcke,\\n1998). Another option is to build approximate language models using techniques\\nlike Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007). Finally, efﬁ-\\nBloom ﬁlters\\ncient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 185, '_split_overlap': [{'doc_id': 'b9dcc1cd20992f81754309ead2bdcf65', 'range': (0, 248)}, {'doc_id': 'cfb54833e2ebf64d8d466a997a9d206', 'range': (249, 483)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2473c2a5c4e5fceb90e37aa6667ac5d4'}>,\n",
       " <Document: {'content': 'Another option is to build approximate language models using techniques\\nlike Bloom ﬁlters (Talbot and Osborne 2007, Church et al. 2007). Finally, efﬁ-\\nBloom ﬁlters\\ncient language model toolkits like KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013) use\\nsorted arrays, efﬁciently combine probabilities and backoffs in a single value, and\\nuse merge sorts to efﬁciently build the probability tables in a minimal number of\\npasses through a large corpus.\\nAlthough with these toolkits it is possible to build web-scale language models\\nusing full Kneser-Ney smoothing, Brants et al. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 186, '_split_overlap': [{'doc_id': '2473c2a5c4e5fceb90e37aa6667ac5d4', 'range': (0, 234)}, {'doc_id': 'fd07f62cdd97fb7a813e324a8bf33b33', 'range': (235, 567)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cfb54833e2ebf64d8d466a997a9d206'}>,\n",
       " <Document: {'content': '2013) use\\nsorted arrays, efﬁciently combine probabilities and backoffs in a single value, and\\nuse merge sorts to efﬁciently build the probability tables in a minimal number of\\npasses through a large corpus.\\nAlthough with these toolkits it is possible to build web-scale language models\\nusing full Kneser-Ney smoothing, Brants et al. (2007) show that with very large lan-\\nguage models a much simpler algorithm may be sufﬁcient. The algorithm is called\\nstupid backoff. Stupid backoff gives up the idea of trying to make the language\\nstupid backoff\\nmodel a true probability distribution. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 187, '_split_overlap': [{'doc_id': 'cfb54833e2ebf64d8d466a997a9d206', 'range': (0, 332)}, {'doc_id': '8be89923fb6f9cdb9d3fd3ce94f3f0f3', 'range': (333, 584)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fd07f62cdd97fb7a813e324a8bf33b33'}>,\n",
       " <Document: {'content': '(2007) show that with very large lan-\\nguage models a much simpler algorithm may be sufﬁcient. The algorithm is called\\nstupid backoff. Stupid backoff gives up the idea of trying to make the language\\nstupid backoff\\nmodel a true probability distribution. There is no discounting of the higher-order\\nprobabilities. If a higher-order n-gram has a zero count, we simply backoff to a\\nlower order n-gram, weighed by a ﬁxed (context-independent) weight. This algo-\\nrithm does not produce a probability distribution, so we’ll follow Brants et al. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 188, '_split_overlap': [{'doc_id': 'fd07f62cdd97fb7a813e324a8bf33b33', 'range': (0, 251)}, {'doc_id': '58c1a02cc7502061532830c121a35ae', 'range': (311, 536)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8be89923fb6f9cdb9d3fd3ce94f3f0f3'}>,\n",
       " <Document: {'content': 'If a higher-order n-gram has a zero count, we simply backoff to a\\nlower order n-gram, weighed by a ﬁxed (context-independent) weight. This algo-\\nrithm does not produce a probability distribution, so we’ll follow Brants et al. (2007)\\nin referring to it as S:\\nS(wi|wi−1\\ni−k+1) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ncount(wi\\ni−k+1)\\ncount(wi−1\\ni−k+1)\\nif count(wi\\ni−k+1) > 0\\nλS(wi|wi−1\\ni−k+2)\\notherwise\\n(3.40)\\nThe backoff terminates in the unigram, which has probability S(w) = count(w)\\nN\\n. Brants\\net al. (2007) ﬁnd that a value of 0.4 worked well for λ.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 189, '_split_overlap': [{'doc_id': '8be89923fb6f9cdb9d3fd3ce94f3f0f3', 'range': (0, 225)}, {'doc_id': 'feada493d98a72e67ad5e193c74c1100', 'range': (226, 519)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '58c1a02cc7502061532830c121a35ae'}>,\n",
       " <Document: {'content': '(2007)\\nin referring to it as S:\\nS(wi|wi−1\\ni−k+1) =\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\ncount(wi\\ni−k+1)\\ncount(wi−1\\ni−k+1)\\nif count(wi\\ni−k+1) > 0\\nλS(wi|wi−1\\ni−k+2)\\notherwise\\n(3.40)\\nThe backoff terminates in the unigram, which has probability S(w) = count(w)\\nN\\n. Brants\\net al. (2007) ﬁnd that a value of 0.4 worked well for λ.\\n3.7\\nAdvanced: Perplexity’s Relation to Entropy\\nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on\\na test set. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 190, '_split_overlap': [{'doc_id': '58c1a02cc7502061532830c121a35ae', 'range': (0, 293)}, {'doc_id': 'b22e232fdb69808431525dcc2397f960', 'range': (244, 432)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'feada493d98a72e67ad5e193c74c1100'}>,\n",
       " <Document: {'content': '(2007) ﬁnd that a value of 0.4 worked well for λ.\\n3.7\\nAdvanced: Perplexity’s Relation to Entropy\\nWe introduced perplexity in Section 3.2.1 as a way to evaluate n-gram models on\\na test set. A better n-gram model is one that assigns a higher probability to the\\x0c3.7\\n•\\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\\n21\\ntest data, and perplexity is a normalized version of the probability of the test set.\\nThe perplexity measure actually arises from the information-theoretic concept of\\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\\nthe inverse probability, for example?) and its relationship to entropy. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 191, '_split_overlap': [{'doc_id': 'feada493d98a72e67ad5e193c74c1100', 'range': (0, 188)}, {'doc_id': '227f73fd09a054e75abab7eee11b09c0', 'range': (189, 630)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b22e232fdb69808431525dcc2397f960'}>,\n",
       " <Document: {'content': 'A better n-gram model is one that assigns a higher probability to the\\x0c3.7\\n•\\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\\n21\\ntest data, and perplexity is a normalized version of the probability of the test set.\\nThe perplexity measure actually arises from the information-theoretic concept of\\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a\\nEntropy\\nmeasure of information. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 192, '_split_overlap': [{'doc_id': 'b22e232fdb69808431525dcc2397f960', 'range': (0, 441)}, {'doc_id': 'ae23516f14a1e3a6bcdd6e0d4e94714d', 'range': (208, 486)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '227f73fd09a054e75abab7eee11b09c0'}>,\n",
       " <Document: {'content': 'The perplexity measure actually arises from the information-theoretic concept of\\ncross-entropy, which explains otherwise mysterious properties of perplexity (why\\nthe inverse probability, for example?) and its relationship to entropy. Entropy is a\\nEntropy\\nmeasure of information. Given a random variable X ranging over whatever we are\\npredicting (words, letters, parts of speech, the set of which we’ll call χ) and with a\\nparticular probability function, call it p(x), the entropy of the random variable X is:\\nH(X) = −\\n�\\nx∈χ\\np(x)log2 p(x)\\n(3.41)\\nThe log can, in principle, be computed in any base. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 193, '_split_overlap': [{'doc_id': '227f73fd09a054e75abab7eee11b09c0', 'range': (0, 278)}, {'doc_id': '157e155c793434f6bde57a24c2539ffb', 'range': (279, 596)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ae23516f14a1e3a6bcdd6e0d4e94714d'}>,\n",
       " <Document: {'content': 'Given a random variable X ranging over whatever we are\\npredicting (words, letters, parts of speech, the set of which we’ll call χ) and with a\\nparticular probability function, call it p(x), the entropy of the random variable X is:\\nH(X) = −\\n�\\nx∈χ\\np(x)log2 p(x)\\n(3.41)\\nThe log can, in principle, be computed in any base. If we use log base 2, the\\nresulting value of entropy will be measured in bits.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 194, '_split_overlap': [{'doc_id': 'ae23516f14a1e3a6bcdd6e0d4e94714d', 'range': (0, 317)}, {'doc_id': '639346801c83772491c8449e5ef7cf5b', 'range': (318, 396)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '157e155c793434f6bde57a24c2539ffb'}>,\n",
       " <Document: {'content': 'If we use log base 2, the\\nresulting value of entropy will be measured in bits.\\nOne intuitive way to think about entropy is as a lower bound on the number of\\nbits it would take to encode a certain decision or piece of information in the optimal\\ncoding scheme.\\nConsider an example from the standard information theory textbook Cover and\\nThomas (1991). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 195, '_split_overlap': [{'doc_id': '157e155c793434f6bde57a24c2539ffb', 'range': (0, 78)}, {'doc_id': 'bd381b9d3ff267e8d8d75ac78658428f', 'range': (79, 349)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '639346801c83772491c8449e5ef7cf5b'}>,\n",
       " <Document: {'content': 'One intuitive way to think about entropy is as a lower bound on the number of\\nbits it would take to encode a certain decision or piece of information in the optimal\\ncoding scheme.\\nConsider an example from the standard information theory textbook Cover and\\nThomas (1991). Imagine that we want to place a bet on a horse race but it is too\\nfar to go all the way to Yonkers Racetrack, so we’d like to send a short message to\\nthe bookie to tell him which of the eight horses to bet on. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 196, '_split_overlap': [{'doc_id': '639346801c83772491c8449e5ef7cf5b', 'range': (0, 270)}, {'doc_id': 'edbf305e1edc0c3dbb1a6f39f6c50b13', 'range': (271, 480)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'bd381b9d3ff267e8d8d75ac78658428f'}>,\n",
       " <Document: {'content': 'Imagine that we want to place a bet on a horse race but it is too\\nfar to go all the way to Yonkers Racetrack, so we’d like to send a short message to\\nthe bookie to tell him which of the eight horses to bet on. One way to encode this\\nmessage is just to use the binary representation of the horse’s number as the code;\\nthus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded\\nas 000. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 197, '_split_overlap': [{'doc_id': 'bd381b9d3ff267e8d8d75ac78658428f', 'range': (0, 209)}, {'doc_id': '466d5e4630b63501b3917d020351ca8f', 'range': (210, 408)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'edbf305e1edc0c3dbb1a6f39f6c50b13'}>,\n",
       " <Document: {'content': 'One way to encode this\\nmessage is just to use the binary representation of the horse’s number as the code;\\nthus, horse 1 would be 001, horse 2 010, horse 3 011, and so on, with horse 8 coded\\nas 000. If we spend the whole day betting and each horse is coded with 3 bits, on\\naverage we would be sending 3 bits per race.\\nCan we do better? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 198, '_split_overlap': [{'doc_id': 'edbf305e1edc0c3dbb1a6f39f6c50b13', 'range': (0, 198)}, {'doc_id': 'e3f019c28c1538784a3d32520d8c5417', 'range': (199, 335)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '466d5e4630b63501b3917d020351ca8f'}>,\n",
       " <Document: {'content': 'If we spend the whole day betting and each horse is coded with 3 bits, on\\naverage we would be sending 3 bits per race.\\nCan we do better? Suppose that the spread is the actual distribution of the bets\\nplaced and that we represent it as the prior probability of each horse as follows:\\nHorse 1\\n1\\n2\\nHorse 5\\n1\\n64\\nHorse 2\\n1\\n4\\nHorse 6\\n1\\n64\\nHorse 3\\n1\\n8\\nHorse 7\\n1\\n64\\nHorse 4\\n1\\n16\\nHorse 8\\n1\\n64\\nThe entropy of the random variable X that ranges over horses gives us a lower\\nbound on the number of bits and is\\nH(X) = −\\ni=8\\n�\\ni=1\\np(i)log p(i)\\n=\\n− 1\\n2 log 1\\n2 − 1\\n4 log 1\\n4 − 1\\n8 log 1\\n8 − 1\\n16 log 1\\n16 −4( 1\\n64 log 1\\n64 )\\n= 2 bits\\n(3.42)\\nA code that averages 2 bits per race can be built with short encodings for more\\nprobable horses, and longer encodings for less probable horses. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 199, '_split_overlap': [{'doc_id': '466d5e4630b63501b3917d020351ca8f', 'range': (0, 136)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e3f019c28c1538784a3d32520d8c5417'}>,\n",
       " <Document: {'content': 'For example, we\\ncould encode the most likely horse with the code 0, and the remaining horses as 10,\\nthen 110, 1110, 111100, 111101, 111110, and 111111.\\nWhat if the horses are equally likely? We saw above that if we used an equal-\\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\\naverage was 3. Is the entropy the same? In this case each horse would have a\\nprobability of 1\\n8. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 200, '_split_overlap': [{'doc_id': '88ba70701e0556d096000768f9ac6940', 'range': (191, 407)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4b0a4487b5c59758b58067d54208b38a'}>,\n",
       " <Document: {'content': 'We saw above that if we used an equal-\\nlength binary code for the horse numbers, each horse took 3 bits to code, so the\\naverage was 3. Is the entropy the same? In this case each horse would have a\\nprobability of 1\\n8. The entropy of the choice of horses is then\\nH(X) = −\\ni=8\\n�\\ni=1\\n1\\n8 log 1\\n8 = −log 1\\n8 = 3 bits\\n(3.43)\\x0c22\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nUntil now we have been computing the entropy of a single variable. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 201, '_split_overlap': [{'doc_id': '4b0a4487b5c59758b58067d54208b38a', 'range': (0, 216)}, {'doc_id': 'e35e719a4ee131e5c3014044718fac00', 'range': (217, 423)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '88ba70701e0556d096000768f9ac6940'}>,\n",
       " <Document: {'content': 'The entropy of the choice of horses is then\\nH(X) = −\\ni=8\\n�\\ni=1\\n1\\n8 log 1\\n8 = −log 1\\n8 = 3 bits\\n(3.43)\\x0c22\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nUntil now we have been computing the entropy of a single variable. But most of\\nwhat we will use entropy for involves sequences. For a grammar, for example, we\\nwill be computing the entropy of some sequence of words W = {w0,w1,w2,...,wn}.\\nOne way to do this is to have a variable that ranges over sequences of words. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 202, '_split_overlap': [{'doc_id': '88ba70701e0556d096000768f9ac6940', 'range': (0, 206)}, {'doc_id': 'c44826c5e97214198b7e35ba441e077a', 'range': (268, 455)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e35e719a4ee131e5c3014044718fac00'}>,\n",
       " <Document: {'content': 'For a grammar, for example, we\\nwill be computing the entropy of some sequence of words W = {w0,w1,w2,...,wn}.\\nOne way to do this is to have a variable that ranges over sequences of words. For\\nexample we can compute the entropy of a random variable that ranges over all ﬁnite\\nsequences of words of length n in some language L as follows:\\nH(w1,w2,...,wn) = −\\n�\\nW n\\n1 ∈L\\np(W n\\n1 )log p(W n\\n1 )\\n(3.44)\\nWe could deﬁne the entropy rate (we could also think of this as the per-word\\nentropy rate\\nentropy) as the entropy of this sequence divided by the number of words:\\n1\\nnH(W n\\n1 ) = −1\\nn\\n�\\nW n\\n1 ∈L\\np(W n\\n1 )log p(W n\\n1 )\\n(3.45)\\nBut to measure the true entropy of a language, we need to consider sequences of\\ninﬁnite length. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 203, '_split_overlap': [{'doc_id': 'e35e719a4ee131e5c3014044718fac00', 'range': (0, 187)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c44826c5e97214198b7e35ba441e077a'}>,\n",
       " <Document: {'content': 'If we think of a language as a stochastic process L that produces a\\nsequence of words, and allow W to represent the sequence of words w1,...,wn, then\\nL’s entropy rate H(L) is deﬁned as\\nH(L) = lim\\nn→∞\\n1\\nnH(w1,w2,...,wn)\\n= − lim\\nn→∞\\n1\\nn\\n�\\nW∈L\\np(w1,...,wn)log p(w1,...,wn)\\n(3.46)\\nThe Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and\\nThomas 1991) states that if the language is regular in certain ways (to be exact, if it\\nis both stationary and ergodic),\\nH(L) = lim\\nn→∞−1\\nn log p(w1w2 ...wn)\\n(3.47)\\nThat is, we can take a single sequence that is long enough instead of summing\\nover all possible sequences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 204, '_split_overlap': []}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '634c37f5d90b4fc38d5e7953f9bd8150'}>,\n",
       " <Document: {'content': 'The intuition of the Shannon-McMillan-Breiman the-\\norem is that a long-enough sequence of words will contain in it many other shorter\\nsequences and that each of these shorter sequences will reoccur in the longer se-\\nquence according to their probabilities.\\nA stochastic process is said to be stationary if the probabilities it assigns to a\\nStationary\\nsequence are invariant with respect to shifts in the time index. In other words, the\\nprobability distribution for words at time t is the same as the probability distribution\\nat time t + 1. Markov models, and hence n-grams, are stationary. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 205, '_split_overlap': [{'doc_id': 'd82b1b8856283bd48977d06b70e03756', 'range': (416, 589)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd2ec20631ae7a080b8d7369f4c8d7435'}>,\n",
       " <Document: {'content': 'In other words, the\\nprobability distribution for words at time t is the same as the probability distribution\\nat time t + 1. Markov models, and hence n-grams, are stationary. For example, in\\na bigram, Pi is dependent only on Pi−1. So if we shift our time index by x, Pi+x is\\nstill dependent on Pi+x−1. But natural language is not stationary, since as we show\\nin Chapter 12, the probability of upcoming words can be dependent on events that\\nwere arbitrarily distant and time dependent. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 206, '_split_overlap': [{'doc_id': 'd2ec20631ae7a080b8d7369f4c8d7435', 'range': (0, 173)}, {'doc_id': '18742512c406c1ac8cd582f7fedf3771', 'range': (301, 483)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd82b1b8856283bd48977d06b70e03756'}>,\n",
       " <Document: {'content': 'But natural language is not stationary, since as we show\\nin Chapter 12, the probability of upcoming words can be dependent on events that\\nwere arbitrarily distant and time dependent. Thus, our statistical models only give\\nan approximation to the correct distributions and entropies of natural language.\\nTo summarize, by making some incorrect but convenient simplifying assump-\\ntions, we can compute the entropy of some stochastic process by taking a very long\\nsample of the output and computing its average log probability.\\nNow we are ready to introduce cross-entropy. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 207, '_split_overlap': [{'doc_id': 'd82b1b8856283bd48977d06b70e03756', 'range': (0, 182)}, {'doc_id': '48e4713836cb1fc8ccdfca0267a05614', 'range': (303, 568)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '18742512c406c1ac8cd582f7fedf3771'}>,\n",
       " <Document: {'content': 'To summarize, by making some incorrect but convenient simplifying assump-\\ntions, we can compute the entropy of some stochastic process by taking a very long\\nsample of the output and computing its average log probability.\\nNow we are ready to introduce cross-entropy. The cross-entropy is useful when\\ncross-entropy\\nwe don’t know the actual probability distribution p that generated some data. It\\x0c3.7\\n•\\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\\n23\\nallows us to use some m, which is a model of p (i.e., an approximation to p). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 208, '_split_overlap': [{'doc_id': '18742512c406c1ac8cd582f7fedf3771', 'range': (0, 265)}, {'doc_id': '8b2222e1fac0bccbed7a49c302e7e9b4', 'range': (266, 523)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '48e4713836cb1fc8ccdfca0267a05614'}>,\n",
       " <Document: {'content': 'The cross-entropy is useful when\\ncross-entropy\\nwe don’t know the actual probability distribution p that generated some data. It\\x0c3.7\\n•\\nADVANCED: PERPLEXITY’S RELATION TO ENTROPY\\n23\\nallows us to use some m, which is a model of p (i.e., an approximation to p). The\\ncross-entropy of m on p is deﬁned by\\nH(p,m) = lim\\nn→∞−1\\nn\\n�\\nW∈L\\np(w1,...,wn)logm(w1,...,wn)\\n(3.48)\\nThat is, we draw sequences according to the probability distribution p, but sum\\nthe log of their probabilities according to m.\\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\\ngodic process:\\nH(p,m) = lim\\nn→∞−1\\nn logm(w1w2 ...wn)\\n(3.49)\\nThis means that, as for entropy, we can estimate the cross-entropy of a model\\nm on some distribution p by taking a single sequence that is long enough instead of\\nsumming over all possible sequences.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 209, '_split_overlap': [{'doc_id': '48e4713836cb1fc8ccdfca0267a05614', 'range': (0, 257)}, {'doc_id': '6c5414cfe5d384623054b26348d0ed15', 'range': (258, 822)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8b2222e1fac0bccbed7a49c302e7e9b4'}>,\n",
       " <Document: {'content': 'The\\ncross-entropy of m on p is deﬁned by\\nH(p,m) = lim\\nn→∞−1\\nn\\n�\\nW∈L\\np(w1,...,wn)logm(w1,...,wn)\\n(3.48)\\nThat is, we draw sequences according to the probability distribution p, but sum\\nthe log of their probabilities according to m.\\nAgain, following the Shannon-McMillan-Breiman theorem, for a stationary er-\\ngodic process:\\nH(p,m) = lim\\nn→∞−1\\nn logm(w1w2 ...wn)\\n(3.49)\\nThis means that, as for entropy, we can estimate the cross-entropy of a model\\nm on some distribution p by taking a single sequence that is long enough instead of\\nsumming over all possible sequences.\\nWhat makes the cross-entropy useful is that the cross-entropy H(p,m) is an up-\\nper bound on the entropy H(p). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 210, '_split_overlap': [{'doc_id': '8b2222e1fac0bccbed7a49c302e7e9b4', 'range': (0, 564)}, {'doc_id': 'da5069aa64eeeae9643398e271dd7ba5', 'range': (565, 674)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6c5414cfe5d384623054b26348d0ed15'}>,\n",
       " <Document: {'content': 'What makes the cross-entropy useful is that the cross-entropy H(p,m) is an up-\\nper bound on the entropy H(p). For any model m:\\nH(p) ≤ H(p,m)\\n(3.50)\\nThis means that we can use some simpliﬁed model m to help estimate the true en-\\ntropy of a sequence of symbols drawn according to probability p. The more accurate\\nm is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,\\nthe difference between H(p,m) and H(p) is a measure of how accurate a model is.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 211, '_split_overlap': [{'doc_id': '6c5414cfe5d384623054b26348d0ed15', 'range': (0, 109)}, {'doc_id': '1266be74f2ecd561583bbae09f050582', 'range': (110, 472)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'da5069aa64eeeae9643398e271dd7ba5'}>,\n",
       " <Document: {'content': 'For any model m:\\nH(p) ≤ H(p,m)\\n(3.50)\\nThis means that we can use some simpliﬁed model m to help estimate the true en-\\ntropy of a sequence of symbols drawn according to probability p. The more accurate\\nm is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus,\\nthe difference between H(p,m) and H(p) is a measure of how accurate a model is.\\nBetween two models m1 and m2, the more accurate model will be the one with the\\nlower cross-entropy. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 212, '_split_overlap': [{'doc_id': 'da5069aa64eeeae9643398e271dd7ba5', 'range': (0, 362)}, {'doc_id': '900f07a1fc6c5a7c04b56fc3c96be420', 'range': (277, 462)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1266be74f2ecd561583bbae09f050582'}>,\n",
       " <Document: {'content': 'Thus,\\nthe difference between H(p,m) and H(p) is a measure of how accurate a model is.\\nBetween two models m1 and m2, the more accurate model will be the one with the\\nlower cross-entropy. (The cross-entropy can never be lower than the true entropy, so\\na model cannot err by underestimating the true entropy.)\\nWe are ﬁnally ready to see the relation between perplexity and cross-entropy as\\nwe saw it in Eq. 3.49. Cross-entropy is deﬁned in the limit, as the length of the\\nobserved word sequence goes to inﬁnity. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 213, '_split_overlap': [{'doc_id': '1266be74f2ecd561583bbae09f050582', 'range': (0, 185)}, {'doc_id': '53fdb7abc771c073c2173099881819e4', 'range': (307, 508)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '900f07a1fc6c5a7c04b56fc3c96be420'}>,\n",
       " <Document: {'content': 'We are ﬁnally ready to see the relation between perplexity and cross-entropy as\\nwe saw it in Eq. 3.49. Cross-entropy is deﬁned in the limit, as the length of the\\nobserved word sequence goes to inﬁnity. We will need an approximation to cross-\\nentropy, relying on a (sufﬁciently long) sequence of ﬁxed length. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 214, '_split_overlap': [{'doc_id': '900f07a1fc6c5a7c04b56fc3c96be420', 'range': (0, 201)}, {'doc_id': '75c76c6b76c7e98dd9d13afd502b9492', 'range': (103, 307)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '53fdb7abc771c073c2173099881819e4'}>,\n",
       " <Document: {'content': 'Cross-entropy is deﬁned in the limit, as the length of the\\nobserved word sequence goes to inﬁnity. We will need an approximation to cross-\\nentropy, relying on a (sufﬁciently long) sequence of ﬁxed length. This approxima-\\ntion to the cross-entropy of a model M = P(wi|wi−N+1...wi−1) on a sequence of\\nwords W is\\nH(W) = − 1\\nN logP(w1w2 ...wN)\\n(3.51)\\nThe perplexity of a model P on a sequence of words W is now formally deﬁned as\\nperplexity\\nthe exp of this cross-entropy:\\nPerplexity(W) = 2H(W)\\n= P(w1w2 ...wN)− 1\\nN\\n=\\nN\\n�\\n1\\nP(w1w2 ...wN)\\n=\\nN\\n�\\n�\\n�\\n�\\nN\\n�\\ni=1\\n1\\nP(wi|w1 ...wi−1)\\n(3.52)\\x0c24\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\n3.8\\nSummary\\nThis chapter introduced language modeling and the n-gram, one of the most widely\\nused tools in language processing.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 215, '_split_overlap': [{'doc_id': '53fdb7abc771c073c2173099881819e4', 'range': (0, 204)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '75c76c6b76c7e98dd9d13afd502b9492'}>,\n",
       " <Document: {'content': '• Language models offer a way to assign a probability to a sentence or other\\nsequence of words, and to predict a word from preceding words.\\n• n-grams are Markov models that estimate words from a ﬁxed window of pre-\\nvious words. n-gram probabilities can be estimated by counting in a corpus\\nand normalizing (the maximum likelihood estimate).\\n• n-gram language models are evaluated extrinsically in some task, or intrinsi-\\ncally using perplexity.\\n• The perplexity of a test set according to a language model is the geometric\\nmean of the inverse test set probability computed by the model.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 216, '_split_overlap': [{'doc_id': '5721da3fd0245b027270e15aa6b56dd8', 'range': (341, 586)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '86b61e27239a1a473386ceab86084e72'}>,\n",
       " <Document: {'content': '• n-gram language models are evaluated extrinsically in some task, or intrinsi-\\ncally using perplexity.\\n• The perplexity of a test set according to a language model is the geometric\\nmean of the inverse test set probability computed by the model.\\n• Smoothing algorithms provide a more sophisticated way to estimate the prob-\\nability of n-grams. Commonly used smoothing algorithms for n-grams rely on\\nlower-order n-gram counts through backoff or interpolation.\\n• Both backoff and interpolation require discounting to create a probability dis-\\ntribution.\\n• Kneser-Ney smoothing makes use of the probability of a word being a novel\\ncontinuation. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 217, '_split_overlap': [{'doc_id': '86b61e27239a1a473386ceab86084e72', 'range': (0, 245)}, {'doc_id': '437c98a73d159fcca6e2be2b5f2ce336', 'range': (344, 641)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5721da3fd0245b027270e15aa6b56dd8'}>,\n",
       " <Document: {'content': 'Commonly used smoothing algorithms for n-grams rely on\\nlower-order n-gram counts through backoff or interpolation.\\n• Both backoff and interpolation require discounting to create a probability dis-\\ntribution.\\n• Kneser-Ney smoothing makes use of the probability of a word being a novel\\ncontinuation. The interpolated Kneser-Ney smoothing algorithm mixes a\\ndiscounted probability with a lower-order continuation probability.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 218, '_split_overlap': [{'doc_id': '5721da3fd0245b027270e15aa6b56dd8', 'range': (0, 297)}, {'doc_id': '5c275ee19c7b1826193ab242c24438ac', 'range': (115, 421)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '437c98a73d159fcca6e2be2b5f2ce336'}>,\n",
       " <Document: {'content': '• Both backoff and interpolation require discounting to create a probability dis-\\ntribution.\\n• Kneser-Ney smoothing makes use of the probability of a word being a novel\\ncontinuation. The interpolated Kneser-Ney smoothing algorithm mixes a\\ndiscounted probability with a lower-order continuation probability.\\nBibliographical and Historical Notes\\nThe underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),\\nwho used what are now called Markov chains (bigrams and trigrams) to predict\\nwhether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-\\nsonant. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 219, '_split_overlap': [{'doc_id': '437c98a73d159fcca6e2be2b5f2ce336', 'range': (0, 306)}, {'doc_id': '4f1fc6b961a31c637d44434f28635bd3', 'range': (307, 586)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5c275ee19c7b1826193ab242c24438ac'}>,\n",
       " <Document: {'content': 'Bibliographical and Historical Notes\\nThe underlying mathematics of the n-gram was ﬁrst proposed by Markov (1913),\\nwho used what are now called Markov chains (bigrams and trigrams) to predict\\nwhether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a con-\\nsonant. Markov classiﬁed 20,000 letters as V or C and computed the bigram and\\ntrigram probability that a given letter would be a vowel given the previous one or\\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\\nword sequences. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 220, '_split_overlap': [{'doc_id': '5c275ee19c7b1826193ab242c24438ac', 'range': (0, 279)}, {'doc_id': 'dcd3b998139f7c8d29a288bf33f600f4', 'range': (280, 529)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4f1fc6b961a31c637d44434f28635bd3'}>,\n",
       " <Document: {'content': 'Markov classiﬁed 20,000 letters as V or C and computed the bigram and\\ntrigram probability that a given letter would be a vowel given the previous one or\\ntwo letters. Shannon (1948) applied n-grams to compute approximations to English\\nword sequences. Based on Shannon’s work, Markov models were commonly used in\\nengineering, linguistic, and psychological work on modeling word sequences by the\\n1950s. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 221, '_split_overlap': [{'doc_id': '4f1fc6b961a31c637d44434f28635bd3', 'range': (0, 249)}, {'doc_id': '346005b8753f3da2fe4973c58090e45a', 'range': (166, 399)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'dcd3b998139f7c8d29a288bf33f600f4'}>,\n",
       " <Document: {'content': 'Shannon (1948) applied n-grams to compute approximations to English\\nword sequences. Based on Shannon’s work, Markov models were commonly used in\\nengineering, linguistic, and psychological work on modeling word sequences by the\\n1950s. In a series of extremely inﬂuential papers starting with Chomsky (1956) and\\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\\nthat “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,\\nwere incapable of being a complete cognitive model of human grammatical knowl-\\nedge. These arguments led many linguists and computational linguists to ignore\\nwork in statistical modeling for decades.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 222, '_split_overlap': [{'doc_id': 'dcd3b998139f7c8d29a288bf33f600f4', 'range': (0, 233)}, {'doc_id': '825d4910eacbefb41b81cc82c95bef11', 'range': (234, 669)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '346005b8753f3da2fe4973c58090e45a'}>,\n",
       " <Document: {'content': 'In a series of extremely inﬂuential papers starting with Chomsky (1956) and\\nincluding Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued\\nthat “ﬁnite-state Markov processes”, while a possibly useful engineering heuristic,\\nwere incapable of being a complete cognitive model of human grammatical knowl-\\nedge. These arguments led many linguists and computational linguists to ignore\\nwork in statistical modeling for decades.\\nThe resurgence of n-gram models came from Jelinek and colleagues at the IBM\\nThomas J. Watson Research Center, who were inﬂuenced by Shannon, and Baker\\nat CMU, who was inﬂuenced by the work of Baum and colleagues. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 223, '_split_overlap': [{'doc_id': '346005b8753f3da2fe4973c58090e45a', 'range': (0, 435)}, {'doc_id': 'a66c5bd77160fbe59cc80d0224233358', 'range': (436, 648)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '825d4910eacbefb41b81cc82c95bef11'}>,\n",
       " <Document: {'content': 'The resurgence of n-gram models came from Jelinek and colleagues at the IBM\\nThomas J. Watson Research Center, who were inﬂuenced by Shannon, and Baker\\nat CMU, who was inﬂuenced by the work of Baum and colleagues. Independently\\nthese two labs successfully used n-grams in their speech recognition systems (Baker 1975b,\\nJelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). A trigram model was\\nused in the IBM TANGORA speech recognition system in the 1970s, but the idea\\nwas not written up until later.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 224, '_split_overlap': [{'doc_id': '825d4910eacbefb41b81cc82c95bef11', 'range': (0, 212)}, {'doc_id': '6be766b01af9f7f49097baa0aac1b951', 'range': (213, 506)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a66c5bd77160fbe59cc80d0224233358'}>,\n",
       " <Document: {'content': 'Independently\\nthese two labs successfully used n-grams in their speech recognition systems (Baker 1975b,\\nJelinek 1976, Baker 1975a, Bahl et al. 1983, Jelinek 1990). A trigram model was\\nused in the IBM TANGORA speech recognition system in the 1970s, but the idea\\nwas not written up until later.\\nAdd-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst\\napplied as an engineering solution to the zero-frequency problem by Jeffreys (1948)\\x0cEXERCISES\\n25\\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\\none algorithm are summarized in Gale and Church (1994).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 225, '_split_overlap': [{'doc_id': 'a66c5bd77160fbe59cc80d0224233358', 'range': (0, 293)}, {'doc_id': '69c81aa08a9d871930678ff517d86540', 'range': (294, 602)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6be766b01af9f7f49097baa0aac1b951'}>,\n",
       " <Document: {'content': 'Add-one smoothing derives from Laplace’s 1812 law of succession and was ﬁrst\\napplied as an engineering solution to the zero-frequency problem by Jeffreys (1948)\\x0cEXERCISES\\n25\\nbased on an earlier Add-K suggestion by Johnson (1932). Problems with the add-\\none algorithm are summarized in Gale and Church (1994).\\nA wide variety of different language modeling and smoothing techniques were\\nproposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to\\nthe n-gram smoothing at IBM by Katz (N´adas 1984, Church and Gale 1991)—\\nWitten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-\\ngram models that used information about word classes.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 226, '_split_overlap': [{'doc_id': '6be766b01af9f7f49097baa0aac1b951', 'range': (0, 308)}, {'doc_id': 'd7c6255ec24dfbe94fdd133c1ed4f8c8', 'range': (309, 671)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '69c81aa08a9d871930678ff517d86540'}>,\n",
       " <Document: {'content': 'A wide variety of different language modeling and smoothing techniques were\\nproposed in the 80s and 90s, including Good-Turing discounting—ﬁrst applied to\\nthe n-gram smoothing at IBM by Katz (N´adas 1984, Church and Gale 1991)—\\nWitten-Bell discounting (Witten and Bell, 1991), and varieties of class-based n-\\ngram models that used information about word classes.\\nclass-based\\nn-gram\\nStarting in the late 1990s, Chen and Goodman produced a highly inﬂuential\\nseries of papers with a comparison of different language models (Chen and Good-\\nman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 227, '_split_overlap': [{'doc_id': '69c81aa08a9d871930678ff517d86540', 'range': (0, 362)}, {'doc_id': 'a5ae82e8f0009188ebed2380fea6cf61', 'range': (363, 606)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd7c6255ec24dfbe94fdd133c1ed4f8c8'}>,\n",
       " <Document: {'content': 'class-based\\nn-gram\\nStarting in the late 1990s, Chen and Goodman produced a highly inﬂuential\\nseries of papers with a comparison of different language models (Chen and Good-\\nman 1996, Chen and Goodman 1998, Chen and Goodman 1999, Goodman 2006).\\nThey performed a number of carefully controlled experiments comparing differ-\\nent discounting algorithms, cache models, class-based models, and other language\\nmodel parameters. They showed the advantages of Modiﬁed Interpolated Kneser-\\nNey, which has since become the standard baseline for language modeling, espe-\\ncially because they showed that caches and class-based models provided only minor\\nadditional improvement. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 228, '_split_overlap': [{'doc_id': 'd7c6255ec24dfbe94fdd133c1ed4f8c8', 'range': (0, 243)}, {'doc_id': 'c2019d146838b3406f39814277b730a1', 'range': (421, 664)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a5ae82e8f0009188ebed2380fea6cf61'}>,\n",
       " <Document: {'content': 'They showed the advantages of Modiﬁed Interpolated Kneser-\\nNey, which has since become the standard baseline for language modeling, espe-\\ncially because they showed that caches and class-based models provided only minor\\nadditional improvement. These papers are recommended for any reader with further\\ninterest in language modeling.\\nTwo commonly used toolkits for building language models are SRILM (Stolcke,\\n2002) and KenLM (Heaﬁeld 2011, Heaﬁeld et al. 2013). Both are publicly available.\\nSRILM offers a wider range of options and types of discounting, while KenLM is\\noptimized for speed and memory size, making it possible to build web-scale lan-\\nguage models.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 229, '_split_overlap': [{'doc_id': 'a5ae82e8f0009188ebed2380fea6cf61', 'range': (0, 243)}, {'doc_id': '239f1fb0bcbfa6e79bdee7e6d9d23ce7', 'range': (461, 662)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c2019d146838b3406f39814277b730a1'}>,\n",
       " <Document: {'content': 'Both are publicly available.\\nSRILM offers a wider range of options and types of discounting, while KenLM is\\noptimized for speed and memory size, making it possible to build web-scale lan-\\nguage models.\\nThe highest accuracy language models are neural network language models.\\nThese solve a major problem with n-gram language models: the number of parame-\\nters increases exponentially as the n-gram order increases, and n-grams have no way\\nto generalize from training to test set. Neural language models instead project words\\ninto a continuous space in which words with similar contexts have similar represen-\\ntations. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 230, '_split_overlap': [{'doc_id': 'c2019d146838b3406f39814277b730a1', 'range': (0, 201)}, {'doc_id': 'a2578e173453fd5f00a2eb416427f496', 'range': (275, 616)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '239f1fb0bcbfa6e79bdee7e6d9d23ce7'}>,\n",
       " <Document: {'content': 'These solve a major problem with n-gram language models: the number of parame-\\nters increases exponentially as the n-gram order increases, and n-grams have no way\\nto generalize from training to test set. Neural language models instead project words\\ninto a continuous space in which words with similar contexts have similar represen-\\ntations. We’ll introduce both feedforward language models (Bengio et al. 2006,\\nSchwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in\\nChapter 9.\\nExercises\\n3.1\\nWrite out the equation for trigram probability estimation (modifying Eq. 3.11).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 231, '_split_overlap': [{'doc_id': '239f1fb0bcbfa6e79bdee7e6d9d23ce7', 'range': (0, 341)}, {'doc_id': '74d07f683cd157c128f0aa1ffd130053', 'range': (342, 593)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a2578e173453fd5f00a2eb416427f496'}>,\n",
       " <Document: {'content': 'We’ll introduce both feedforward language models (Bengio et al. 2006,\\nSchwenk 2007) in Chapter 7, and recurrent language models (Mikolov, 2012) in\\nChapter 9.\\nExercises\\n3.1\\nWrite out the equation for trigram probability estimation (modifying Eq. 3.11).\\nNow write out all the non-zero trigram probabilities for the I am Sam corpus\\non page 4.\\n3.2\\nCalculate the probability of the sentence i want chinese food. Give two\\nprobabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on\\npage 6, and another using the add-1 smoothed table in Fig. 3.6. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 232, '_split_overlap': [{'doc_id': 'a2578e173453fd5f00a2eb416427f496', 'range': (0, 251)}, {'doc_id': 'fb780e105e8081441c5c824e423a7841', 'range': (340, 561)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '74d07f683cd157c128f0aa1ffd130053'}>,\n",
       " <Document: {'content': '3.2\\nCalculate the probability of the sentence i want chinese food. Give two\\nprobabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on\\npage 6, and another using the add-1 smoothed table in Fig. 3.6. Assume the\\nadditional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =\\n0.40.\\n3.3\\nWhich of the two probabilities you computed in the previous exercise is higher,\\nunsmoothed or smoothed? Explain why.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 233, '_split_overlap': [{'doc_id': '74d07f683cd157c128f0aa1ffd130053', 'range': (0, 221)}, {'doc_id': 'd79f935ecb80a6e50fd9c963b78b7f46', 'range': (222, 434)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'fb780e105e8081441c5c824e423a7841'}>,\n",
       " <Document: {'content': 'Assume the\\nadditional add-1 smoothed probabilities P(i|<s>) = 0.19 and P(</s>|food) =\\n0.40.\\n3.3\\nWhich of the two probabilities you computed in the previous exercise is higher,\\nunsmoothed or smoothed? Explain why.\\n3.4\\nWe are given the following corpus, modiﬁed from the one in the chapter:\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\x0c26\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nUsing a bigram language model with add-one smoothing, what is P(Sam |\\nam)? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 234, '_split_overlap': [{'doc_id': 'fb780e105e8081441c5c824e423a7841', 'range': (0, 212)}, {'doc_id': '98ff18c70deaa6e88172c39cca2313a2', 'range': (213, 497)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd79f935ecb80a6e50fd9c963b78b7f46'}>,\n",
       " <Document: {'content': '3.4\\nWe are given the following corpus, modiﬁed from the one in the chapter:\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\x0c26\\nCHAPTER 3\\n•\\nN-GRAM LANGUAGE MODELS\\nUsing a bigram language model with add-one smoothing, what is P(Sam |\\nam)? Include <s> and </s> in your counts just like any other token.\\n3.5\\nSuppose we didn’t use the end-symbol </s>. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 235, '_split_overlap': [{'doc_id': 'd79f935ecb80a6e50fd9c963b78b7f46', 'range': (0, 284)}, {'doc_id': 'e66f0ac87b97c34b1cd58f761a6bae48', 'range': (285, 394)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98ff18c70deaa6e88172c39cca2313a2'}>,\n",
       " <Document: {'content': 'Include <s> and </s> in your counts just like any other token.\\n3.5\\nSuppose we didn’t use the end-symbol </s>. Train an unsmoothed bigram\\ngrammar on the following training corpus without using the end-symbol </s>:\\n<s> a b\\n<s> b b\\n<s> b a\\n<s> a a\\nDemonstrate that your bigram model does not assign a single probability dis-\\ntribution across all sentence lengths by showing that the sum of the probability\\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\\nis also 1.0.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 236, '_split_overlap': [{'doc_id': '98ff18c70deaa6e88172c39cca2313a2', 'range': (0, 109)}, {'doc_id': '117a4d1eb7e14f1af487642626b4d71e', 'range': (110, 573)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'e66f0ac87b97c34b1cd58f761a6bae48'}>,\n",
       " <Document: {'content': 'Train an unsmoothed bigram\\ngrammar on the following training corpus without using the end-symbol </s>:\\n<s> a b\\n<s> b b\\n<s> b a\\n<s> a a\\nDemonstrate that your bigram model does not assign a single probability dis-\\ntribution across all sentence lengths by showing that the sum of the probability\\nof the four possible 2 word sentences over the alphabet {a,b} is 1.0, and the\\nsum of the probability of all possible 3 word sentences over the alphabet {a,b}\\nis also 1.0.\\n3.6\\nSuppose we train a trigram language model with add-one smoothing on a\\ngiven corpus. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 237, '_split_overlap': [{'doc_id': 'e66f0ac87b97c34b1cd58f761a6bae48', 'range': (0, 463)}, {'doc_id': '4a994ad12aecfd67cf621e2836c87b14', 'range': (464, 551)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '117a4d1eb7e14f1af487642626b4d71e'}>,\n",
       " <Document: {'content': '3.6\\nSuppose we train a trigram language model with add-one smoothing on a\\ngiven corpus. The corpus contains V word types. Express a formula for esti-\\nmating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\\nin terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to\\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\\nso on for bigrams and unigrams.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 238, '_split_overlap': [{'doc_id': '117a4d1eb7e14f1af487642626b4d71e', 'range': (0, 87)}, {'doc_id': '2d78dbaf2a2f75509cc47c718c8878a3', 'range': (122, 404)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4a994ad12aecfd67cf621e2836c87b14'}>,\n",
       " <Document: {'content': 'Express a formula for esti-\\nmating P(w3|w1,w2), where w3 is a word which follows the bigram (w1,w2),\\nin terms of various N-gram counts and V. Use the notation c(w1,w2,w3) to\\ndenote the number of times that trigram (w1,w2,w3) occurs in the corpus, and\\nso on for bigrams and unigrams.\\n3.7\\nWe are given the following corpus, modiﬁed from the one in the chapter:\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\\ngram model and a maximum-likelihood unigram model with λ1 = 1\\n2 and λ2 =\\n1\\n2, what is P(Sam|am)? ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 239, '_split_overlap': [{'doc_id': '4a994ad12aecfd67cf621e2836c87b14', 'range': (0, 282)}, {'doc_id': 'ac6f181a1bfc49442f44a1a67579a36a', 'range': (283, 625)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2d78dbaf2a2f75509cc47c718c8878a3'}>,\n",
       " <Document: {'content': '3.7\\nWe are given the following corpus, modiﬁed from the one in the chapter:\\n<s> I am Sam </s>\\n<s> Sam I am </s>\\n<s> I am Sam </s>\\n<s> I do not like green eggs and Sam </s>\\nIf we use linear interpolation smoothing between a maximum-likelihood bi-\\ngram model and a maximum-likelihood unigram model with λ1 = 1\\n2 and λ2 =\\n1\\n2, what is P(Sam|am)? Include <s> and </s> in your counts just like any\\nother token.\\n3.8\\nWrite a program to compute unsmoothed unigrams and bigrams.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 240, '_split_overlap': [{'doc_id': '2d78dbaf2a2f75509cc47c718c8878a3', 'range': (0, 342)}, {'doc_id': 'b95460512274b283b99a728c892b13ae', 'range': (343, 469)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ac6f181a1bfc49442f44a1a67579a36a'}>,\n",
       " <Document: {'content': 'Include <s> and </s> in your counts just like any\\nother token.\\n3.8\\nWrite a program to compute unsmoothed unigrams and bigrams.\\n3.9\\nRun your n-gram program on two different small corpora of your choice (you\\nmight use email text or newsgroups). Now compare the statistics of the two\\ncorpora. What are the differences in the most common unigrams between the\\ntwo? How about interesting differences in bigrams?\\n3.10 Add an option to your program to generate random sentences.\\n3.11 Add an option to your program to compute the perplexity of a test set.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 241, '_split_overlap': [{'doc_id': 'ac6f181a1bfc49442f44a1a67579a36a', 'range': (0, 126)}, {'doc_id': '71d45e4c3040bb8878c6b54be962f9d1', 'range': (360, 546)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'b95460512274b283b99a728c892b13ae'}>,\n",
       " <Document: {'content': 'How about interesting differences in bigrams?\\n3.10 Add an option to your program to generate random sentences.\\n3.11 Add an option to your program to compute the perplexity of a test set.\\n3.12 Given a training set of 100 numbers consists of 91 zeros and 1 each of the\\nother digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What\\nis the unigram perplexity?\\x0cExercises\\n27\\nAlgoet, P. H. and Cover, T. M. (1988). A sandwich proof of\\nthe Shannon-McMillan-Breiman theorem. The Annals of\\nProbability, 16(2), 899–909.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 242, '_split_overlap': [{'doc_id': 'b95460512274b283b99a728c892b13ae', 'range': (0, 186)}, {'doc_id': '258ab9524e2b6de04cdde1ba46305169', 'range': (285, 525)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '71d45e4c3040bb8878c6b54be962f9d1'}>,\n",
       " <Document: {'content': 'Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What\\nis the unigram perplexity?\\x0cExercises\\n27\\nAlgoet, P. H. and Cover, T. M. (1988). A sandwich proof of\\nthe Shannon-McMillan-Breiman theorem. The Annals of\\nProbability, 16(2), 899–909.\\nBahl, L. R., Jelinek, F., and Mercer, R. L. (1983). A max-\\nimum likelihood approach to continuous speech recogni-\\ntion. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 5(2), 179–190.\\nBaker, J. K. (1975a). The DRAGON system – An overview.\\nIEEE Transactions on Acoustics, Speech, and Signal Pro-\\ncessing, ASSP-23(1), 24–29.\\nBaker, J. K. (1975b).\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 243, '_split_overlap': [{'doc_id': '71d45e4c3040bb8878c6b54be962f9d1', 'range': (0, 240)}, {'doc_id': '67ec25810b1f43b6cc8cd0b060c452a3', 'range': (361, 600)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '258ab9524e2b6de04cdde1ba46305169'}>,\n",
       " <Document: {'content': 'IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 5(2), 179–190.\\nBaker, J. K. (1975a). The DRAGON system – An overview.\\nIEEE Transactions on Acoustics, Speech, and Signal Pro-\\ncessing, ASSP-23(1), 24–29.\\nBaker, J. K. (1975b).\\nStochastic modeling for automatic\\nspeech understanding.\\nIn Reddy, D. R. (Ed.), Speech\\nRecognition. Academic Press.\\nBengio, Y., Schwenk, H., Sen´ecal, J.-S., Morin, F., and Gau-\\nvain, J.-L. (2006). Neural probabilistic language models.\\nIn Innovations in Machine Learning, 137–186. Springer.\\nBlodgett, S. L. and O’Connor, B. (2017). Racial disparity in\\nnatural language processing: A case study of social media\\nafrican-american english. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 244, '_split_overlap': [{'doc_id': '258ab9524e2b6de04cdde1ba46305169', 'range': (0, 239)}, {'doc_id': '62ac7d31ed3bea46a9e3fa9953e8f87c', 'range': (437, 674)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '67ec25810b1f43b6cc8cd0b060c452a3'}>,\n",
       " <Document: {'content': 'Neural probabilistic language models.\\nIn Innovations in Machine Learning, 137–186. Springer.\\nBlodgett, S. L. and O’Connor, B. (2017). Racial disparity in\\nnatural language processing: A case study of social media\\nafrican-american english. In Fairness, Accountability, and\\nTransparency in Machine Learning (FAT/ML) Workshop,\\nKDD.\\nBrants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.\\n(2007). Large language models in machine translation. In\\nEMNLP/CoNLL 2007.\\nBuck, C., Heaﬁeld, K., and Van Ooyen, B. (2014). N-gram\\ncounts and language models from the common crawl. In\\nProceedings of LREC.\\nChen, S. F. and Goodman, J. (1996). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 245, '_split_overlap': [{'doc_id': '67ec25810b1f43b6cc8cd0b060c452a3', 'range': (0, 237)}, {'doc_id': '2bf3e3b35f64397c3aa66d84185c8d12', 'range': (441, 627)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '62ac7d31ed3bea46a9e3fa9953e8f87c'}>,\n",
       " <Document: {'content': 'In\\nEMNLP/CoNLL 2007.\\nBuck, C., Heaﬁeld, K., and Van Ooyen, B. (2014). N-gram\\ncounts and language models from the common crawl. In\\nProceedings of LREC.\\nChen, S. F. and Goodman, J. (1996). An empirical study of\\nsmoothing techniques for language modeling. In ACL-96,\\n310–318.\\nChen, S. F. and Goodman, J. (1998). An empirical study of\\nsmoothing techniques for language modeling. Tech. rep.\\nTR-10-98, Computer Science Group, Harvard University.\\nChen, S. F. and Goodman, J. (1999). An empirical study of\\nsmoothing techniques for language modeling. Computer\\nSpeech and Language, 13, 359–394.\\nChomsky, N. (1956). Three models for the description of\\nlanguage. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 246, '_split_overlap': [{'doc_id': '62ac7d31ed3bea46a9e3fa9953e8f87c', 'range': (0, 186)}, {'doc_id': '8f0304c8dd22b366cad4b0a5f6715bd3', 'range': (440, 650)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2bf3e3b35f64397c3aa66d84185c8d12'}>,\n",
       " <Document: {'content': 'Chen, S. F. and Goodman, J. (1999). An empirical study of\\nsmoothing techniques for language modeling. Computer\\nSpeech and Language, 13, 359–394.\\nChomsky, N. (1956). Three models for the description of\\nlanguage. IRE Transactions on Information Theory, 2(3),\\n113–124.\\nChomsky, N. (1957).\\nSyntactic Structures.\\nMouton, The\\nHague.\\nChurch, K. W. and Gale, W. A. (1991). A comparison of\\nthe enhanced Good-Turing and deleted estimation methods\\nfor estimating probabilities of English bigrams. Computer\\nSpeech and Language, 5, 19–54.\\nChurch, K. W., Hart, T., and Gao, J. (2007).\\nCompress-\\ning trigram language models with Golomb coding.\\nIn\\nEMNLP/CoNLL 2007, 199–207.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 247, '_split_overlap': [{'doc_id': '2bf3e3b35f64397c3aa66d84185c8d12', 'range': (0, 210)}, {'doc_id': 'a06d5b5520b2ce02d73dd36d96f6249', 'range': (365, 658)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '8f0304c8dd22b366cad4b0a5f6715bd3'}>,\n",
       " <Document: {'content': 'A comparison of\\nthe enhanced Good-Turing and deleted estimation methods\\nfor estimating probabilities of English bigrams. Computer\\nSpeech and Language, 5, 19–54.\\nChurch, K. W., Hart, T., and Gao, J. (2007).\\nCompress-\\ning trigram language models with Golomb coding.\\nIn\\nEMNLP/CoNLL 2007, 199–207.\\nCover, T. M. and Thomas, J. A. (1991). Elements of Infor-\\nmation Theory. Wiley.\\nFranz, A. and Brants, T. (2006). All our n-gram are belong to\\nyou. http://googleresearch.blogspot.com/2006/\\n08/all-our-n-gram-are-belong-to-you.html.\\nGale, W. A. and Church, K. W. (1994).\\nWhat is wrong\\nwith adding one?. In Oostdijk, N. and de Haan, P. (Eds.),\\nCorpus-Based Research into Language, 189–198. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 248, '_split_overlap': [{'doc_id': '8f0304c8dd22b366cad4b0a5f6715bd3', 'range': (0, 293)}, {'doc_id': 'c366ba1d6d8a657a8bc2a77760363371', 'range': (441, 679)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a06d5b5520b2ce02d73dd36d96f6249'}>,\n",
       " <Document: {'content': 'http://googleresearch.blogspot.com/2006/\\n08/all-our-n-gram-are-belong-to-you.html.\\nGale, W. A. and Church, K. W. (1994).\\nWhat is wrong\\nwith adding one?. In Oostdijk, N. and de Haan, P. (Eds.),\\nCorpus-Based Research into Language, 189–198. Rodopi.\\nGoodman, J. (2006). A bit of progress in language mod-\\neling: Extended version.\\nTech. rep. MSR-TR-2001-72,\\nMachine Learning and Applied Statistics Group, Microsoft\\nResearch, Redmond, WA.\\nHeaﬁeld, K. (2011). KenLM: Faster and smaller language\\nmodel queries. In Workshop on Statistical Machine Trans-\\nlation, 187–197.\\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.\\n(2013). Scalable modiﬁed Kneser-Ney language model es-\\ntimation.. In ACL 2013, 690–696.\\nJeffreys, H. (1948). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 249, '_split_overlap': [{'doc_id': 'a06d5b5520b2ce02d73dd36d96f6249', 'range': (0, 238)}, {'doc_id': '2cb014014d2ddf962a5bbc9fc0647ff9', 'range': (504, 729)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'c366ba1d6d8a657a8bc2a77760363371'}>,\n",
       " <Document: {'content': 'In Workshop on Statistical Machine Trans-\\nlation, 187–197.\\nHeaﬁeld, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.\\n(2013). Scalable modiﬁed Kneser-Ney language model es-\\ntimation.. In ACL 2013, 690–696.\\nJeffreys, H. (1948). Theory of Probability (2nd Ed.). Claren-\\ndon Press. Section 3.23.\\nJelinek, F. (1976). Continuous speech recognition by statis-\\ntical methods. Proceedings of the IEEE, 64(4), 532–557.\\nJelinek, F. (1990).\\nSelf-organized language modeling for\\nspeech recognition. In Waibel, A. and Lee, K.-F. (Eds.),\\nReadings in Speech Recognition, 450–506. Morgan Kauf-\\nmann. Originally distributed as IBM technical report in\\n1985.\\nJelinek, F. and Mercer, R. L. (1980). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 250, '_split_overlap': [{'doc_id': 'c366ba1d6d8a657a8bc2a77760363371', 'range': (0, 225)}, {'doc_id': '99864a867f0402c88bcda707098afb19', 'range': (486, 676)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '2cb014014d2ddf962a5bbc9fc0647ff9'}>,\n",
       " <Document: {'content': 'In Waibel, A. and Lee, K.-F. (Eds.),\\nReadings in Speech Recognition, 450–506. Morgan Kauf-\\nmann. Originally distributed as IBM technical report in\\n1985.\\nJelinek, F. and Mercer, R. L. (1980). Interpolated estimation\\nof Markov source parameters from sparse data. In Gelsema,\\nE. S. and Kanal, L. N. (Eds.), Proceedings, Workshop on\\nPattern Recognition in Practice, 381–397. North Holland.\\nJohnson, W. E. (1932). Probability: deductive and inductive\\nproblems (appendix to). Mind, 41(164), 421–423.\\nJurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke,\\nA., Fosler, E., and Morgan, N. (1994). The Berkeley restau-\\nrant project. In ICSLP-94, 2139–2142.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 251, '_split_overlap': [{'doc_id': '2cb014014d2ddf962a5bbc9fc0647ff9', 'range': (0, 190)}, {'doc_id': '4d798c00a8f57ea1c4f2c23336a13b21', 'range': (409, 652)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '99864a867f0402c88bcda707098afb19'}>,\n",
       " <Document: {'content': 'Probability: deductive and inductive\\nproblems (appendix to). Mind, 41(164), 421–423.\\nJurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke,\\nA., Fosler, E., and Morgan, N. (1994). The Berkeley restau-\\nrant project. In ICSLP-94, 2139–2142.\\nJurgens, D., Tsvetkov, Y., and Jurafsky, D. (2017). Incorpo-\\nrating dialectal variability for socially equitable language\\nidentiﬁcation. In ACL 2017, 51–57.\\nKane, S. K., Morris, M. R., Paradiso, A., and Campbell, J.\\n(2017).\\n“at times avuncular and cantankerous, with the\\nreﬂexes of a mongoose”: Understanding self-expression\\nthrough augmentative and alternative communication de-\\nvices. In CSCW 2017, 1166–1179.\\nKneser, R. and Ney, H. (1995). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 252, '_split_overlap': [{'doc_id': '99864a867f0402c88bcda707098afb19', 'range': (0, 243)}, {'doc_id': 'd6e8ac6d4682acaa1dab647cff77b161', 'range': (468, 686)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '4d798c00a8f57ea1c4f2c23336a13b21'}>,\n",
       " <Document: {'content': '“at times avuncular and cantankerous, with the\\nreﬂexes of a mongoose”: Understanding self-expression\\nthrough augmentative and alternative communication de-\\nvices. In CSCW 2017, 1166–1179.\\nKneser, R. and Ney, H. (1995). Improved backing-off for M-\\ngram language modeling. In ICASSP-95, Vol. 1, 181–184.\\nMarkov, A. A. (1913). Essai d’une recherche statistique sur\\nle texte du roman “Eugene Onegin” illustrant la liaison des\\nepreuve en chain (‘Example of a statistical investigation of\\nthe text of “Eugene Onegin” illustrating the dependence be-\\ntween samples in chain’). Izvistia Imperatorskoi Akademii\\nNauk (Bulletin de l’Acad´emie Imp´eriale des Sciences de\\nSt.-P´etersbourg), 7, 153–162.\\nMikolov, T. (2012). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 253, '_split_overlap': [{'doc_id': '4d798c00a8f57ea1c4f2c23336a13b21', 'range': (0, 218)}, {'doc_id': '343d47c301c4ad91914c5b78a7fa5999', 'range': (324, 708)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd6e8ac6d4682acaa1dab647cff77b161'}>,\n",
       " <Document: {'content': 'Essai d’une recherche statistique sur\\nle texte du roman “Eugene Onegin” illustrant la liaison des\\nepreuve en chain (‘Example of a statistical investigation of\\nthe text of “Eugene Onegin” illustrating the dependence be-\\ntween samples in chain’). Izvistia Imperatorskoi Akademii\\nNauk (Bulletin de l’Acad´emie Imp´eriale des Sciences de\\nSt.-P´etersbourg), 7, 153–162.\\nMikolov, T. (2012). Statistical language models based on\\nneural networks. Ph.D. thesis, Ph. D. thesis, Brno Univer-\\nsity of Technology.\\nMiller, G. A. and Chomsky, N. (1963). Finitary models of\\nlanguage users. In Luce, R. D., Bush, R. R., and Galanter,\\nE. (Eds.), Handbook of Mathematical Psychology, Vol. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 254, '_split_overlap': [{'doc_id': 'd6e8ac6d4682acaa1dab647cff77b161', 'range': (0, 384)}, {'doc_id': '5721c7227a31b21b08ce924c164a521e', 'range': (457, 669)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '343d47c301c4ad91914c5b78a7fa5999'}>,\n",
       " <Document: {'content': 'D. thesis, Brno Univer-\\nsity of Technology.\\nMiller, G. A. and Chomsky, N. (1963). Finitary models of\\nlanguage users. In Luce, R. D., Bush, R. R., and Galanter,\\nE. (Eds.), Handbook of Mathematical Psychology, Vol. II,\\n419–491. John Wiley.\\nMiller, G. A. and Selfridge, J. A. (1950).\\nVerbal context\\nand the recall of meaningful material. American Journal of\\nPsychology, 63, 176–185.\\nN´adas, A. (1984). Estimation of probabilities in the language\\nmodel of the IBM speech recognition system. IEEE Trans-\\nactions on Acoustics, Speech, Signal Processing, 32(4),\\n859–861.\\nSchwenk, H. (2007). Continuous space language models.\\n', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 255, '_split_overlap': [{'doc_id': '343d47c301c4ad91914c5b78a7fa5999', 'range': (0, 212)}, {'doc_id': 'a3c17c3b1ec9e4892d0607e41a08192a', 'range': (399, 617)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '5721c7227a31b21b08ce924c164a521e'}>,\n",
       " <Document: {'content': 'Estimation of probabilities in the language\\nmodel of the IBM speech recognition system. IEEE Trans-\\nactions on Acoustics, Speech, Signal Processing, 32(4),\\n859–861.\\nSchwenk, H. (2007). Continuous space language models.\\nComputer Speech & Language, 21(3), 492–518.\\nShannon, C. E. (1948). A mathematical theory of commu-\\nnication. Bell System Technical Journal, 27(3), 379–423.\\nContinued in the following volume.\\x0c28\\nChapter 3\\n•\\nN-gram Language Models\\nShannon, C. E. (1951). Prediction and entropy of printed\\nEnglish. Bell System Technical Journal, 30, 50–64.\\nStolcke, A. (1998). Entropy-based pruning of backoff lan-\\nguage models. In Proc. ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 256, '_split_overlap': [{'doc_id': '5721c7227a31b21b08ce924c164a521e', 'range': (0, 218)}, {'doc_id': '9e017f25e15335fd13392ba0ca5c1ff3', 'range': (410, 636)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'a3c17c3b1ec9e4892d0607e41a08192a'}>,\n",
       " <Document: {'content': '28\\nChapter 3\\n•\\nN-gram Language Models\\nShannon, C. E. (1951). Prediction and entropy of printed\\nEnglish. Bell System Technical Journal, 30, 50–64.\\nStolcke, A. (1998). Entropy-based pruning of backoff lan-\\nguage models. In Proc. DARPA Broadcast News Transcrip-\\ntion and Understanding Workshop, 270–274.\\nStolcke, A. (2002). SRILM – an extensible language model-\\ning toolkit. In ICSLP-02.\\nTalbot, D. and Osborne, M. (2007). Smoothed Bloom Fil-\\nter Language Models: Tera-Scale LMs on the Cheap. In\\nEMNLP/CoNLL 2007, 468–476.\\nTrnka, K., Yarrington, D., McCaw, J., McCoy, K. F., and\\nPennington, C. (2007). ', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 257, '_split_overlap': [{'doc_id': 'a3c17c3b1ec9e4892d0607e41a08192a', 'range': (0, 226)}, {'doc_id': '6392d57d19ab5ca0b17badf5d965bf94', 'range': (385, 598)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e017f25e15335fd13392ba0ca5c1ff3'}>,\n",
       " <Document: {'content': 'Talbot, D. and Osborne, M. (2007). Smoothed Bloom Fil-\\nter Language Models: Tera-Scale LMs on the Cheap. In\\nEMNLP/CoNLL 2007, 468–476.\\nTrnka, K., Yarrington, D., McCaw, J., McCoy, K. F., and\\nPennington, C. (2007). The effects of word prediction on\\ncommunication rate for AAC. In NAACL-HLT 07, 173–\\n176.\\nWitten, I. H. and Bell, T. C. (1991).\\nThe zero-frequency\\nproblem: Estimating the probabilities of novel events in\\nadaptive text compression. IEEE Transactions on Informa-\\ntion Theory, 37(4), 1085–1094.', 'content_type': 'text', 'score': None, 'meta': {'name': 'ngrams reading.pdf', '_split_id': 258, '_split_overlap': [{'doc_id': '9e017f25e15335fd13392ba0ca5c1ff3', 'range': (0, 213)}]}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6392d57d19ab5ca0b17badf5d965bf94'}>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54097cd-0618-4b8d-8553-49fce5bab70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
