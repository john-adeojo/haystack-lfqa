{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e82b0f58-cf03-4238-80b6-1c4ace11ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "from haystack.utils import convert_files_to_docs\n",
    "\n",
    "# pre-process docs \n",
    "def preprocess_docs(doc_dir):\n",
    "    all_docs = convert_files_to_docs(dir_path=doc_dir)\n",
    "    preprocessor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=False,\n",
    "        split_by=\"word\",\n",
    "        split_respect_sentence_boundary=True,\n",
    "        split_overlap=30, \n",
    "        split_length=100\n",
    "    )\n",
    "    docs = preprocessor.process(all_docs)\n",
    "    print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6cbd3aff-7328-40d1-a247-15079576c12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing:   0%|                                                                             | 0/2 [00:00<?, ?docs/s]We found one or more sentences whose word count is higher than the split length.\n",
      "Preprocessing: 100%|█████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17.72docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_files_input: 2\n",
      "n_docs_output: 276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc_dir = r\"C:\\Users\\johna\\anaconda3\\envs\\lfqa_env\\haystack-lfqa\\documents\"\n",
    "docs = preprocess_docs(doc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04558e28-3493-4459-9951-829a89432d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.cursor.LegacyCursorResult at 0x1e92adc3940>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('sqlite:///faiss_document_store.db')  # Use the correct path to your SQLite DB file\n",
    "# engine.execute(\"DROP TABLE document\")  # Be careful with this, it will delete all your documents!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21db427b-5ba1-43f0-a221-10e58ad706c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Documents: 10000it [00:00, 16048.11it/s]                                                                         \n"
     ]
    }
   ],
   "source": [
    "from haystack.document_stores import FAISSDocumentStore\n",
    "\n",
    "# create FAISS in memory\n",
    "def vector_stores(docs):\n",
    "    document_store = FAISSDocumentStore(sql_url=\"sqlite:///:memory:\", faiss_index_factory_str=\"Flat\", embedding_dim=384)\n",
    "    document_store.write_documents(docs)\n",
    "    return document_store\n",
    "\n",
    "document_store = vector_stores(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c54097cd-0618-4b8d-8553-49fce5bab70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Embedding:   0%|                                                                     | 0/276 [00:00<?, ? docs/s]\n",
      "Batches:   0%|                                                                                     | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Batches:  11%|████████▌                                                                    | 1/9 [00:04<00:32,  4.05s/it]\u001b[A\n",
      "Batches:  22%|█████████████████                                                            | 2/9 [00:06<00:20,  2.87s/it]\u001b[A\n",
      "Batches:  33%|█████████████████████████▋                                                   | 3/9 [00:08<00:15,  2.55s/it]\u001b[A\n",
      "Batches:  44%|██████████████████████████████████▏                                          | 4/9 [00:09<00:10,  2.10s/it]\u001b[A\n",
      "Batches:  56%|██████████████████████████████████████████▊                                  | 5/9 [00:11<00:08,  2.01s/it]\u001b[A\n",
      "Batches:  67%|███████████████████████████████████████████████████▎                         | 6/9 [00:13<00:05,  1.97s/it]\u001b[A\n",
      "Batches:  78%|███████████████████████████████████████████████████████████▉                 | 7/9 [00:15<00:03,  1.97s/it]\u001b[A\n",
      "Batches:  89%|████████████████████████████████████████████████████████████████████▍        | 8/9 [00:16<00:01,  1.78s/it]\u001b[A\n",
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████| 9/9 [00:17<00:00,  1.95s/it]\u001b[A\n",
      "Documents Processed: 10000 docs [00:17, 567.55 docs/s]                                                                   \n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import EmbeddingRetriever\n",
    "\n",
    "\n",
    "def generate_embeddings(document_store):\n",
    "    retriever = EmbeddingRetriever(\n",
    "        document_store=document_store,\n",
    "        embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    # Important:\n",
    "    # Now that we initialized the Retriever, we need to call update_embeddings() to iterate over all\n",
    "    # previously indexed documents and update their embedding representation.\n",
    "    # While this can be a time consuming operation (depending on the corpus size), it only needs to be done once.\n",
    "    # At query time, we only need to embed the query and compare it to the existing document embeddings, which is very fast.\n",
    "    document_store.update_embeddings(retriever)\n",
    "    return retriever\n",
    "\n",
    "retriever = generate_embeddings(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1fa1423-25c9-4dce-932b-5e024c46ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import PromptNode, PromptTemplate, AnswerParser\n",
    "\n",
    "lfqa_prompt = PromptTemplate(\n",
    "    prompt=\"\"\"Synthesize a comprehensive answer from the text for the given question.\n",
    "                             Provide a clear and concise response that summarizes the key points and information presented in the text.\n",
    "                             Your answer should be in your own words and be no longer than 50 words.\n",
    "                             \\n\\n Related text: {join(documents)} \\n\\n Question: {query} \\n\\n Answer:\"\"\",\n",
    "    output_parser=AnswerParser(),\n",
    ")\n",
    "\n",
    "prompt_node = PromptNode(model_name_or_path=\"google/flan-t5-large\", default_prompt_template=lfqa_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bd62eda-cb03-4d52-b397-9c5e451797a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|█████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1645 > 512). Running this sequence through the model will result in indexing errors\n",
      "The prompt has been truncated from 1645 tokens to 412 tokens so that the prompt length and answer length (100 tokens) fit within the max token limit (512 tokens). Shorten the prompt to prevent it from being cut off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The simplest language model that assigns probabilities language model LM to sentences and sequences of words is the n-gram.\n"
     ]
    }
   ],
   "source": [
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.add_node(component=retriever, name=\"retriever\", inputs=[\"Query\"])\n",
    "pipe.add_node(component=prompt_node, name=\"prompt_node\", inputs=[\"retriever\"])\n",
    "output = pipe.run(query=\"what are ngram language models?\")\n",
    "\n",
    "print(output[\"answers\"][0].answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adde8f7-984b-46c1-aa09-60963a863069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
